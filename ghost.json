{"db":[{"meta":{"exported_on":1757403222586,"version":"4.48.9"},"data":{"benefits":[],"custom_theme_settings":[],"newsletters":[{"id":"67b142161d62d30001b31eba","name":"Software Consulting and Contracting - A & H Solutions","description":"We help startups, small businesses and Enterprises to build and manage tech products.","slug":"default-newsletter","sender_email":null,"sender_reply_to":"newsletter","status":"active","visibility":"members","subscribe_on_signup":1,"sort_order":0,"header_image":null,"show_header_icon":1,"show_header_title":1,"title_font_category":"sans_serif","title_alignment":"center","show_feature_image":1,"body_font_category":"serif","footer_content":"Let's go paperless together.","show_badge":0,"sender_name":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":null,"show_header_name":0,"uuid":"2a9feba7-026b-4df4-98f6-fcae40170eaa"}],"offer_redemptions":[],"offers":[],"posts":[{"id":"60b6ee631d684200019a9576","uuid":"98f1ad21-3e10-4449-8fb9-46280e3bd1ea","title":"About","slug":"about","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"We want to build a better working world through our own actions and by engaging with like-minded organizations and individuals. This is our purpose — and why we exist as an organization.\"]]],[1,\"p\",[[0,[0],1,\"When businesses works better, the world works better.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>We want to build a better working world through our own actions and by engaging with like-minded organizations and individuals. This is our purpose — and why we exist as an organization.</p><p><strong>When businesses works better, the world works better.</strong></p>","comment_id":"60b6ee631d684200019a9576","plaintext":"We want to build a better working world through our own actions and by engaging\nwith like-minded organizations and individuals. This is our purpose — and why we\nexist as an organization.\n\nWhen businesses works better, the world works better.","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"5951f5fca366002ebd5dbef7","created_at":"2021-06-02T02:35:15.000Z","updated_at":"2022-10-15T23:16:48.000Z","published_at":"2021-06-02T02:35:21.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b6ee631d684200019a9578","uuid":"73a4ba40-ed85-4266-8ed6-d6c90f04b202","title":"Contact","slug":"contact","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Do You Need Help?\"]]],[1,\"p\",[[0,[],0,\"A & H is here to help you find the right information and answer any questions.\"]]],[1,\"blockquote\",[[0,[0,0],2,\"contact@aandhsolutions.com\"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Do You Need Help?</p><p>A &amp; H is here to help you find the right information and answer any questions.</p><blockquote><strong><strong>contact@aandhsolutions.com</strong></strong></blockquote>","comment_id":"60b6ee631d684200019a9578","plaintext":"Do You Need Help?\n\nA & H is here to help you find the right information and answer any questions.\n\n> contact@aandhsolutions.com","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"5951f5fca366002ebd5dbef7","created_at":"2021-06-02T02:35:15.000Z","updated_at":"2021-06-02T14:26:10.000Z","published_at":"2021-06-02T02:35:22.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b6ee631d684200019a957a","uuid":"11cfc8e0-77a9-4f21-8034-4d797e8f2bfb","title":"Privacy","slug":"privacy","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"__GHOST_URL__/\",\"rel\",\"external nofollow noopener\"]],[\"a\",[\"href\",\"https://www.freeprivacypolicy.com/blog/cookies/\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Privacy Policy for A & H Solutions\"]]],[1,\"p\",[[0,[],0,\"Last updated: June 03, 2021\"]]],[1,\"p\",[[0,[],0,\"This Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.\"]]],[1,\"p\",[[0,[],0,\"We use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. \"]]],[1,\"h1\",[[0,[0,0],2,\"Interpretation and Definitions\"]]],[1,\"h2\",[[0,[0,0],2,\"Interpretation\"]]],[1,\"p\",[[0,[],0,\"The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.\"]]],[1,\"h2\",[[0,[0,0],2,\"Definitions\"]]],[1,\"p\",[[0,[],0,\"For the purposes of this Privacy Policy:\"]]],[3,\"ul\",[[[0,[0],1,\"Account\"],[0,[],0,\" means a unique account created for You to access our Service or parts of our Service.\"]],[[0,[0],1,\"Company\"],[0,[],0,\" (referred to as either \\\"the Company\\\", \\\"We\\\", \\\"Us\\\" or \\\"Our\\\" in this Agreement) refers to A & H Solutions, 17833 9A Ave SW Edmonton Canada T6W 3K1.\"]],[[0,[0],1,\"Cookies\"],[0,[],0,\" are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.\"]],[[0,[0],1,\"Country\"],[0,[],0,\" refers to: Alberta, Canada\"]],[[0,[0],1,\"Device\"],[0,[],0,\" means any device that can access the Service such as a computer, a cellphone or a digital tablet.\"]],[[0,[0],1,\"Personal Data\"],[0,[],0,\" is any information that relates to an identified or identifiable individual.\"]],[[0,[0],1,\"Service\"],[0,[],0,\" refers to the Website.\"]],[[0,[0],1,\"Service Provider\"],[0,[],0,\" means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.\"]],[[0,[0],1,\"Usage Data\"],[0,[],0,\" refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).\"]],[[0,[0],1,\"Website\"],[0,[],0,\" refers to A & H Solutions, accessible from \"],[0,[1],1,\"https://aandhsolutions.com\"]],[[0,[0],1,\"You\"],[0,[],0,\" means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.\"]]]],[1,\"h1\",[[0,[0,0],2,\"Collecting and Using Your Personal Data\"]]],[1,\"h2\",[[0,[0,0],2,\"Types of Data Collected\"]]],[1,\"h3\",[[0,[0,0],2,\"Personal Data\"]]],[1,\"p\",[[0,[],0,\"While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:\"]]],[3,\"ul\",[[[0,[],0,\"Email address\"]],[[0,[],0,\"Usage Data\"]]]],[1,\"h3\",[[0,[0,0],2,\"Usage Data\"]]],[1,\"p\",[[0,[],0,\"Usage Data is collected automatically when using the Service.\"]]],[1,\"p\",[[0,[],0,\"Usage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.\"]]],[1,\"p\",[[0,[],0,\"When You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.\"]]],[1,\"p\",[[0,[],0,\"We may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.\"]]],[1,\"h3\",[[0,[0,0],2,\"Tracking Technologies and Cookies\"]]],[1,\"p\",[[0,[],0,\"We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:\"]]],[3,\"ul\",[[[0,[0],1,\"Cookies or Browser Cookies.\"],[0,[],0,\" A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.\"]]]],[1,\"p\",[[0,[],0,\"Cookies can be \\\"Persistent\\\" or \\\"Session\\\" Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. Learn more about cookies: \"],[0,[2],1,\"Cookies: What Do They Do?\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"We use both Session and Persistent Cookies for the purposes set out below:\"]]],[1,\"p\",[[0,[0],1,\"Necessary / Essential Cookies\"]]],[1,\"p\",[[0,[],0,\"Type: Session Cookies\"]]],[1,\"p\",[[0,[],0,\"Administered by: Us\"]]],[1,\"p\",[[0,[],0,\"Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.\"]]],[1,\"p\",[[0,[0],1,\"Cookies Policy / Notice Acceptance Cookies\"]]],[1,\"p\",[[0,[],0,\"Type: Persistent Cookies\"]]],[1,\"p\",[[0,[],0,\"Administered by: Us\"]]],[1,\"p\",[[0,[],0,\"Purpose: These Cookies identify if users have accepted the use of cookies on the Website.\"]]],[1,\"p\",[[0,[0],1,\"Functionality Cookies\"]]],[1,\"p\",[[0,[],0,\"Type: Persistent Cookies\"]]],[1,\"p\",[[0,[],0,\"Administered by: Us\"]]],[1,\"p\",[[0,[],0,\"Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.\"]]],[1,\"p\",[[0,[],0,\"For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.\"]]],[1,\"h2\",[[0,[0,0],2,\"Use of Your Personal Data\"]]],[1,\"p\",[[0,[],0,\"The Company may use Personal Data for the following purposes:\"]]],[3,\"ul\",[[[0,[0],1,\"To provide and maintain our Service\"],[0,[],0,\", including to monitor the usage of our Service.\"]],[[0,[0],1,\"To manage Your Account:\"],[0,[],0,\" to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.\"]],[[0,[0],1,\"For the performance of a contract:\"],[0,[],0,\" the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.\"]],[[0,[0],1,\"To contact You:\"],[0,[],0,\" To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.\"]],[[0,[0],1,\"To provide You\"],[0,[],0,\" with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.\"]],[[0,[0],1,\"To manage Your requests:\"],[0,[],0,\" To attend and manage Your requests to Us.\"]],[[0,[0],1,\"For business transfers:\"],[0,[],0,\" We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.\"]],[[0,[0],1,\"For other purposes\"],[0,[],0,\": We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.\"]]]],[1,\"p\",[[0,[],0,\"We may share Your personal information in the following situations:\"]]],[3,\"ul\",[[[0,[0],1,\"With Service Providers:\"],[0,[],0,\" We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.\"]],[[0,[0],1,\"For business transfers:\"],[0,[],0,\" We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.\"]],[[0,[0],1,\"With Affiliates:\"],[0,[],0,\" We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.\"]],[[0,[0],1,\"With business partners:\"],[0,[],0,\" We may share Your information with Our business partners to offer You certain products, services or promotions.\"]],[[0,[0],1,\"With other users:\"],[0,[],0,\" when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.\"]],[[0,[0],1,\"With Your consent\"],[0,[],0,\": We may disclose Your personal information for any other purpose with Your consent.\"]]]],[1,\"h2\",[[0,[0,0],2,\"Retention of Your Personal Data\"]]],[1,\"p\",[[0,[],0,\"The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.\"]]],[1,\"p\",[[0,[],0,\"The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.\"]]],[1,\"h2\",[[0,[0,0],2,\"Transfer of Your Personal Data\"]]],[1,\"p\",[[0,[],0,\"Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.\"]]],[1,\"p\",[[0,[],0,\"Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.\"]]],[1,\"p\",[[0,[],0,\"The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.\"]]],[1,\"h2\",[[0,[0,0],2,\"Disclosure of Your Personal Data\"]]],[1,\"h3\",[[0,[0,0],2,\"Business Transactions\"]]],[1,\"p\",[[0,[],0,\"If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.\"]]],[1,\"h3\",[[0,[0,0],2,\"Law enforcement\"]]],[1,\"p\",[[0,[],0,\"Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).\"]]],[1,\"h3\",[[0,[0,0],2,\"Other legal requirements\"]]],[1,\"p\",[[0,[],0,\"The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:\"]]],[3,\"ul\",[[[0,[],0,\"Comply with a legal obligation\"]],[[0,[],0,\"Protect and defend the rights or property of the Company\"]],[[0,[],0,\"Prevent or investigate possible wrongdoing in connection with the Service\"]],[[0,[],0,\"Protect the personal safety of Users of the Service or the public\"]],[[0,[],0,\"Protect against legal liability\"]]]],[1,\"h2\",[[0,[0,0],2,\"Security of Your Personal Data\"]]],[1,\"p\",[[0,[],0,\"The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.\"]]],[1,\"h1\",[[0,[0,0],2,\"Children's Privacy\"]]],[1,\"p\",[[0,[],0,\"Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.\"]]],[1,\"p\",[[0,[],0,\"If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.\"]]],[1,\"h1\",[[0,[0,0],2,\"Links to Other Websites\"]]],[1,\"p\",[[0,[],0,\"Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.\"]]],[1,\"p\",[[0,[],0,\"We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.\"]]],[1,\"h1\",[[0,[0,0],2,\"Changes to this Privacy Policy\"]]],[1,\"p\",[[0,[],0,\"We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.\"]]],[1,\"p\",[[0,[],0,\"We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \\\"Last updated\\\" date at the top of this Privacy Policy.\"]]],[1,\"p\",[[0,[],0,\"You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.\"]]],[1,\"h1\",[[0,[0,0],2,\"Contact Us\"]]],[1,\"p\",[[0,[],0,\"If you have any questions about this Privacy Policy, You can contact us:\"]]],[3,\"ul\",[[[0,[],0,\"By email: contact@aandhsolutions.com\"]]]],[1,\"p\",[[0,[],0,\"Privacy Policy for A & H Solutions\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Privacy Policy for A &amp; H Solutions</strong></p><p>Last updated: June 03, 2021</p><p>This Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.</p><p>We use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. </p><h1 id=\"interpretation-and-definitions\"><strong><strong>Interpretation and Definitions</strong></strong></h1><h2 id=\"interpretation\"><strong><strong>Interpretation</strong></strong></h2><p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p><h2 id=\"definitions\"><strong><strong>Definitions</strong></strong></h2><p>For the purposes of this Privacy Policy:</p><ul><li><strong>Account</strong> means a unique account created for You to access our Service or parts of our Service.</li><li><strong>Company</strong> (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to A &amp; H Solutions, 17833 9A Ave SW Edmonton Canada T6W 3K1.</li><li><strong>Cookies</strong> are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.</li><li><strong>Country</strong> refers to: Alberta, Canada</li><li><strong>Device</strong> means any device that can access the Service such as a computer, a cellphone or a digital tablet.</li><li><strong>Personal Data</strong> is any information that relates to an identified or identifiable individual.</li><li><strong>Service</strong> refers to the Website.</li><li><strong>Service Provider</strong> means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.</li><li><strong>Usage Data</strong> refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).</li><li><strong>Website</strong> refers to A &amp; H Solutions, accessible from <a href=\"__GHOST_URL__/\" rel=\"external nofollow noopener\">https://aandhsolutions.com</a></li><li><strong>You</strong> means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</li></ul><h1 id=\"collecting-and-using-your-personal-data\"><strong><strong>Collecting and Using Your Personal Data</strong></strong></h1><h2 id=\"types-of-data-collected\"><strong><strong>Types of Data Collected</strong></strong></h2><h3 id=\"personal-data\"><strong><strong>Personal Data</strong></strong></h3><p>While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:</p><ul><li>Email address</li><li>Usage Data</li></ul><h3 id=\"usage-data\"><strong><strong>Usage Data</strong></strong></h3><p>Usage Data is collected automatically when using the Service.</p><p>Usage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.</p><p>When You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.</p><p>We may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.</p><h3 id=\"tracking-technologies-and-cookies\"><strong><strong>Tracking Technologies and Cookies</strong></strong></h3><p>We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:</p><ul><li><strong>Cookies or Browser Cookies.</strong> A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.</li></ul><p>Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. Learn more about cookies: <a href=\"https://www.freeprivacypolicy.com/blog/cookies/\">Cookies: What Do They Do?</a>.</p><p>We use both Session and Persistent Cookies for the purposes set out below:</p><p><strong>Necessary / Essential Cookies</strong></p><p>Type: Session Cookies</p><p>Administered by: Us</p><p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services.</p><p><strong>Cookies Policy / Notice Acceptance Cookies</strong></p><p>Type: Persistent Cookies</p><p>Administered by: Us</p><p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website.</p><p><strong>Functionality Cookies</strong></p><p>Type: Persistent Cookies</p><p>Administered by: Us</p><p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p><p>For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.</p><h2 id=\"use-of-your-personal-data\"><strong><strong>Use of Your Personal Data</strong></strong></h2><p>The Company may use Personal Data for the following purposes:</p><ul><li><strong>To provide and maintain our Service</strong>, including to monitor the usage of our Service.</li><li><strong>To manage Your Account:</strong> to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.</li><li><strong>For the performance of a contract:</strong> the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.</li><li><strong>To contact You:</strong> To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.</li><li><strong>To provide You</strong> with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.</li><li><strong>To manage Your requests:</strong> To attend and manage Your requests to Us.</li><li><strong>For business transfers:</strong> We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.</li><li><strong>For other purposes</strong>: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.</li></ul><p>We may share Your personal information in the following situations:</p><ul><li><strong>With Service Providers:</strong> We may share Your personal information with Service Providers to monitor and analyze the use of our Service, to contact You.</li><li><strong>For business transfers:</strong> We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.</li><li><strong>With Affiliates:</strong> We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.</li><li><strong>With business partners:</strong> We may share Your information with Our business partners to offer You certain products, services or promotions.</li><li><strong>With other users:</strong> when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside.</li><li><strong>With Your consent</strong>: We may disclose Your personal information for any other purpose with Your consent.</li></ul><h2 id=\"retention-of-your-personal-data\"><strong><strong>Retention of Your Personal Data</strong></strong></h2><p>The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.</p><p>The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.</p><h2 id=\"transfer-of-your-personal-data\"><strong><strong>Transfer of Your Personal Data</strong></strong></h2><p>Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.</p><p>Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.</p><p>The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.</p><h2 id=\"disclosure-of-your-personal-data\"><strong><strong>Disclosure of Your Personal Data</strong></strong></h2><h3 id=\"business-transactions\"><strong><strong>Business Transactions</strong></strong></h3><p>If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.</p><h3 id=\"law-enforcement\"><strong><strong>Law enforcement</strong></strong></h3><p>Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).</p><h3 id=\"other-legal-requirements\"><strong><strong>Other legal requirements</strong></strong></h3><p>The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:</p><ul><li>Comply with a legal obligation</li><li>Protect and defend the rights or property of the Company</li><li>Prevent or investigate possible wrongdoing in connection with the Service</li><li>Protect the personal safety of Users of the Service or the public</li><li>Protect against legal liability</li></ul><h2 id=\"security-of-your-personal-data\"><strong><strong>Security of Your Personal Data</strong></strong></h2><p>The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.</p><h1 id=\"childrens-privacy\"><strong><strong>Children's Privacy</strong></strong></h1><p>Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.</p><p>If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.</p><h1 id=\"links-to-other-websites\"><strong><strong>Links to Other Websites</strong></strong></h1><p>Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.</p><p>We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.</p><h1 id=\"changes-to-this-privacy-policy\"><strong><strong>Changes to this Privacy Policy</strong></strong></h1><p>We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.</p><p>We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \"Last updated\" date at the top of this Privacy Policy.</p><p>You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.</p><h1 id=\"contact-us\"><strong><strong>Contact Us</strong></strong></h1><p>If you have any questions about this Privacy Policy, You can contact us:</p><ul><li>By email: contact@aandhsolutions.com</li></ul><p>Privacy Policy for A &amp; H Solutions</p>","comment_id":"60b6ee631d684200019a957a","plaintext":"Privacy Policy for A & H Solutions\n\nLast updated: June 03, 2021\n\nThis Privacy Policy describes Our policies and procedures on the collection, use\nand disclosure of Your information when You use the Service and tells You about\nYour privacy rights and how the law protects You.\n\nWe use Your Personal data to provide and improve the Service. By using the\nService, You agree to the collection and use of information in accordance with\nthis Privacy Policy. \n\nInterpretation and Definitions\nInterpretation\nThe words of which the initial letter is capitalized have meanings defined under\nthe following conditions. The following definitions shall have the same meaning\nregardless of whether they appear in singular or in plural.\n\nDefinitions\nFor the purposes of this Privacy Policy:\n\n * Account means a unique account created for You to access our Service or parts\n   of our Service.\n * Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this\n   Agreement) refers to A & H Solutions, 17833 9A Ave SW Edmonton Canada T6W\n   3K1.\n * Cookies are small files that are placed on Your computer, mobile device or\n   any other device by a website, containing the details of Your browsing\n   history on that website among its many uses.\n * Country refers to: Alberta, Canada\n * Device means any device that can access the Service such as a computer, a\n   cellphone or a digital tablet.\n * Personal Data is any information that relates to an identified or\n   identifiable individual.\n * Service refers to the Website.\n * Service Provider means any natural or legal person who processes the data on\n   behalf of the Company. It refers to third-party companies or individuals\n   employed by the Company to facilitate the Service, to provide the Service on\n   behalf of the Company, to perform services related to the Service or to\n   assist the Company in analyzing how the Service is used.\n * Usage Data refers to data collected automatically, either generated by the\n   use of the Service or from the Service infrastructure itself (for example,\n   the duration of a page visit).\n * Website refers to A & H Solutions, accessible from https://aandhsolutions.com\n   [__GHOST_URL__/]\n * You means the individual accessing or using the Service, or the company, or\n   other legal entity on behalf of which such individual is accessing or using\n   the Service, as applicable.\n\nCollecting and Using Your Personal Data\nTypes of Data Collected\nPersonal Data\nWhile using Our Service, We may ask You to provide Us with certain personally\nidentifiable information that can be used to contact or identify You. Personally\nidentifiable information may include, but is not limited to:\n\n * Email address\n * Usage Data\n\nUsage Data\nUsage Data is collected automatically when using the Service.\n\nUsage Data may include information such as Your Device's Internet Protocol\naddress (e.g. IP address), browser type, browser version, the pages of our\nService that You visit, the time and date of Your visit, the time spent on those\npages, unique device identifiers and other diagnostic data.\n\nWhen You access the Service by or through a mobile device, We may collect\ncertain information automatically, including, but not limited to, the type of\nmobile device You use, Your mobile device unique ID, the IP address of Your\nmobile device, Your mobile operating system, the type of mobile Internet browser\nYou use, unique device identifiers and other diagnostic data.\n\nWe may also collect information that Your browser sends whenever You visit our\nService or when You access the Service by or through a mobile device.\n\nTracking Technologies and Cookies\nWe use Cookies and similar tracking technologies to track the activity on Our\nService and store certain information. Tracking technologies used are beacons,\ntags, and scripts to collect and track information and to improve and analyze\nOur Service. The technologies We use may include:\n\n * Cookies or Browser Cookies. A cookie is a small file placed on Your Device.\n   You can instruct Your browser to refuse all Cookies or to indicate when a\n   Cookie is being sent. However, if You do not accept Cookies, You may not be\n   able to use some parts of our Service. Unless you have adjusted Your browser\n   setting so that it will refuse Cookies, our Service may use Cookies.\n\nCookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on\nYour personal computer or mobile device when You go offline, while Session\nCookies are deleted as soon as You close Your web browser. Learn more about\ncookies: Cookies: What Do They Do?\n[https://www.freeprivacypolicy.com/blog/cookies/].\n\nWe use both Session and Persistent Cookies for the purposes set out below:\n\nNecessary / Essential Cookies\n\nType: Session Cookies\n\nAdministered by: Us\n\nPurpose: These Cookies are essential to provide You with services available\nthrough the Website and to enable You to use some of its features. They help to\nauthenticate users and prevent fraudulent use of user accounts. Without these\nCookies, the services that You have asked for cannot be provided, and We only\nuse these Cookies to provide You with those services.\n\nCookies Policy / Notice Acceptance Cookies\n\nType: Persistent Cookies\n\nAdministered by: Us\n\nPurpose: These Cookies identify if users have accepted the use of cookies on the\nWebsite.\n\nFunctionality Cookies\n\nType: Persistent Cookies\n\nAdministered by: Us\n\nPurpose: These Cookies allow us to remember choices You make when You use the\nWebsite, such as remembering your login details or language preference. The\npurpose of these Cookies is to provide You with a more personal experience and\nto avoid You having to re-enter your preferences every time You use the Website.\n\nFor more information about the cookies we use and your choices regarding\ncookies, please visit our Cookies Policy or the Cookies section of our Privacy\nPolicy.\n\nUse of Your Personal Data\nThe Company may use Personal Data for the following purposes:\n\n * To provide and maintain our Service, including to monitor the usage of our\n   Service.\n * To manage Your Account: to manage Your registration as a user of the Service.\n   The Personal Data You provide can give You access to different\n   functionalities of the Service that are available to You as a registered\n   user.\n * For the performance of a contract: the development, compliance and\n   undertaking of the purchase contract for the products, items or services You\n   have purchased or of any other contract with Us through the Service.\n * To contact You: To contact You by email, telephone calls, SMS, or other\n   equivalent forms of electronic communication, such as a mobile application's\n   push notifications regarding updates or informative communications related to\n   the functionalities, products or contracted services, including the security\n   updates, when necessary or reasonable for their implementation.\n * To provide You with news, special offers and general information about other\n   goods, services and events which we offer that are similar to those that you\n   have already purchased or enquired about unless You have opted not to receive\n   such information.\n * To manage Your requests: To attend and manage Your requests to Us.\n * For business transfers: We may use Your information to evaluate or conduct a\n   merger, divestiture, restructuring, reorganization, dissolution, or other\n   sale or transfer of some or all of Our assets, whether as a going concern or\n   as part of bankruptcy, liquidation, or similar proceeding, in which Personal\n   Data held by Us about our Service users is among the assets transferred.\n * For other purposes: We may use Your information for other purposes, such as\n   data analysis, identifying usage trends, determining the effectiveness of our\n   promotional campaigns and to evaluate and improve our Service, products,\n   services, marketing and your experience.\n\nWe may share Your personal information in the following situations:\n\n * With Service Providers: We may share Your personal information with Service\n   Providers to monitor and analyze the use of our Service, to contact You.\n * For business transfers: We may share or transfer Your personal information in\n   connection with, or during negotiations of, any merger, sale of Company\n   assets, financing, or acquisition of all or a portion of Our business to\n   another company.\n * With Affiliates: We may share Your information with Our affiliates, in which\n   case we will require those affiliates to honor this Privacy Policy.\n   Affiliates include Our parent company and any other subsidiaries, joint\n   venture partners or other companies that We control or that are under common\n   control with Us.\n * With business partners: We may share Your information with Our business\n   partners to offer You certain products, services or promotions.\n * With other users: when You share personal information or otherwise interact\n   in the public areas with other users, such information may be viewed by all\n   users and may be publicly distributed outside.\n * With Your consent: We may disclose Your personal information for any other\n   purpose with Your consent.\n\nRetention of Your Personal Data\nThe Company will retain Your Personal Data only for as long as is necessary for\nthe purposes set out in this Privacy Policy. We will retain and use Your\nPersonal Data to the extent necessary to comply with our legal obligations (for\nexample, if we are required to retain your data to comply with applicable laws),\nresolve disputes, and enforce our legal agreements and policies.\n\nThe Company will also retain Usage Data for internal analysis purposes. Usage\nData is generally retained for a shorter period of time, except when this data\nis used to strengthen the security or to improve the functionality of Our\nService, or We are legally obligated to retain this data for longer time\nperiods.\n\nTransfer of Your Personal Data\nYour information, including Personal Data, is processed at the Company's\noperating offices and in any other places where the parties involved in the\nprocessing are located. It means that this information may be transferred to —\nand maintained on — computers located outside of Your state, province, country\nor other governmental jurisdiction where the data protection laws may differ\nthan those from Your jurisdiction.\n\nYour consent to this Privacy Policy followed by Your submission of such\ninformation represents Your agreement to that transfer.\n\nThe Company will take all steps reasonably necessary to ensure that Your data is\ntreated securely and in accordance with this Privacy Policy and no transfer of\nYour Personal Data will take place to an organization or a country unless there\nare adequate controls in place including the security of Your data and other\npersonal information.\n\nDisclosure of Your Personal Data\nBusiness Transactions\nIf the Company is involved in a merger, acquisition or asset sale, Your Personal\nData may be transferred. We will provide notice before Your Personal Data is\ntransferred and becomes subject to a different Privacy Policy.\n\nLaw enforcement\nUnder certain circumstances, the Company may be required to disclose Your\nPersonal Data if required to do so by law or in response to valid requests by\npublic authorities (e.g. a court or a government agency).\n\nOther legal requirements\nThe Company may disclose Your Personal Data in the good faith belief that such\naction is necessary to:\n\n * Comply with a legal obligation\n * Protect and defend the rights or property of the Company\n * Prevent or investigate possible wrongdoing in connection with the Service\n * Protect the personal safety of Users of the Service or the public\n * Protect against legal liability\n\nSecurity of Your Personal Data\nThe security of Your Personal Data is important to Us, but remember that no\nmethod of transmission over the Internet, or method of electronic storage is\n100% secure. While We strive to use commercially acceptable means to protect\nYour Personal Data, We cannot guarantee its absolute security.\n\nChildren's Privacy\nOur Service does not address anyone under the age of 13. We do not knowingly\ncollect personally identifiable information from anyone under the age of 13. If\nYou are a parent or guardian and You are aware that Your child has provided Us\nwith Personal Data, please contact Us. If We become aware that We have collected\nPersonal Data from anyone under the age of 13 without verification of parental\nconsent, We take steps to remove that information from Our servers.\n\nIf We need to rely on consent as a legal basis for processing Your information\nand Your country requires consent from a parent, We may require Your parent's\nconsent before We collect and use that information.\n\nLinks to Other Websites\nOur Service may contain links to other websites that are not operated by Us. If\nYou click on a third party link, You will be directed to that third party's\nsite. We strongly advise You to review the Privacy Policy of every site You\nvisit.\n\nWe have no control over and assume no responsibility for the content, privacy\npolicies or practices of any third party sites or services.\n\nChanges to this Privacy Policy\nWe may update Our Privacy Policy from time to time. We will notify You of any\nchanges by posting the new Privacy Policy on this page.\n\nWe will let You know via email and/or a prominent notice on Our Service, prior\nto the change becoming effective and update the \"Last updated\" date at the top\nof this Privacy Policy.\n\nYou are advised to review this Privacy Policy periodically for any changes.\nChanges to this Privacy Policy are effective when they are posted on this page.\n\nContact Us\nIf you have any questions about this Privacy Policy, You can contact us:\n\n * By email: contact@aandhsolutions.com\n\nPrivacy Policy for A & H Solutions","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"5951f5fca366002ebd5dbef7","created_at":"2021-06-02T02:35:15.000Z","updated_at":"2021-06-03T20:56:49.000Z","published_at":"2021-06-02T02:35:23.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b81d4afda06e0001f8371f","uuid":"d3730aff-c447-450d-9ac7-3272357a2ca2","title":"Interfaces in Golang","slug":"interfaces-in-golang","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/6b747d94d4a8b3e49528261029df755d.js\\\"></script>\\n\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/2b4410e5eaf290bd6c89693a81c71ca8.js\\\"></script>\"}],[\"code\",{\"code\":\"type error interface {\\n    Error() string\\n}\",\"language\":\"golang\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/ad2e1cdcc74a388c579309d3f36a4d45.js\\\"></script>\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/cc45793074d1bcb22077cc1610b8d003.js\\\"></script>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.airs.com/blog/archives/277\",\"rel\",\"noopener\"]],[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://groups.google.com/g/golang-dev/c/F3l9Iz1JX4g/discussion\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://github.com/golang/go/wiki/CodeReviewComments#interfaces\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://golang.org/doc/effective_go#generality\",\"rel\",\"noopener\"]],[\"code\"],[\"a\",[\"href\",\"https://sendgrid.com/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=PAAkCSZUG1c&t=5m17s\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[],0,\"In my few years of using Golang, I’ve come across several discussions involving the use of interfaces. The arguments range from:\"]]],[1,\"blockquote\",[[0,[],0,\"Why are we not defining interfaces with the type definition like in typical static languages like C++, Java etc ?\"]]],[1,\"blockquote\",[[0,[],0,\"Should this package export interface in combination with the exposed type implementing the interface?\"]]],[1,\"blockquote\",[[0,[],0,\"Should this function return an interface rather than the concrete type?\"]]],[1,\"p\",[[0,[],0,\"Let’s try to address these questions one by one.\"]]],[1,\"h4\",[[0,[],0,\"Why are we not defining interfaces with the type definition like in typical static languages like C++, Java etc ?\"]]],[1,\"p\",[[0,[],0,\"In languages like C++, Java, one needs to specify that a type implements an interface like in the code given below:\"]]],[10,0],[1,\"p\",[[0,[],0,\"In such languages, defining the interface for the Object enables the compiler to form a dispatch tables for the objects pointing to the functions.\"]]],[1,\"p\",[[0,[],0,\"Go doesn’t have a traditional dispatch table, and can rely on the interface values during a method dispatch. It’s literally more of a freestyle dispatcher mechanism that requires some work during interface value assignment — it generates a tiny lookup hash-table for the concrete type it’s pointing to. \"],[0,[0],1,\"Here\"],[0,[],0,\" is a great blog post in case you want further reading.\"]]],[1,\"p\",[[0,[],0,\"Even though a bit more expensive, this enables go to have a cleaner type system without the baggage of defining the interface for every type.\"]]],[1,\"p\",[[0,[],0,\"For a developer, this means that \"],[0,[1],0,\"the object \"],[0,[2],1,\"implementing\"],[0,[],0,\" the interface does not need to \"],[0,[2],1,\"explicitly\"],[0,[],1,\" say it implements it,\"],[0,[],0,\" as shown in the code below:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Thus any struct can satisfy an interface simply by implementing its method signatures. It offers several advantages like:\"]]],[3,\"ul\",[[[0,[],0,\"Makes it easier to use mocks instead of real objects in unit tests.\"]],[[0,[],0,\"Helps enforce decoupling between parts of your codebase.\"]]]],[1,\"h4\",[[0,[],0,\"Should this package export interface in combination with the exposed type implementing the interface?\"]]],[1,\"p\",[[0,[],0,\"The short answer is:\"]]],[1,\"blockquote\",[[0,[],0,\"Don’t export any interfaces unless you have to.\"]]],[1,\"p\",[[0,[],0,\"If the consumer of your package requires some level of “inversion of control”, they can define the interface in their own scope.\"]]],[1,\"p\",[[0,[],0,\"However, there might be scenarios where one might want to standardise how a functionality is used. Take a case of golang error interface.\"]]],[10,2],[1,\"p\",[[0,[],0,\"It is a builtin interface in the standard library that standardises the error behaviour. There have been other discussions in the community about standardising some behaviours, e.g. having a common \"],[0,[3],1,\"logging\"],[0,[],0,\" interface. Similarly, it might be useful to have a company wide representation of some common behaviour to provide uniformity and code reusability. However, in this scenario, make sure that the interfaces are small, meaning 1–2 methods.\"]]],[1,\"h4\",[[0,[],0,\"Should this package return an interface rather than the concrete type?\"]]],[1,\"p\",[[0,[],0,\"According to CodeReviewComments, \"],[0,[4],1,\"Go interfaces generally belong in the package that uses values of the interface type, not the package that implements those values.\"]]],[1,\"p\",[[0,[],0,\"However \"],[0,[5],1,\"Effective go\"],[0,[],0,\" docs also complements it by saying that\"]]],[1,\"blockquote\",[[0,[],0,\"if a type exists only to implement an interface and will never have exported methods beyond that interface, there is no need to export the type itself.\"]]],[1,\"p\",[[0,[],0,\"But the question is how do you identify such scenarios? How do you know that the type will have no additional value in the future? In my experience, the answer is to “wait”. Do not start off by returning interfaces, but wait till your code evolves and you see the need for them. As Rob Pike says:\"]]],[1,\"blockquote\",[[0,[],0,\"“Don’t design with interfaces, discover them.”\"]]],[1,\"p\",[[0,[],0,\"A good hint for exposing an interface is \"],[0,[1],1,\"when you have multiple types in your package implementing the same method signature\"],[0,[],0,\". If you look at \"],[0,[6],1,\"http\"],[0,[],0,\" package in standard library, you’ll see that it internally has multiple implementations of \"],[0,[6],1,\"http.Handler\"],[0,[],0,\" interface.\"]]],[1,\"p\",[[0,[],0,\"In case of confusion, it is helpful to look for some \"],[0,[1],1,\"red flags that can signals that you’re probably using interfaces wrong\"],[0,[],0,\". Some are:\"]]],[1,\"h4\",[[0,[],0,\"Your interface is not decoupling an API from change.\"]]],[1,\"p\",[[0,[],0,\"Imagine an implementation of a third party api client package for sending emails e.g. \"],[0,[7],1,\"SendGrid\"]]],[10,3],[1,\"p\",[[0,[],0,\"Now imagine, is there ever going to be a new implementation of \"],[0,[6],1,\"SendGrid\"],[0,[],0,\" ?\"],[1,[],0,0],[0,[],0,\"There is most probably going to be a new implementation of a different email sending service in case SendGrid is replaced with let’s say, \"],[0,[6],1,\"Mailjet\"],[0,[],0,\" . Hence you can just rewrite the code to return the concrete type.\"]]],[1,\"p\",[[0,[],0,\"Also imagine that you want to add a new method to this interface that’s used by lots of people, how do you add a new method to it without breaking their code?\"]]],[10,4],[1,\"p\",[[0,[],0,\"By exposing the struct type itself, you can add new methods to struct itself, without intensive refactoring\"]]],[1,\"h4\",[[0,[],0,\"Your interface has more than 1 or 2 methods.\"]]],[1,\"p\",[[0,[],0,\"Having too many methods for your interface reduces its usability. Taking example of \"],[0,[6],1,\"fmt.Stringer\"],[0,[],0,\" interface, it has only one method signature, i.e.\"]]],[1,\"p\",[[0,[],0,\"type Stringer interface {\"],[0,[],0,\"     String() string\"],[0,[],0,\"}\"]]],[1,\"p\",[[0,[],0,\"and is used in 30 places in the standard library, excluding tests. You can find similar other examples in standard library like \"],[0,[6],1,\"http.Handler\"],[0,[],0,\" , \"],[0,[6],1,\"io.Reader\"],[0,[],0,\" etc.\"]]],[1,\"p\",[[0,[],0,\"Now imagine an interface with 5–6 methods, it cannot really be used at more than 1 or 2 places, as not all types can manage to implement large number of methods.\"]]],[1,\"blockquote\",[[0,[8],1,\"The bigger the interface, the weaker the abstraction.\"]]],[1,\"h4\",[[0,[],0,\"Conclusion:\"]]],[1,\"p\",[[0,[],0,\"The interfaces in Golang can be hard to master, the general recommendation is to follow the rule of thumbs. In case of exceptions to the general rules, wait to see how your code evolves and make adaptations accordingly.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p>In my few years of using Golang, I’ve come across several discussions involving the use of interfaces. The arguments range from:</p><blockquote>Why are we not defining interfaces with the type definition like in typical static languages like C++, Java etc ?</blockquote><blockquote>Should this package export interface in combination with the exposed type implementing the interface?</blockquote><blockquote>Should this function return an interface rather than the concrete type?</blockquote><p>Let’s try to address these questions one by one.</p><h4 id=\"why-are-we-not-defining-interfaces-with-the-type-definition-like-in-typical-static-languages-like-c-java-etc\">Why are we not defining interfaces with the type definition like in typical static languages like C++, Java etc ?</h4><p>In languages like C++, Java, one needs to specify that a type implements an interface like in the code given below:</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/6b747d94d4a8b3e49528261029df755d.js\"></script>\n<!--kg-card-end: html--><p>In such languages, defining the interface for the Object enables the compiler to form a dispatch tables for the objects pointing to the functions.</p><p>Go doesn’t have a traditional dispatch table, and can rely on the interface values during a method dispatch. It’s literally more of a freestyle dispatcher mechanism that requires some work during interface value assignment — it generates a tiny lookup hash-table for the concrete type it’s pointing to. <a href=\"https://www.airs.com/blog/archives/277\" rel=\"noopener\">Here</a> is a great blog post in case you want further reading.</p><p>Even though a bit more expensive, this enables go to have a cleaner type system without the baggage of defining the interface for every type.</p><p>For a developer, this means that <strong>the object <em>implementing</em> the interface does not need to <em>explicitly</em> say it implements it,</strong> as shown in the code below:</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/2b4410e5eaf290bd6c89693a81c71ca8.js\"></script><!--kg-card-end: html--><p>Thus any struct can satisfy an interface simply by implementing its method signatures. It offers several advantages like:</p><ul><li>Makes it easier to use mocks instead of real objects in unit tests.</li><li>Helps enforce decoupling between parts of your codebase.</li></ul><h4 id=\"should-this-package-export-interface-in-combination-with-the-exposed-type-implementing-the-interface\">Should this package export interface in combination with the exposed type implementing the interface?</h4><p>The short answer is:</p><blockquote>Don’t export any interfaces unless you have to.</blockquote><p>If the consumer of your package requires some level of “inversion of control”, they can define the interface in their own scope.</p><p>However, there might be scenarios where one might want to standardise how a functionality is used. Take a case of golang error interface.</p><pre><code class=\"language-golang\">type error interface {\n    Error() string\n}</code></pre><p>It is a builtin interface in the standard library that standardises the error behaviour. There have been other discussions in the community about standardising some behaviours, e.g. having a common <a href=\"https://groups.google.com/g/golang-dev/c/F3l9Iz1JX4g/discussion\" rel=\"noopener\">logging</a> interface. Similarly, it might be useful to have a company wide representation of some common behaviour to provide uniformity and code reusability. However, in this scenario, make sure that the interfaces are small, meaning 1–2 methods.</p><h4 id=\"should-this-package-return-an-interface-rather-than-the-concrete-type\">Should this package return an interface rather than the concrete type?</h4><p>According to CodeReviewComments, <a href=\"https://github.com/golang/go/wiki/CodeReviewComments#interfaces\" rel=\"noopener\">Go interfaces generally belong in the package that uses values of the interface type, not the package that implements those values.</a></p><p>However <a href=\"https://golang.org/doc/effective_go#generality\" rel=\"noopener\">Effective go</a> docs also complements it by saying that</p><blockquote>if a type exists only to implement an interface and will never have exported methods beyond that interface, there is no need to export the type itself.</blockquote><p>But the question is how do you identify such scenarios? How do you know that the type will have no additional value in the future? In my experience, the answer is to “wait”. Do not start off by returning interfaces, but wait till your code evolves and you see the need for them. As Rob Pike says:</p><blockquote>“Don’t design with interfaces, discover them.”</blockquote><p>A good hint for exposing an interface is <strong>when you have multiple types in your package implementing the same method signature</strong>. If you look at <code>http</code> package in standard library, you’ll see that it internally has multiple implementations of <code>http.Handler</code> interface.</p><p>In case of confusion, it is helpful to look for some <strong>red flags that can signals that you’re probably using interfaces wrong</strong>. Some are:</p><h4 id=\"your-interface-is-not-decoupling-an-api-from-change\">Your interface is not decoupling an API from change.</h4><p>Imagine an implementation of a third party api client package for sending emails e.g. <a href=\"https://sendgrid.com/\" rel=\"noopener\">SendGrid</a></p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/ad2e1cdcc74a388c579309d3f36a4d45.js\"></script><!--kg-card-end: html--><p>Now imagine, is there ever going to be a new implementation of <code>SendGrid</code> ?<br>There is most probably going to be a new implementation of a different email sending service in case SendGrid is replaced with let’s say, <code>Mailjet</code> . Hence you can just rewrite the code to return the concrete type.</p><p>Also imagine that you want to add a new method to this interface that’s used by lots of people, how do you add a new method to it without breaking their code?</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/cc45793074d1bcb22077cc1610b8d003.js\"></script><!--kg-card-end: html--><p>By exposing the struct type itself, you can add new methods to struct itself, without intensive refactoring</p><h4 id=\"your-interface-has-more-than-1-or-2-methods\">Your interface has more than 1 or 2 methods.</h4><p>Having too many methods for your interface reduces its usability. Taking example of <code>fmt.Stringer</code> interface, it has only one method signature, i.e.</p><p>type Stringer interface {     String() string}</p><p>and is used in 30 places in the standard library, excluding tests. You can find similar other examples in standard library like <code>http.Handler</code> , <code>io.Reader</code> etc.</p><p>Now imagine an interface with 5–6 methods, it cannot really be used at more than 1 or 2 places, as not all types can manage to implement large number of methods.</p><blockquote><a href=\"https://www.youtube.com/watch?v=PAAkCSZUG1c&amp;t=5m17s\" rel=\"noopener\">The bigger the interface, the weaker the abstraction.</a></blockquote><h4 id=\"conclusion\">Conclusion:</h4><p>The interfaces in Golang can be hard to master, the general recommendation is to follow the rule of thumbs. In case of exceptions to the general rules, wait to see how your code evolves and make adaptations accordingly.</p>","comment_id":"60b81d4afda06e0001f8371f","plaintext":"\nIn my few years of using Golang, I’ve come across several discussions involving\nthe use of interfaces. The arguments range from:\n\n> Why are we not defining interfaces with the type definition like in typical\nstatic languages like C++, Java etc ?\n> Should this package export interface in combination with the exposed type\nimplementing the interface?\n> Should this function return an interface rather than the concrete type?\nLet’s try to address these questions one by one.\n\nWhy are we not defining interfaces with the type definition like in typical\nstatic languages like C++, Java etc ?\nIn languages like C++, Java, one needs to specify that a type implements an\ninterface like in the code given below:\n\nIn such languages, defining the interface for the Object enables the compiler to\nform a dispatch tables for the objects pointing to the functions.\n\nGo doesn’t have a traditional dispatch table, and can rely on the interface\nvalues during a method dispatch. It’s literally more of a freestyle dispatcher\nmechanism that requires some work during interface value assignment — it\ngenerates a tiny lookup hash-table for the concrete type it’s pointing to. Here\n[https://www.airs.com/blog/archives/277] is a great blog post in case you want\nfurther reading.\n\nEven though a bit more expensive, this enables go to have a cleaner type system\nwithout the baggage of defining the interface for every type.\n\nFor a developer, this means that the object implementing the interface does not\nneed to explicitly say it implements it, as shown in the code below:\n\nThus any struct can satisfy an interface simply by implementing its method\nsignatures. It offers several advantages like:\n\n * Makes it easier to use mocks instead of real objects in unit tests.\n * Helps enforce decoupling between parts of your codebase.\n\nShould this package export interface in combination with the exposed type\nimplementing the interface?\nThe short answer is:\n\n> Don’t export any interfaces unless you have to.\nIf the consumer of your package requires some level of “inversion of control”,\nthey can define the interface in their own scope.\n\nHowever, there might be scenarios where one might want to standardise how a\nfunctionality is used. Take a case of golang error interface.\n\ntype error interface {\n    Error() string\n}\n\nIt is a builtin interface in the standard library that standardises the error\nbehaviour. There have been other discussions in the community about\nstandardising some behaviours, e.g. having a common logging\n[https://groups.google.com/g/golang-dev/c/F3l9Iz1JX4g/discussion] interface.\nSimilarly, it might be useful to have a company wide representation of some\ncommon behaviour to provide uniformity and code reusability. However, in this\nscenario, make sure that the interfaces are small, meaning 1–2 methods.\n\nShould this package return an interface rather than the concrete type?\nAccording to CodeReviewComments, Go interfaces generally belong in the package\nthat uses values of the interface type, not the package that implements those\nvalues. [https://github.com/golang/go/wiki/CodeReviewComments#interfaces]\n\nHowever Effective go [https://golang.org/doc/effective_go#generality] docs also\ncomplements it by saying that\n\n> if a type exists only to implement an interface and will never have exported\nmethods beyond that interface, there is no need to export the type itself.\nBut the question is how do you identify such scenarios? How do you know that the\ntype will have no additional value in the future? In my experience, the answer\nis to “wait”. Do not start off by returning interfaces, but wait till your code\nevolves and you see the need for them. As Rob Pike says:\n\n> “Don’t design with interfaces, discover them.”\nA good hint for exposing an interface is when you have multiple types in your\npackage implementing the same method signature. If you look at http package in\nstandard library, you’ll see that it internally has multiple implementations of \nhttp.Handler interface.\n\nIn case of confusion, it is helpful to look for some red flags that can signals\nthat you’re probably using interfaces wrong. Some are:\n\nYour interface is not decoupling an API from change.\nImagine an implementation of a third party api client package for sending emails\ne.g. SendGrid [https://sendgrid.com/]\n\nNow imagine, is there ever going to be a new implementation of SendGrid ?\nThere is most probably going to be a new implementation of a different email\nsending service in case SendGrid is replaced with let’s say, Mailjet . Hence you\ncan just rewrite the code to return the concrete type.\n\nAlso imagine that you want to add a new method to this interface that’s used by\nlots of people, how do you add a new method to it without breaking their code?\n\nBy exposing the struct type itself, you can add new methods to struct itself,\nwithout intensive refactoring\n\nYour interface has more than 1 or 2 methods.\nHaving too many methods for your interface reduces its usability. Taking example\nof fmt.Stringer interface, it has only one method signature, i.e.\n\ntype Stringer interface {     String() string}\n\nand is used in 30 places in the standard library, excluding tests. You can find\nsimilar other examples in standard library like http.Handler , io.Reader etc.\n\nNow imagine an interface with 5–6 methods, it cannot really be used at more than\n1 or 2 places, as not all types can manage to implement large number of methods.\n\n> The bigger the interface, the weaker the abstraction.\n[https://www.youtube.com/watch?v=PAAkCSZUG1c&t=5m17s]\nConclusion:\nThe interfaces in Golang can be hard to master, the general recommendation is to\nfollow the rule of thumbs. In case of exceptions to the general rules, wait to\nsee how your code evolves and make adaptations accordingly.","feature_image":"__GHOST_URL__/content/images/2022/10/Interface.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T00:07:38.000Z","updated_at":"2022-10-26T04:33:22.000Z","published_at":"2022-10-03T00:18:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b84fa0fda06e0001f8375c","uuid":"31bf5935-ca90-4ec5-8ae5-8e8410d67eb5","title":"Run nginx as unprivileged user in Docker container on Kubernetes","slug":"run-nginx-as-unprivileged-user-in-docker-container-on-kubernetes","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/bba7b60dcce2d6f9e93c56c4ec09d2ca.js\\\"></script>\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/9030fcceafbcba549e99c20f8f5e8e6a.js\\\"></script>\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/7c9a5740c74fe02b3d3d6a5e041c5d60.js\\\"></script>\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/42b14b052d77d37201064382b4c7e3ab.js\\\"></script>\"}],[\"code\",{\"code\":\"helm install nginx https://github.com/Harsimran1/nginx-k8s-unprivileged/archive/v0.1.0.tar.gz\",\"language\":\"shell\"}]],\"markups\":[[\"a\",[\"href\",\"https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b\"]],[\"a\",[\"href\",\"https://helm.sh/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://hub.docker.com/r/nginxinc/nginx-unprivileged\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://github.com/nginxinc/docker-nginx-unprivileged\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://github.com/nginxinc/docker-nginx-unprivileged/blob/994b117bef62b0a24e925169a219013f09793704/stable/alpine/Dockerfile#L104\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"http://nginx.org/en/docs/ngx_core_module.html#user\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/service/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[],0,\"While on production, it is important to run your containers as non root users to avoid any security vulnerabilities.\"]]],[1,\"h3\",[[0,[],0,\"The rationale\"]]],[1,\"p\",[[0,[],0,\"From security perspective, running a process on container as root user is as bad as running a process as root on host machine itself. If a user manages to break out of an application running as root in a container, he may be able to gain access to the host with the same root user. More information about this can be found at this interesting \"],[0,[0],1,\"blog post\"],[0,[],0,\".\"]]],[1,\"h3\",[[0,[],0,\"Prerequisites\"]]],[1,\"p\",[[0,[],0,\"We will be using \"],[0,[1],1,\"helm\"],[0,[],0,\" for deploying our nginx application on kubernetes.\"]]],[1,\"h3\",[[0,[],0,\"Create helm charts\"]]],[1,\"p\",[[0,[],0,\"The first thing we need to create is a deployment.yaml. The most basic deployment.yaml as given on \"],[0,[2],1,\"official kubernetes doc\"],[0,[],0,\"s looks like this. Here we have used the \"],[0,[3],1,\"nginx unprivileged docker image\"],[0,[],0,\" instead of the nginx image. We can trust this image because it is also maintained and published by nginxinc.\"]]],[10,0],[1,\"p\",[[0,[],0,\"If you’re not using custom config files for nginx, that should be all, in order to use this image. But, since in most cases, we need to use custom configs to be mounted on our pods, the deployment.yaml would look like this.\"]]],[10,1],[1,\"p\",[[0,[],0,\"In this case, we need to make sure our configs are using the right defaults as mentioned in Readme of the \"],[0,[4],1,\"unprivileged docker image\"],[0,[],0,\" we’re using. That basically means changing default values of some \"],[0,[5],1,\"directives\"],[0,[],0,\" . Our basic configmap will look like below. The comments in example mentions the changed values.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Notice that we’re not using any \"],[0,[6],1,\"user\"],[0,[],0,\" directive in our config.\"]]],[1,\"p\",[[0,[],0,\"Now we will create a \"],[0,[7],1,\"service\"],[0,[],0,\" to direct traffic to our pods created from deployment.yaml. Create a basic service as mentioned in \"],[0,[8],1,\"kubernetes documentation\"],[0,[],0,\".\"]]],[10,3],[1,\"p\",[[0,[],0,\"The service will now target port 8080 on the pods labelled nginx.\"]]],[1,\"p\",[[0,[],0,\"You can try and test out the deployment using\"]]],[10,4],[1,\"p\",[[1,[],0,0]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p>While on production, it is important to run your containers as non root users to avoid any security vulnerabilities.</p><h3 id=\"the-rationale\">The rationale</h3><p>From security perspective, running a process on container as root user is as bad as running a process as root on host machine itself. If a user manages to break out of an application running as root in a container, he may be able to gain access to the host with the same root user. More information about this can be found at this interesting <a href=\"https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b\">blog post</a>.</p><h3 id=\"prerequisites\">Prerequisites</h3><p>We will be using <a href=\"https://helm.sh/\" rel=\"noopener\">helm</a> for deploying our nginx application on kubernetes.</p><h3 id=\"create-helm-charts\">Create helm charts</h3><p>The first thing we need to create is a deployment.yaml. The most basic deployment.yaml as given on <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment\" rel=\"noopener\">official kubernetes doc</a>s looks like this. Here we have used the <a href=\"https://hub.docker.com/r/nginxinc/nginx-unprivileged\" rel=\"noopener\">nginx unprivileged docker image</a> instead of the nginx image. We can trust this image because it is also maintained and published by nginxinc.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/bba7b60dcce2d6f9e93c56c4ec09d2ca.js\"></script><!--kg-card-end: html--><p>If you’re not using custom config files for nginx, that should be all, in order to use this image. But, since in most cases, we need to use custom configs to be mounted on our pods, the deployment.yaml would look like this.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/9030fcceafbcba549e99c20f8f5e8e6a.js\"></script><!--kg-card-end: html--><p>In this case, we need to make sure our configs are using the right defaults as mentioned in Readme of the <a href=\"https://github.com/nginxinc/docker-nginx-unprivileged\" rel=\"noopener\">unprivileged docker image</a> we’re using. That basically means changing default values of some <a href=\"https://github.com/nginxinc/docker-nginx-unprivileged/blob/994b117bef62b0a24e925169a219013f09793704/stable/alpine/Dockerfile#L104\" rel=\"noopener\">directives</a> . Our basic configmap will look like below. The comments in example mentions the changed values.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/7c9a5740c74fe02b3d3d6a5e041c5d60.js\"></script><!--kg-card-end: html--><p>Notice that we’re not using any <a href=\"http://nginx.org/en/docs/ngx_core_module.html#user\" rel=\"noopener\">user</a> directive in our config.</p><p>Now we will create a <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\" rel=\"noopener\">service</a> to direct traffic to our pods created from deployment.yaml. Create a basic service as mentioned in <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service\" rel=\"noopener\">kubernetes documentation</a>.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/42b14b052d77d37201064382b4c7e3ab.js\"></script><!--kg-card-end: html--><p>The service will now target port 8080 on the pods labelled nginx.</p><p>You can try and test out the deployment using</p><pre><code class=\"language-shell\">helm install nginx https://github.com/Harsimran1/nginx-k8s-unprivileged/archive/v0.1.0.tar.gz</code></pre>","comment_id":"60b84fa0fda06e0001f8375c","plaintext":"\nWhile on production, it is important to run your containers as non root users to\navoid any security vulnerabilities.\n\nThe rationale\nFrom security perspective, running a process on container as root user is as bad\nas running a process as root on host machine itself. If a user manages to break\nout of an application running as root in a container, he may be able to gain\naccess to the host with the same root user. More information about this can be\nfound at this interesting blog post\n[https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b]\n.\n\nPrerequisites\nWe will be using helm [https://helm.sh/] for deploying our nginx application on\nkubernetes.\n\nCreate helm charts\nThe first thing we need to create is a deployment.yaml. The most basic\ndeployment.yaml as given on official kubernetes doc\n[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment]\ns looks like this. Here we have used the nginx unprivileged docker image\n[https://hub.docker.com/r/nginxinc/nginx-unprivileged] instead of the nginx\nimage. We can trust this image because it is also maintained and published by\nnginxinc.\n\nIf you’re not using custom config files for nginx, that should be all, in order\nto use this image. But, since in most cases, we need to use custom configs to be\nmounted on our pods, the deployment.yaml would look like this.\n\nIn this case, we need to make sure our configs are using the right defaults as\nmentioned in Readme of the unprivileged docker image\n[https://github.com/nginxinc/docker-nginx-unprivileged] we’re using. That\nbasically means changing default values of some directives\n[https://github.com/nginxinc/docker-nginx-unprivileged/blob/994b117bef62b0a24e925169a219013f09793704/stable/alpine/Dockerfile#L104] \n. Our basic configmap will look like below. The comments in example mentions the\nchanged values.\n\nNotice that we’re not using any user\n[http://nginx.org/en/docs/ngx_core_module.html#user] directive in our config.\n\nNow we will create a service\n[https://kubernetes.io/docs/concepts/services-networking/service/] to direct\ntraffic to our pods created from deployment.yaml. Create a basic service as\nmentioned in kubernetes documentation\n[https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service]\n.\n\nThe service will now target port 8080 on the pods labelled nginx.\n\nYou can try and test out the deployment using\n\nhelm install nginx https://github.com/Harsimran1/nginx-k8s-unprivileged/archive/v0.1.0.tar.gz","feature_image":"__GHOST_URL__/content/images/2021/06/1520171840913.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T03:42:24.000Z","updated_at":"2022-03-29T01:21:08.000Z","published_at":"2021-06-03T03:51:44.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b8524afda06e0001f8377d","uuid":"037e8e35-cd09-4215-816e-41ff3ffcf455","title":"How to optimise docker images for lesser build size and time.","slug":"optimise-docker-images-for-lesser-build-size-and-time","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/1200/1*0pnRn_E-js3KtefQ9XRV_A.png\",\"alt\":\"\",\"title\":\"\",\"cardWidth\":\"wide\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/1200/1*hYmxJVg7VXYF0lvDwSN4gw.png\",\"alt\":\"\",\"title\":\"\",\"cardWidth\":\"wide\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/1200/1*30I6G3sttfdTpUCIFwOAlw.png\",\"alt\":\"\",\"title\":\"\",\"cardWidth\":\"wide\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/1200/1*3qCkKTifzkXO1c1p-Jwhag.png\",\"alt\":\"\",\"title\":\"\",\"cardWidth\":\"wide\"}]],\"markups\":[[\"strong\"],[\"code\"]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[],0,\"Docker, as we all know, is the most popular tool for creating containerised applications these days. The more I use it, the more I fall in love with the concept. As easy as it is for even the beginners to write a Dockerfile, some small things can help you to optimise your images to a large extent. Following are few very useful tips which I have found helpful personally.\"]]],[1,\"h4\",[[0,[0],1,\"Understand Docker Layering.\"]]],[1,\"p\",[[0,[],0,\"The key point to keep in mind is that newer versions of Docker creates layers on \"],[0,[1],1,\"RUN\"],[0,[],0,\", \"],[0,[1],1,\"ADD\"],[0,[],0,\" and \"],[0,[1],1,\"COPY\"],[0,[],0,\" commands. So try to minimise the layers by keeping these three commands to minimum. Look at the below code samples and you’ll figure out which is more optimum.\"]]],[10,0],[1,\"p\",[[0,[],0,\"Just putting all the \"],[0,[1],1,\"RUN\"],[0,[],0,\" commands into a single command in this scenario, we are reducing a layer and hence build size.\"]]],[1,\"h4\",[[0,[0],1,\"Understand Docker Caching.\"]]],[1,\"p\",[[0,[],0,\"The thing to keep in mind here is that docker caching happens sequentially from top to bottom. If there is a change in one of the commands in the sequence, the cache after that command is discarded and every command that follows will be executed again.\"]]],[1,\"p\",[[0,[],0,\"I was working the other day with a Dockerfile which took annoyingly long to build. I looked at it and realised that just shuffling the docker commands could save me my sanity.\"]]],[1,\"p\",[[0,[],0,\"Below are the two versions:\"]]],[10,1],[1,\"p\",[[0,[],0,\"Here, the issue is the placement of \"],[0,[1],1,\"COPY\"],[0,[],0,\" command. Since the code in our working directory (docker context) is the one which will most likely be changed, we need to put it somewhere at the bottom. After moving it below the \"],[0,[1],1,\"RUN\"],[0,[],0,\" command, the \"],[0,[1],1,\"RUN\"],[0,[],0,\" command will not run every time but will be taken from cache, hence reducing the build time to a large extent.\"]]],[1,\"h4\",[[0,[0],1,\"Use Multistage Builds\"]]],[1,\"p\",[[0,[],0,\"You can use multistage builds to keep your final image as lean as possible. The most common use case is doing all necessary steps to build your binary in an intermediate stage and copying only your binary in the final stage. It will also help you avoid unwanted security vulnerabilities.\"]]],[10,2],[1,\"p\",[[0,[],0,\"You can specify the target stage in your docker command if you don’t want to build subsequent docker stages using \"],[0,[1],1,\"docker build --target builder .\"],[0,[],0,\" Here \"],[0,[1],1,\"builder\"],[0,[],0,\" is the name of our first stage. Specifying \"],[0,[1],1,\"target\"],[0,[],0,\" is specifically useful while running a test stage. By doing so, you will not unnecessarily build your code if tests doesn’t pass for your code.\"]]],[1,\"h4\",[[0,[],0,\"Use Smaller Base Images\"]]],[1,\"p\",[[0,[],0,\"Last, but the most important, keep your base images small. This can have a huge impact on your build size, time and of course image security.\"]]],[10,3],[1,\"p\",[[0,[],0,\"The size difference difference between the 2 images created is around 30MB since \"],[0,[1],1,\"scratch\"],[0,[],0,\" is smaller than \"],[0,[1],1,\"golang:1.12\"],[0,[],0,\" image.\"]]],[1,\"p\",[[0,[],0,\"I hope the above points help you as much as they have helped me. Keep building !!!!\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p>Docker, as we all know, is the most popular tool for creating containerised applications these days. The more I use it, the more I fall in love with the concept. As easy as it is for even the beginners to write a Dockerfile, some small things can help you to optimise your images to a large extent. Following are few very useful tips which I have found helpful personally.</p><h4 id=\"understand-docker-layering\"><strong>Understand Docker Layering.</strong></h4><p>The key point to keep in mind is that newer versions of Docker creates layers on <code>RUN</code>, <code>ADD</code> and <code>COPY</code> commands. So try to minimise the layers by keeping these three commands to minimum. Look at the below code samples and you’ll figure out which is more optimum.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://cdn-images-1.medium.com/max/1200/1*0pnRn_E-js3KtefQ9XRV_A.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Just putting all the <code>RUN</code> commands into a single command in this scenario, we are reducing a layer and hence build size.</p><h4 id=\"understand-docker-caching\"><strong>Understand Docker Caching.</strong></h4><p>The thing to keep in mind here is that docker caching happens sequentially from top to bottom. If there is a change in one of the commands in the sequence, the cache after that command is discarded and every command that follows will be executed again.</p><p>I was working the other day with a Dockerfile which took annoyingly long to build. I looked at it and realised that just shuffling the docker commands could save me my sanity.</p><p>Below are the two versions:</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://cdn-images-1.medium.com/max/1200/1*hYmxJVg7VXYF0lvDwSN4gw.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Here, the issue is the placement of <code>COPY</code> command. Since the code in our working directory (docker context) is the one which will most likely be changed, we need to put it somewhere at the bottom. After moving it below the <code>RUN</code> command, the <code>RUN</code> command will not run every time but will be taken from cache, hence reducing the build time to a large extent.</p><h4 id=\"use-multistage-builds\"><strong>Use Multistage Builds</strong></h4><p>You can use multistage builds to keep your final image as lean as possible. The most common use case is doing all necessary steps to build your binary in an intermediate stage and copying only your binary in the final stage. It will also help you avoid unwanted security vulnerabilities.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://cdn-images-1.medium.com/max/1200/1*30I6G3sttfdTpUCIFwOAlw.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>You can specify the target stage in your docker command if you don’t want to build subsequent docker stages using <code>docker build --target builder .</code> Here <code>builder</code> is the name of our first stage. Specifying <code>target</code> is specifically useful while running a test stage. By doing so, you will not unnecessarily build your code if tests doesn’t pass for your code.</p><h4 id=\"use-smaller-base-images\">Use Smaller Base Images</h4><p>Last, but the most important, keep your base images small. This can have a huge impact on your build size, time and of course image security.</p><figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"https://cdn-images-1.medium.com/max/1200/1*3qCkKTifzkXO1c1p-Jwhag.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>The size difference difference between the 2 images created is around 30MB since <code>scratch</code> is smaller than <code>golang:1.12</code> image.</p><p>I hope the above points help you as much as they have helped me. Keep building !!!!</p>","comment_id":"60b8524afda06e0001f8377d","plaintext":"\nDocker, as we all know, is the most popular tool for creating containerised\napplications these days. The more I use it, the more I fall in love with the\nconcept. As easy as it is for even the beginners to write a Dockerfile, some\nsmall things can help you to optimise your images to a large extent. Following\nare few very useful tips which I have found helpful personally.\n\nUnderstand Docker Layering.\nThe key point to keep in mind is that newer versions of Docker creates layers on \nRUN, ADD and COPY commands. So try to minimise the layers by keeping these three\ncommands to minimum. Look at the below code samples and you’ll figure out which\nis more optimum.\n\nJust putting all the RUN commands into a single command in this scenario, we are\nreducing a layer and hence build size.\n\nUnderstand Docker Caching.\nThe thing to keep in mind here is that docker caching happens sequentially from\ntop to bottom. If there is a change in one of the commands in the sequence, the\ncache after that command is discarded and every command that follows will be\nexecuted again.\n\nI was working the other day with a Dockerfile which took annoyingly long to\nbuild. I looked at it and realised that just shuffling the docker commands could\nsave me my sanity.\n\nBelow are the two versions:\n\nHere, the issue is the placement of COPY command. Since the code in our working\ndirectory (docker context) is the one which will most likely be changed, we need\nto put it somewhere at the bottom. After moving it below the RUN command, the \nRUN command will not run every time but will be taken from cache, hence reducing\nthe build time to a large extent.\n\nUse Multistage Builds\nYou can use multistage builds to keep your final image as lean as possible. The\nmost common use case is doing all necessary steps to build your binary in an\nintermediate stage and copying only your binary in the final stage. It will also\nhelp you avoid unwanted security vulnerabilities.\n\nYou can specify the target stage in your docker command if you don’t want to\nbuild subsequent docker stages using docker build --target builder . Here \nbuilder is the name of our first stage. Specifying target is specifically useful\nwhile running a test stage. By doing so, you will not unnecessarily build your\ncode if tests doesn’t pass for your code.\n\nUse Smaller Base Images\nLast, but the most important, keep your base images small. This can have a huge\nimpact on your build size, time and of course image security.\n\nThe size difference difference between the 2 images created is around 30MB since \nscratch is smaller than golang:1.12 image.\n\nI hope the above points help you as much as they have helped me. Keep building\n!!!!","feature_image":"__GHOST_URL__/content/images/2021/06/docker.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T03:53:46.000Z","updated_at":"2022-03-29T01:21:27.000Z","published_at":"2020-10-16T03:55:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b85376fda06e0001f8378d","uuid":"67850225-ac02-48f4-abb7-8288578b9d95","title":"Connect to Raspberry Pi 3 in headless mode.","slug":"connect-to-raspberry-pi-3-in-headless-mode","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}]],\"cards\":[[\"code\",{\"code\":\"$ unzip 2018-11-13-raspbian-stretch.zip\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ diskutil list\",\"language\":\"bash\"}],[\"code\",{\"code\":\"/dev/disk2 (external, physical):\\n   #:     TYPE NAME                   SIZE       IDENTIFIER\\n   0:     FDisk_partition_scheme      *15.9 GB    disk2\\n   1:     Windows_FAT_32 boot         46.0 MB     disk2s1\\n   2:     Linux                       15.9 GB     disk2s2\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ sudo dd bs=1m if=raspbian-stretch.img of=/dev/disk2\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ cd /Volumes/boot\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ touch ssh\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ touch wpa_supplicant.conf\"}],[\"code\",{\"code\":\"country=IN\\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\\nupdate_config=1\\n\\nnetwork={\\n    ssid=\\\"NETWORK-NAME\\\"\\n    psk=\\\"NETWORK-PASSWORD\\\"\\n}\",\"language\":\"bash\"}],[\"code\",{\"code\":\"$ ssh pi@raspberrypi.local\",\"language\":\"bash\"}],[\"code\",{\"code\":\"pi@raspberrypi:~ $ passwd\",\"language\":\"bash\"}]],\"markups\":[[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://www.raspberrypi.org/downloads/raspbian/\",\"rel\",\"noopener\"]],[\"code\"],[\"a\",[\"href\",\"http://www.linfo.org/mount_point.html\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://desertbot.io/blog/headless-raspberry-pi-3-bplus-ssh-wifi-setup\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://stackoverflow.com/questions/44205431/how-to-install-raspbian-to-sd-card-on-mac\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[],0,\"Imagine you have a Raspberry Pi and a lazy ass, or you just don’t have a monitor and keyboard to connect with your Raspberry Pi, but you still want to be able to work with it, or you just want to use it as all things embedded (like a robot), this article is for you. If you’re fond of command line, you can run your Raspberry Pi in headless mode, i.e. without a monitor or keyboard.\"]]],[1,\"p\",[[0,[],0,\"Other than Raspberry Pi 3, you will need a power source, SD Card and a computer( with MacOS is our case ).\"]]],[1,\"p\",[[0,[],0,\"First things first, connect the Raspberry Pi 3 to power source. The good thing about Raspberry Pi 3 is that it comes with a WiFi and we will use it to connect it with our computer. If you don’t have an OS already installed on your SD card, do the following:\"]]],[1,\"p\",[[0,[0],1,\"Installing OS\"]]],[1,\"p\",[[0,[1],1,\"Note that the commands listed below are for Unix based systems.\"]]],[3,\"ol\",[[[0,[],0,\"Make sure your SD card is inserted to your computer.\"]],[[0,[],0,\"Go to \"],[0,[2],1,\"https://www.raspberrypi.org/downloads/raspbian/\"],[0,[],0,\" and download the latest Raspbian image. If you want to use it in embedded systems, it is recommended to use \"],[0,[3],1,\"lite\"],[0,[],0,\" image.\"]],[[0,[],0,\"Unzip the image. For command line, use:\"]]]],[10,0],[1,\"p\",[[0,[],0,\"3. Check out the partition of your mounted SD Card using \"],[0,[3],1,\"diskutil\"],[0,[],0,\" :\"]]],[10,1],[1,\"p\",[[0,[],0,\"The output should contain (external, physical) disk if you have SD card inserted in your computer.\"]]],[10,2],[1,\"p\",[[0,[],0,\"4. Note the address of partition (external, physical disk). In this case, it is \"],[0,[3],1,\"/dev/disk2\"]]],[1,\"p\",[[0,[],0,\"5. Run the command to burn your unzipped Raspbian image on SD card.\"]]],[10,3],[1,\"p\",[[0,[],0,\"It will take some time (maybe around 15 mins) to finish this command.\"]]],[1,\"h4\",[[0,[0],1,\"Connecting to \"],[0,[],0,\"WiFi\"]]],[1,\"p\",[[0,[],0,\"Now that we have Raspbian image on your SD card, we need to make some pre-configurations so that it connects to WiFi when you insert your card in Raspberry Pi 3.\"]]],[3,\"ol\",[[[0,[],0,\"Go to \"],[0,[3],1,\"/boot\"],[0,[],0,\" folder in /\"],[0,[3],1,\"Volumes\"],[0,[],0,\" directory. \"],[0,[3],1,\"/Volumes \"],[0,[],0,\"is the invisible directory used by OS X and macOS as the OS’s default \"],[0,[4],1,\"mount point\"],[0,[],0,\" for accessing the filesystems of other storage (like SD cards, external hard drives, USB flash drives, mounted disk images, network fileshares, etc.)\"]]]],[10,4],[1,\"p\",[[0,[],0,\"2. Inside the boot directory, create a \"],[0,[3],1,\"ssh\"],[0,[],0,\" file. \"],[0,[3],1,\"touch\"],[0,[],0,\" command creates an empty file with the given filename.\"]]],[10,5],[1,\"p\",[[0,[],0,\"3. The first step to connect to wireless network is having \"],[0,[1],1,\"wpa_supplicant\"],[0,[],0,\" obtain authentication from a WPA authenticator. In order to do this, \"],[0,[1],1,\"wpa_supplicant\"],[0,[],0,\" must be configured so that it will be able to submit the correct credentials to the authenticator. Create a \"],[0,[3],1,\"wpa_supplicant.conf\"],[0,[],0,\" file using\"]]],[10,6],[1,\"p\",[[0,[],0,\"4. Edit the file created above and add below information by adjusting country code, \"],[0,[3],1,\"NETWORK-NAME\"],[0,[],0,\", \"],[0,[3],1,\"NETWORK-PASSWORD\"],[0,[],0,\". \"],[0,[3],1,\"NETWORK-NAME\"],[0,[],0,\" and \"],[0,[3],1,\"NETWORK-PASSWORD \"],[0,[],0,\"are your WiFi name and password you use to connect your devices.\"]]],[10,7],[1,\"p\",[[0,[],0,\"Now insert the SD card in your Raspberry Pi . It will automatically connect to WiFi. Note that it will take around 60 sec to boot up the Pi. Once you boot up the SD card in the Raspberry Pi, the install process will also pull your customized \"],[0,[3],1,\"wpa_supplicant.conf\"],[0,[],0,\" file into the main system allowing for the Pi to hop on your network.\"]]],[1,\"h4\",[[0,[0],1,\"Connecting to Raspberry Pi over WiFi\"]]],[1,\"p\",[[0,[],0,\"Once your Raspberry Pi is connected to the same WiFi, you can simply ssh into it. The default username is \"],[0,[3],1,\"pi\"],[0,[],0,\" , default network name is \"],[0,[3],1,\"raspberrypi\"],[0,[],0,\" and default password is \"],[0,[3],1,\"raspberry\"],[0,[],0,\" .\"]]],[10,8],[1,\"p\",[[0,[],0,\"Type the password \"],[0,[3],1,\"raspberry\"],[0,[],0,\" when prompted.\"]]],[1,\"p\",[[0,[],0,\"Now you have successfully logged in into your Raspberry Pi, it is recommended to change password if you don’t want your Pi to be part of \"],[0,[3],1,\"botnet\"],[0,[],0,\" for illegal activities. You can do it by \"],[0,[3],1,\"passwd\"],[0,[],0,\" command.\"]]],[10,9],[1,\"p\",[[0,[],0,\"References:\"],[1,[],0,0],[0,[5],1,\"https://desertbot.io/blog/headless-raspberry-pi-3-bplus-ssh-wifi-setup\"]]],[1,\"p\",[[0,[6],1,\"https://stackoverflow.com/questions/44205431/how-to-install-raspbian-to-sd-card-on-mac\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p>Imagine you have a Raspberry Pi and a lazy ass, or you just don’t have a monitor and keyboard to connect with your Raspberry Pi, but you still want to be able to work with it, or you just want to use it as all things embedded (like a robot), this article is for you. If you’re fond of command line, you can run your Raspberry Pi in headless mode, i.e. without a monitor or keyboard.</p><p>Other than Raspberry Pi 3, you will need a power source, SD Card and a computer( with MacOS is our case ).</p><p>First things first, connect the Raspberry Pi 3 to power source. The good thing about Raspberry Pi 3 is that it comes with a WiFi and we will use it to connect it with our computer. If you don’t have an OS already installed on your SD card, do the following:</p><p><strong>Installing OS</strong></p><p><em>Note that the commands listed below are for Unix based systems.</em></p><ol><li>Make sure your SD card is inserted to your computer.</li><li>Go to <a href=\"https://www.raspberrypi.org/downloads/raspbian/\" rel=\"noopener\">https://www.raspberrypi.org/downloads/raspbian/</a> and download the latest Raspbian image. If you want to use it in embedded systems, it is recommended to use <code>lite</code> image.</li><li>Unzip the image. For command line, use:</li></ol><pre><code class=\"language-bash\">$ unzip 2018-11-13-raspbian-stretch.zip</code></pre><p>3. Check out the partition of your mounted SD Card using <code>diskutil</code> :</p><pre><code class=\"language-bash\">$ diskutil list</code></pre><p>The output should contain (external, physical) disk if you have SD card inserted in your computer.</p><pre><code class=\"language-bash\">/dev/disk2 (external, physical):\n   #:     TYPE NAME                   SIZE       IDENTIFIER\n   0:     FDisk_partition_scheme      *15.9 GB    disk2\n   1:     Windows_FAT_32 boot         46.0 MB     disk2s1\n   2:     Linux                       15.9 GB     disk2s2</code></pre><p>4. Note the address of partition (external, physical disk). In this case, it is <code>/dev/disk2</code></p><p>5. Run the command to burn your unzipped Raspbian image on SD card.</p><pre><code class=\"language-bash\">$ sudo dd bs=1m if=raspbian-stretch.img of=/dev/disk2</code></pre><p>It will take some time (maybe around 15 mins) to finish this command.</p><h4 id=\"connecting-to-wifi\"><strong>Connecting to </strong>WiFi</h4><p>Now that we have Raspbian image on your SD card, we need to make some pre-configurations so that it connects to WiFi when you insert your card in Raspberry Pi 3.</p><ol><li>Go to <code>/boot</code> folder in /<code>Volumes</code> directory. <code>/Volumes </code>is the invisible directory used by OS X and macOS as the OS’s default <a href=\"http://www.linfo.org/mount_point.html\" rel=\"noopener\">mount point</a> for accessing the filesystems of other storage (like SD cards, external hard drives, USB flash drives, mounted disk images, network fileshares, etc.)</li></ol><pre><code class=\"language-bash\">$ cd /Volumes/boot</code></pre><p>2. Inside the boot directory, create a <code>ssh</code> file. <code>touch</code> command creates an empty file with the given filename.</p><pre><code class=\"language-bash\">$ touch ssh</code></pre><p>3. The first step to connect to wireless network is having <em>wpa_supplicant</em> obtain authentication from a WPA authenticator. In order to do this, <em>wpa_supplicant</em> must be configured so that it will be able to submit the correct credentials to the authenticator. Create a <code>wpa_supplicant.conf</code> file using</p><pre><code>$ touch wpa_supplicant.conf</code></pre><p>4. Edit the file created above and add below information by adjusting country code, <code>NETWORK-NAME</code>, <code>NETWORK-PASSWORD</code>. <code>NETWORK-NAME</code> and <code>NETWORK-PASSWORD </code>are your WiFi name and password you use to connect your devices.</p><pre><code class=\"language-bash\">country=IN\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"NETWORK-NAME\"\n    psk=\"NETWORK-PASSWORD\"\n}</code></pre><p>Now insert the SD card in your Raspberry Pi . It will automatically connect to WiFi. Note that it will take around 60 sec to boot up the Pi. Once you boot up the SD card in the Raspberry Pi, the install process will also pull your customized <code>wpa_supplicant.conf</code> file into the main system allowing for the Pi to hop on your network.</p><h4 id=\"connecting-to-raspberry-pi-over-wifi\"><strong>Connecting to Raspberry Pi over WiFi</strong></h4><p>Once your Raspberry Pi is connected to the same WiFi, you can simply ssh into it. The default username is <code>pi</code> , default network name is <code>raspberrypi</code> and default password is <code>raspberry</code> .</p><pre><code class=\"language-bash\">$ ssh pi@raspberrypi.local</code></pre><p>Type the password <code>raspberry</code> when prompted.</p><p>Now you have successfully logged in into your Raspberry Pi, it is recommended to change password if you don’t want your Pi to be part of <code>botnet</code> for illegal activities. You can do it by <code>passwd</code> command.</p><pre><code class=\"language-bash\">pi@raspberrypi:~ $ passwd</code></pre><p>References:<br><a href=\"https://desertbot.io/blog/headless-raspberry-pi-3-bplus-ssh-wifi-setup\" rel=\"noopener\">https://desertbot.io/blog/headless-raspberry-pi-3-bplus-ssh-wifi-setup</a></p><p><a href=\"https://stackoverflow.com/questions/44205431/how-to-install-raspbian-to-sd-card-on-mac\" rel=\"noopener\">https://stackoverflow.com/questions/44205431/how-to-install-raspbian-to-sd-card-on-mac</a></p>","comment_id":"60b85376fda06e0001f8378d","plaintext":"\nImagine you have a Raspberry Pi and a lazy ass, or you just don’t have a monitor\nand keyboard to connect with your Raspberry Pi, but you still want to be able to\nwork with it, or you just want to use it as all things embedded (like a robot),\nthis article is for you. If you’re fond of command line, you can run your\nRaspberry Pi in headless mode, i.e. without a monitor or keyboard.\n\nOther than Raspberry Pi 3, you will need a power source, SD Card and a computer(\nwith MacOS is our case ).\n\nFirst things first, connect the Raspberry Pi 3 to power source. The good thing\nabout Raspberry Pi 3 is that it comes with a WiFi and we will use it to connect\nit with our computer. If you don’t have an OS already installed on your SD card,\ndo the following:\n\nInstalling OS\n\nNote that the commands listed below are for Unix based systems.\n\n 1. Make sure your SD card is inserted to your computer.\n 2. Go to https://www.raspberrypi.org/downloads/raspbian/ and download the\n    latest Raspbian image. If you want to use it in embedded systems, it is\n    recommended to use lite image.\n 3. Unzip the image. For command line, use:\n\n$ unzip 2018-11-13-raspbian-stretch.zip\n\n3. Check out the partition of your mounted SD Card using diskutil :\n\n$ diskutil list\n\nThe output should contain (external, physical) disk if you have SD card inserted\nin your computer.\n\n/dev/disk2 (external, physical):\n   #:     TYPE NAME                   SIZE       IDENTIFIER\n   0:     FDisk_partition_scheme      *15.9 GB    disk2\n   1:     Windows_FAT_32 boot         46.0 MB     disk2s1\n   2:     Linux                       15.9 GB     disk2s2\n\n4. Note the address of partition (external, physical disk). In this case, it is \n/dev/disk2\n\n5. Run the command to burn your unzipped Raspbian image on SD card.\n\n$ sudo dd bs=1m if=raspbian-stretch.img of=/dev/disk2\n\nIt will take some time (maybe around 15 mins) to finish this command.\n\nConnecting to WiFi\nNow that we have Raspbian image on your SD card, we need to make some\npre-configurations so that it connects to WiFi when you insert your card in\nRaspberry Pi 3.\n\n 1. Go to /boot folder in /Volumes directory. /Volumes is the invisible\n    directory used by OS X and macOS as the OS’s default mount point\n    [http://www.linfo.org/mount_point.html] for accessing the filesystems of\n    other storage (like SD cards, external hard drives, USB flash drives,\n    mounted disk images, network fileshares, etc.)\n\n$ cd /Volumes/boot\n\n2. Inside the boot directory, create a ssh file. touch command creates an empty\nfile with the given filename.\n\n$ touch ssh\n\n3. The first step to connect to wireless network is having wpa_supplicant obtain\nauthentication from a WPA authenticator. In order to do this, wpa_supplicant \nmust be configured so that it will be able to submit the correct credentials to\nthe authenticator. Create a wpa_supplicant.conf file using\n\n$ touch wpa_supplicant.conf\n\n4. Edit the file created above and add below information by adjusting country\ncode, NETWORK-NAME, NETWORK-PASSWORD. NETWORK-NAME and NETWORK-PASSWORD are your\nWiFi name and password you use to connect your devices.\n\ncountry=IN\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"NETWORK-NAME\"\n    psk=\"NETWORK-PASSWORD\"\n}\n\nNow insert the SD card in your Raspberry Pi . It will automatically connect to\nWiFi. Note that it will take around 60 sec to boot up the Pi. Once you boot up\nthe SD card in the Raspberry Pi, the install process will also pull your\ncustomized wpa_supplicant.conf file into the main system allowing for the Pi to\nhop on your network.\n\nConnecting to Raspberry Pi over WiFi\nOnce your Raspberry Pi is connected to the same WiFi, you can simply ssh into\nit. The default username is pi , default network name is raspberrypi and default\npassword is raspberry .\n\n$ ssh pi@raspberrypi.local\n\nType the password raspberry when prompted.\n\nNow you have successfully logged in into your Raspberry Pi, it is recommended to\nchange password if you don’t want your Pi to be part of botnet for illegal\nactivities. You can do it by passwd command.\n\npi@raspberrypi:~ $ passwd\n\nReferences:\nhttps://desertbot.io/blog/headless-raspberry-pi-3-bplus-ssh-wifi-setup\n\nhttps://stackoverflow.com/questions/44205431/how-to-install-raspbian-to-sd-card-on-mac","feature_image":"__GHOST_URL__/content/images/2021/06/raspberry.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T03:58:46.000Z","updated_at":"2021-06-03T04:07:12.000Z","published_at":"2020-06-18T04:06:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b85707fda06e0001f837c9","uuid":"b9f97782-a15f-4380-a2a0-9d44fd5b899c","title":"Step by Step Guide: Host your own website.","slug":"step-by-step-guide-host-your-own-website","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*kKgKkSEsDMbfHVYAdNSnuQ.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*6amC779xu2WbqErkbL3lxw.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*3SWPQXC6IKuZWR1FPGy_Iw.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*WpKpdwwrqSQFe9LDKk5DEQ.png\",\"alt\":\"\",\"title\":\"\",\"caption\":\"Example CNAME entry for godaddy.com\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/22bed16c4f4416cda1b5fb4346b76f7d.js\\\"></script>\"}]],\"markups\":[[\"a\",[\"href\",\"https://www.eresume.tech\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://www.godaddy.com/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://www.godaddy.com/garage/how-to-buy-a-domain-name/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://www.heroku.com/\",\"rel\",\"noopener\"]],[\"strong\"],[\"a\",[\"href\",\"https://aws.amazon.com/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://azure.microsoft.com/en-us/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://in.godaddy.com/help/add-a-cname-record-19236\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://letsencrypt.org/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://letsencrypt.org/getting-started/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://www.nginx.com/\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://hub.docker.com/_/nginx\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://mediatemple.net/community/products/developer/204405534/install-nginx-on-ubuntu\",\"rel\",\"noopener\"]],[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"In this technical era, everyone has at least once in their life thought about creating a website and showcasing it to the world. Is it your pet project? A business idea you want to test? Or just a skill you you want to acquire for future use? A lot of non technical people use Wordpress, Wix or other providers to host their content, but if you need your own domain, you would have to pay for it.\"]]],[1,\"p\",[[0,[],0,\"Below is the step by step guide of how we hosted our \"],[0,[0],1,\"website\"],[0,[],0,\" from scratch.\"],[1,[],0,0],[0,[],0,\"Let’s start.\"]]],[1,\"h4\",[[0,[],0,\"Get a domain\"]]],[1,\"p\",[[0,[],0,\"First step is to buy a domain from a domain registrar. Domain registrar are basically middlemen that manages the domain name registration process with the respective registries like .com, .in, .ca etc. This is also a place where you can check the possible domains available and do the cost vs name analysis as shown in the image below. The domain registrar we used is \"],[0,[1],1,\"Godaddy.com\"],[0,[],0,\". You can find more information \"],[0,[2],1,\"here\"],[0,[],0,\" about how to buy a domain.\"]]],[10,0],[1,\"h4\",[[0,[],0,\"Get a server to host the application/ website.\"]]],[1,\"p\",[[0,[],0,\"Now that we have defined how people are going to reach us, we need to find a host where that specific domain should be pointed to. A host can be any machine connected to internet that runs your application/ hosts your content. However it is a full time effort to ensure 24/7 availability, if you’re running this machine yourself. This is where cloud services come into place. A cloud server is basically a computer that runs somewhere on the internet, but some one else is taking the responsibility ensure 24/7 availability.\"],[1,[],0,1],[0,[],0,\"There are several cloud service platforms available.\"]]],[1,\"p\",[[0,[3,4],2,\"Heroku\"],[0,[],0,\": A developer friendly web hosting platform, which offers you a ready-to-use environment that allows you to deploy your code fast and is initially free of cost. But might ask you to pay when you start using additional addons like TLS certificate setup, databases etc. Nevertheless a good option if you just want to totally outsource the deployment and operations of your project. However when you scale, there is a high probability that you would want to move to other hosting platforms because of increasing costs.\"]]],[1,\"p\",[[0,[5,4],2,\"AWS\"],[0,[4],1,\": \"],[0,[],0,\"Even though the deployment process is a bit complicated than heroku, it is everyone’s best friend so far. It was the first major Infrastructure as a service (IaaS) provider and holds the biggest market share. AWS will provide instances and services for you to manage. There are free EC2 microservers available with an option to scale on demand if you want to test your application.\"]]],[1,\"p\",[[0,[6,4],2,\"Azure\"],[0,[],0,\": A new guy(pretty old now) in town, that comes from Microsoft, however doesn’t have option of free microservers. It caters better for larger organizations already committed to Microsoft products moving an existing infrastructure to the cloud.\"]]],[1,\"p\",[[0,[],0,\"Since we didn’t want to pay money to heroku for setting up TLS certificates for our website, we moved to AWS. One can create an account and get a free EC2 server for one year.\"]]],[10,1],[1,\"p\",[[0,[],0,\"After getting the servers, one needs to change the default security settings to open http ports 8080 (for http) and 443 (for https). More info \"],[0,[7],1,\"here\"],[0,[],0,\".\"]]],[10,2],[1,\"p\",[[0,[],0,\"Once you have your EC2 Instance running, it is time to point your domain name to your host. Copy the \"],[0,[4],1,\"Public DNS\"],[0,[],0,\" value of your EC2 Instance and add a DNS entry for \"],[0,[4],1,\"type: CNAME, name: www\"],[0,[],0,\" in your domain name provider. In \"],[0,[1],1,\"godaddy.com\"],[0,[],0,\", the values would look like below. More information \"],[0,[8],1,\"here\"],[0,[],0,\".\"]]],[10,3],[1,\"h4\",[[0,[],0,\"Setup TLS Certificate\"]]],[1,\"p\",[[0,[],0,\"On web, it is important that your website has a TLS certificate for 2 reasons:\"],[1,[],0,2],[0,[],0,\"1. Encrypt information sent over internet.\"],[1,[],0,3],[0,[],0,\"2. Provide identity assurance, both of which help online consumers to positively identify and trust websites that are safe to transact with.\"]]],[1,\"p\",[[0,[],0,\"One can get free TLS certificate using Non Profit Certificate Authority(CA) \"],[0,[9],1,\"Let’s Encrypt\"],[0,[],0,\". The instructions for getting certificate can be found \"],[0,[10],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"We used \"],[0,[4],1,\"certbot\"],[0,[],0,\", an open source command line ACME client for the process of receiving certificate, as mentioned in the article. One you pass the ACME challenge to verify you are the owner of the domain, you will get 2 files with keys: \"],[0,[4],1,\"fullkey.pem\"],[0,[],0,\" and \"],[0,[4],1,\"privkey.pem\"],[0,[],0,\". Make sure to use these files for ssl_certificate on your api gateway as described in the next step.\"]]],[1,\"h4\",[[0,[],0,\"Setup Api Gateway\"]]],[1,\"p\",[[0,[],0,\"API Gateway is used for following concerns:\"],[1,[],0,4],[0,[],0,\"1. Separate out cross cutting concerns e.g. authentication, ssl termination, DDOS protection/ throttling from the actual services.\"],[1,[],0,5],[0,[],0,\"2. Routing the api requests to a specific application in case you have multiple components running that are responsible for a specific function. (Microservices based architecture).\"]]],[1,\"p\",[[0,[11],1,\"Nginx\"],[0,[],0,\" is a great open source software that is lightweight, performant, easy to setup and provides the functionality of API Gateway and Load Balancing (in case you have multiple servers running your application). You can use docker to run nginx on your host using the instructions \"],[0,[12],1,\"here\"],[0,[],0,\". Note that docker is used just to containerise(recommended) your nginx application, you can also install nginx on your host manually using the steps \"],[0,[13],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"The nginx.conf file needs to be updated with the ssl certificate lines as described below.\"]]],[10,4],[1,\"p\",[[0,[],0,\"Note that the cert files referred in directive \"],[0,[14],1,\"ssl_certificate\"],[0,[],0,\" and \"],[0,[14],1,\"ssl_certificate_key\"],[0,[],0,\" were generated in the previous steps and should exist / copied over to the host.\"]]],[1,\"p\",[[0,[4],1,\"Start the application/ Add the content on the host.\"],[1,[],0,6],[0,[],0,\" The above configuration will look for an index file in \"],[0,[14],1,\"/sites/default\"],[0,[],0,\" directory. One can change the root path to point to a separate directory where your content is placed. In case one has an application running on some port, say 8080, of the host, one can uncomment the last \"],[0,[14],1,\"proxy_pass http://127.0.0.1:8080\"],[0,[],0,\" directive and comment the previous lines in location directive. Pointing to a specific IP and Port is a very common practice when one is running in micro-services based or containerised architecture.\"]]],[1,\"p\",[[0,[],0,\"Now that you type your domain name in the browser, you should be able to get results content from your application.\"]]],[1,\"p\",[[0,[],0,\"Hope that above guide acted as a helpful first step. Happy Hosting!\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>In this technical era, everyone has at least once in their life thought about creating a website and showcasing it to the world. Is it your pet project? A business idea you want to test? Or just a skill you you want to acquire for future use? A lot of non technical people use Wordpress, Wix or other providers to host their content, but if you need your own domain, you would have to pay for it.</p><p>Below is the step by step guide of how we hosted our <a href=\"https://www.eresume.tech\" rel=\"noopener\">website</a> from scratch.<br>Let’s start.</p><h4 id=\"get-a-domain\">Get a domain</h4><p>First step is to buy a domain from a domain registrar. Domain registrar are basically middlemen that manages the domain name registration process with the respective registries like .com, .in, .ca etc. This is also a place where you can check the possible domains available and do the cost vs name analysis as shown in the image below. The domain registrar we used is <a href=\"https://www.godaddy.com/\" rel=\"noopener\">Godaddy.com</a>. You can find more information <a href=\"https://www.godaddy.com/garage/how-to-buy-a-domain-name/\" rel=\"noopener\">here</a> about how to buy a domain.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*kKgKkSEsDMbfHVYAdNSnuQ.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><h4 id=\"get-a-server-to-host-the-application-website\">Get a server to host the application/ website.</h4><p>Now that we have defined how people are going to reach us, we need to find a host where that specific domain should be pointed to. A host can be any machine connected to internet that runs your application/ hosts your content. However it is a full time effort to ensure 24/7 availability, if you’re running this machine yourself. This is where cloud services come into place. A cloud server is basically a computer that runs somewhere on the internet, but some one else is taking the responsibility ensure 24/7 availability.<br>There are several cloud service platforms available.</p><p><a href=\"https://www.heroku.com/\" rel=\"noopener\"><strong>Heroku</strong></a>: A developer friendly web hosting platform, which offers you a ready-to-use environment that allows you to deploy your code fast and is initially free of cost. But might ask you to pay when you start using additional addons like TLS certificate setup, databases etc. Nevertheless a good option if you just want to totally outsource the deployment and operations of your project. However when you scale, there is a high probability that you would want to move to other hosting platforms because of increasing costs.</p><p><a href=\"https://aws.amazon.com/\" rel=\"noopener\"><strong>AWS</strong></a><strong>: </strong>Even though the deployment process is a bit complicated than heroku, it is everyone’s best friend so far. It was the first major Infrastructure as a service (IaaS) provider and holds the biggest market share. AWS will provide instances and services for you to manage. There are free EC2 microservers available with an option to scale on demand if you want to test your application.</p><p><a href=\"https://azure.microsoft.com/en-us/\" rel=\"noopener\"><strong>Azure</strong></a>: A new guy(pretty old now) in town, that comes from Microsoft, however doesn’t have option of free microservers. It caters better for larger organizations already committed to Microsoft products moving an existing infrastructure to the cloud.</p><p>Since we didn’t want to pay money to heroku for setting up TLS certificates for our website, we moved to AWS. One can create an account and get a free EC2 server for one year.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*6amC779xu2WbqErkbL3lxw.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>After getting the servers, one needs to change the default security settings to open http ports 8080 (for http) and 443 (for https). More info <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/\" rel=\"noopener\">here</a>.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*3SWPQXC6IKuZWR1FPGy_Iw.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Once you have your EC2 Instance running, it is time to point your domain name to your host. Copy the <strong>Public DNS</strong> value of your EC2 Instance and add a DNS entry for <strong>type: CNAME, name: www</strong> in your domain name provider. In <a href=\"https://www.godaddy.com/\" rel=\"noopener\">godaddy.com</a>, the values would look like below. More information <a href=\"https://in.godaddy.com/help/add-a-cname-record-19236\" rel=\"noopener\">here</a>.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://cdn-images-1.medium.com/max/800/1*WpKpdwwrqSQFe9LDKk5DEQ.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Example CNAME entry for godaddy.com</figcaption></figure><h4 id=\"setup-tls-certificate\">Setup TLS Certificate</h4><p>On web, it is important that your website has a TLS certificate for 2 reasons:<br>1. Encrypt information sent over internet.<br>2. Provide identity assurance, both of which help online consumers to positively identify and trust websites that are safe to transact with.</p><p>One can get free TLS certificate using Non Profit Certificate Authority(CA) <a href=\"https://letsencrypt.org/\" rel=\"noopener\">Let’s Encrypt</a>. The instructions for getting certificate can be found <a href=\"https://letsencrypt.org/getting-started/\" rel=\"noopener\">here</a>.</p><p>We used <strong>certbot</strong>, an open source command line ACME client for the process of receiving certificate, as mentioned in the article. One you pass the ACME challenge to verify you are the owner of the domain, you will get 2 files with keys: <strong>fullkey.pem</strong> and <strong>privkey.pem</strong>. Make sure to use these files for ssl_certificate on your api gateway as described in the next step.</p><h4 id=\"setup-api-gateway\">Setup Api Gateway</h4><p>API Gateway is used for following concerns:<br>1. Separate out cross cutting concerns e.g. authentication, ssl termination, DDOS protection/ throttling from the actual services.<br>2. Routing the api requests to a specific application in case you have multiple components running that are responsible for a specific function. (Microservices based architecture).</p><p><a href=\"https://www.nginx.com/\" rel=\"noopener\">Nginx</a> is a great open source software that is lightweight, performant, easy to setup and provides the functionality of API Gateway and Load Balancing (in case you have multiple servers running your application). You can use docker to run nginx on your host using the instructions <a href=\"https://hub.docker.com/_/nginx\" rel=\"noopener\">here</a>. Note that docker is used just to containerise(recommended) your nginx application, you can also install nginx on your host manually using the steps <a href=\"https://mediatemple.net/community/products/developer/204405534/install-nginx-on-ubuntu\" rel=\"noopener\">here</a>.</p><p>The nginx.conf file needs to be updated with the ssl certificate lines as described below.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/22bed16c4f4416cda1b5fb4346b76f7d.js\"></script><!--kg-card-end: html--><p>Note that the cert files referred in directive <code>ssl_certificate</code> and <code>ssl_certificate_key</code> were generated in the previous steps and should exist / copied over to the host.</p><p><strong>Start the application/ Add the content on the host.</strong><br> The above configuration will look for an index file in <code>/sites/default</code> directory. One can change the root path to point to a separate directory where your content is placed. In case one has an application running on some port, say 8080, of the host, one can uncomment the last <code>proxy_pass http://127.0.0.1:8080</code> directive and comment the previous lines in location directive. Pointing to a specific IP and Port is a very common practice when one is running in micro-services based or containerised architecture.</p><p>Now that you type your domain name in the browser, you should be able to get results content from your application.</p><p>Hope that above guide acted as a helpful first step. Happy Hosting!</p>","comment_id":"60b85707fda06e0001f837c9","plaintext":"In this technical era, everyone has at least once in their life thought about\ncreating a website and showcasing it to the world. Is it your pet project? A\nbusiness idea you want to test? Or just a skill you you want to acquire for\nfuture use? A lot of non technical people use Wordpress, Wix or other providers\nto host their content, but if you need your own domain, you would have to pay\nfor it.\n\nBelow is the step by step guide of how we hosted our website\n[https://www.eresume.tech] from scratch.\nLet’s start.\n\nGet a domain\nFirst step is to buy a domain from a domain registrar. Domain registrar are\nbasically middlemen that manages the domain name registration process with the\nrespective registries like .com, .in, .ca etc. This is also a place where you\ncan check the possible domains available and do the cost vs name analysis as\nshown in the image below. The domain registrar we used is Godaddy.com\n[https://www.godaddy.com/]. You can find more information here\n[https://www.godaddy.com/garage/how-to-buy-a-domain-name/] about how to buy a\ndomain.\n\nGet a server to host the application/ website.\nNow that we have defined how people are going to reach us, we need to find a\nhost where that specific domain should be pointed to. A host can be any machine\nconnected to internet that runs your application/ hosts your content. However it\nis a full time effort to ensure 24/7 availability, if you’re running this\nmachine yourself. This is where cloud services come into place. A cloud server\nis basically a computer that runs somewhere on the internet, but some one else\nis taking the responsibility ensure 24/7 availability.\nThere are several cloud service platforms available.\n\nHeroku [https://www.heroku.com/]: A developer friendly web hosting platform,\nwhich offers you a ready-to-use environment that allows you to deploy your code\nfast and is initially free of cost. But might ask you to pay when you start\nusing additional addons like TLS certificate setup, databases etc. Nevertheless\na good option if you just want to totally outsource the deployment and\noperations of your project. However when you scale, there is a high probability\nthat you would want to move to other hosting platforms because of increasing\ncosts.\n\nAWS [https://aws.amazon.com/]: Even though the deployment process is a bit\ncomplicated than heroku, it is everyone’s best friend so far. It was the first\nmajor Infrastructure as a service (IaaS) provider and holds the biggest market\nshare. AWS will provide instances and services for you to manage. There are free\nEC2 microservers available with an option to scale on demand if you want to test\nyour application.\n\nAzure [https://azure.microsoft.com/en-us/]: A new guy(pretty old now) in town,\nthat comes from Microsoft, however doesn’t have option of free microservers. It\ncaters better for larger organizations already committed to Microsoft products\nmoving an existing infrastructure to the cloud.\n\nSince we didn’t want to pay money to heroku for setting up TLS certificates for\nour website, we moved to AWS. One can create an account and get a free EC2\nserver for one year.\n\nAfter getting the servers, one needs to change the default security settings to\nopen http ports 8080 (for http) and 443 (for https). More info here\n[https://aws.amazon.com/premiumsupport/knowledge-center/connect-http-https-ec2/]\n.\n\nOnce you have your EC2 Instance running, it is time to point your domain name to\nyour host. Copy the Public DNS value of your EC2 Instance and add a DNS entry\nfor type: CNAME, name: www in your domain name provider. In godaddy.com\n[https://www.godaddy.com/], the values would look like below. More information \nhere [https://in.godaddy.com/help/add-a-cname-record-19236].\n\nExample CNAME entry for godaddy.comSetup TLS Certificate\nOn web, it is important that your website has a TLS certificate for 2 reasons:\n1. Encrypt information sent over internet.\n2. Provide identity assurance, both of which help online consumers to positively\nidentify and trust websites that are safe to transact with.\n\nOne can get free TLS certificate using Non Profit Certificate Authority(CA) \nLet’s Encrypt [https://letsencrypt.org/]. The instructions for getting\ncertificate can be found here [https://letsencrypt.org/getting-started/].\n\nWe used certbot, an open source command line ACME client for the process of\nreceiving certificate, as mentioned in the article. One you pass the ACME\nchallenge to verify you are the owner of the domain, you will get 2 files with\nkeys: fullkey.pem and privkey.pem. Make sure to use these files for\nssl_certificate on your api gateway as described in the next step.\n\nSetup Api Gateway\nAPI Gateway is used for following concerns:\n1. Separate out cross cutting concerns e.g. authentication, ssl termination,\nDDOS protection/ throttling from the actual services.\n2. Routing the api requests to a specific application in case you have multiple\ncomponents running that are responsible for a specific function. (Microservices\nbased architecture).\n\nNginx [https://www.nginx.com/] is a great open source software that is\nlightweight, performant, easy to setup and provides the functionality of API\nGateway and Load Balancing (in case you have multiple servers running your\napplication). You can use docker to run nginx on your host using the\ninstructions here [https://hub.docker.com/_/nginx]. Note that docker is used\njust to containerise(recommended) your nginx application, you can also install\nnginx on your host manually using the steps here\n[https://mediatemple.net/community/products/developer/204405534/install-nginx-on-ubuntu]\n.\n\nThe nginx.conf file needs to be updated with the ssl certificate lines as\ndescribed below.\n\nNote that the cert files referred in directive ssl_certificate and \nssl_certificate_key were generated in the previous steps and should exist /\ncopied over to the host.\n\nStart the application/ Add the content on the host.\nThe above configuration will look for an index file in /sites/default directory.\nOne can change the root path to point to a separate directory where your content\nis placed. In case one has an application running on some port, say 8080, of the\nhost, one can uncomment the last proxy_pass http://127.0.0.1:8080 directive and\ncomment the previous lines in location directive. Pointing to a specific IP and\nPort is a very common practice when one is running in micro-services based or\ncontainerised architecture.\n\nNow that you type your domain name in the browser, you should be able to get\nresults content from your application.\n\nHope that above guide acted as a helpful first step. Happy Hosting!","feature_image":"__GHOST_URL__/content/images/2021/06/host.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T04:13:59.000Z","updated_at":"2021-06-03T04:17:32.000Z","published_at":"2021-01-20T04:15:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b858b6fda06e0001f837dd","uuid":"7485e181-1958-4788-a11b-2ec81ff835c3","title":"Load testing tool comparison: Siege vs Apache JMeter.","slug":"load-testing-tool-comparison-siege-vs-apache-jmeter","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"code\",{\"code\":\"siege -c5 -t1m -d3 https://your.server.api\",\"language\":\"bash\"}],[\"code\",{\"code\":\"siege -c100 -r1 -H ‘content-type: application/json’ -f siege.txt\",\"language\":\"bash\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*dxRk2cyy-5x7wbPUjp8tXQ.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*SWX_23pgllvYvTE3QiGybQ.png\",\"alt\":\"\",\"title\":\"\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://github.com/JoeDog/siege\",\"rel\",\"noopener\"]],[\"em\"],[\"a\",[\"href\",\"https://blog.worldline.tech/2018/01/29/keepalive.html\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"h3\",[[0,[],0,\"Why load test?\"]]],[1,\"p\",[[0,[],0,\"Imagine a scenario where people are waiting at the airport to do the COVID-19 test before they could get home. And the only way they can register for that test is by using your app. Now imagine their frustration if your app fails to load, or some of your apis fail because they are not performant enough.\"]]],[1,\"p\",[[0,[],0,\"Had you had load tested your services, you would have an idea how much load your services could handle and what steps you would need to take in order to enable your services to handle bigger load while optimising the cost of resources. Remember, the solution for every performance problem might not always be to increase the number of servers, at least not always the optimum and a cost effective solution.\"]]],[1,\"h3\",[[0,[0],1,\"How?\"]]],[1,\"p\",[[0,[],0,\"When I started load testing our apis at my company, we started using siege.\"]]],[1,\"h4\",[[0,[1],1,\"Siege\"]]],[1,\"p\",[[0,[],0,\"It is a simple \"],[0,[0],1,\"command line load testing tool\"],[0,[],0,\".\"]]],[1,\"blockquote\",[[0,[],0,\"It can stress test a single URL with a user defined number of simulated users, or it can read many URLs into memory and stress them simultaneously. The program reports the total number of hits recorded, bytes transferred, response time, concurrency, and return status.\"]]],[1,\"p\",[[0,[],0,\"It is a great tool if you want to get a general idea of how your apis behave, without having to invest a lot of time.\"]]],[1,\"p\",[[0,[],0,\"Below are some \"],[0,[0],1,\"advantages\"],[0,[],0,\" from a practical perspective.\"]]],[3,\"ul\",[[[0,[],0,\"Lightweight and easy to get started with. One can simply download the tool and start testing their apis using simple command like:\"]]]],[10,0],[1,\"p\",[[0,[],0,\"The above command triggers 5 users making the requests to the server for a time period of 1 minute, with a delay of 3 seconds between requests.\"]]],[3,\"ul\",[[[0,[],0,\"Supports triggering of multiple concurrent requests with different URLs or different request bodies using a file.\"]]]],[10,1],[1,\"p\",[[0,[],0,\"The above command reads the first 100 api requests from siege.txt file in memory and fire them simultaneously single time.\"]]],[1,\"p\",[[0,[],0,\"This is how a sample result may look like:\"]]],[10,2],[1,\"p\",[[0,[],0,\"However there are some \"],[0,[0],1,\"pitfalls\"],[0,[],0,\" one needs to be aware of:\"]]],[3,\"ul\",[[[0,[],0,\"Siege results also account for network latency, i.e the time it took for your requests to reach the server. Hence it is only useful to test your apis locally or get a general idea about the behaviour and relative (compared to other apis on the same server) throughput.\"]],[[0,[],0,\"It only supports HTTP/1.0 and 1.1 protocols, the GET and POST directives, cookies, transaction logging, and basic authentication. If one plans to go beyond that for their tests, one needs to consider a more advanced tool.\"]]]],[1,\"h4\",[[0,[],0,\"Apache jMeter\"]]],[1,\"p\",[[0,[],0,\"The above limitation prompted us to switch to \"],[0,[0],1,\"jMeter\"],[0,[],0,\", as most other open source load testing tools don’t support PUT, DELETE apis. Apache jMeter is a GUI based load testing tool based on Java. It offers a lot of flexibility with what you can test, and how you can interpret results.\"]]],[1,\"p\",[[0,[],0,\"But as with all things that offer more, it can be confusing to get started with (\"],[0,[2],1,\"don’t quote me on that\"],[0,[],0,\"), and might take some time to do the initial on boarding. Some of the \"],[0,[0],1,\"advantages\"],[0,[],0,\" are:\"]]],[3,\"ul\",[[[0,[],0,\"Allows to get more accurate test results without accounting for network latency. By turning on KeepAlive option in HTTP requests, one can keep the same TCP connection open and hence calculate the real throughput of your server. More on it \"],[0,[3],1,\"here\"],[0,[],0,\".\"]],[[0,[],0,\"It offers multiple ways to analyse your results.\"]],[[0,[],0,\"It has extension capabilities using plugins. e.g. creating random numbers\"]]]],[1,\"p\",[[0,[],0,\"This is how a sample aggregate report from jMeter looks like:\"]]],[10,3],[1,\"h4\",[[0,[],0,\"Conclusion:\"]]],[1,\"p\",[[0,[],0,\"If you’re looking for a getting a general idea for your api behaviour and have very less time, siege is a good option. However if you want to do some advanced testing and have some time to invest for initial learning, go ahead and use jMeter.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3 id=\"why-load-test\">Why load test?</h3><p>Imagine a scenario where people are waiting at the airport to do the COVID-19 test before they could get home. And the only way they can register for that test is by using your app. Now imagine their frustration if your app fails to load, or some of your apis fail because they are not performant enough.</p><p>Had you had load tested your services, you would have an idea how much load your services could handle and what steps you would need to take in order to enable your services to handle bigger load while optimising the cost of resources. Remember, the solution for every performance problem might not always be to increase the number of servers, at least not always the optimum and a cost effective solution.</p><h3 id=\"how\"><strong>How?</strong></h3><p>When I started load testing our apis at my company, we started using siege.</p><h4 id=\"siege\"><a href=\"https://github.com/JoeDog/siege\" rel=\"noopener\">Siege</a></h4><p>It is a simple <strong>command line load testing tool</strong>.</p><blockquote>It can stress test a single URL with a user defined number of simulated users, or it can read many URLs into memory and stress them simultaneously. The program reports the total number of hits recorded, bytes transferred, response time, concurrency, and return status.</blockquote><p>It is a great tool if you want to get a general idea of how your apis behave, without having to invest a lot of time.</p><p>Below are some <strong>advantages</strong> from a practical perspective.</p><ul><li>Lightweight and easy to get started with. One can simply download the tool and start testing their apis using simple command like:</li></ul><pre><code class=\"language-bash\">siege -c5 -t1m -d3 https://your.server.api</code></pre><p>The above command triggers 5 users making the requests to the server for a time period of 1 minute, with a delay of 3 seconds between requests.</p><ul><li>Supports triggering of multiple concurrent requests with different URLs or different request bodies using a file.</li></ul><pre><code class=\"language-bash\">siege -c100 -r1 -H ‘content-type: application/json’ -f siege.txt</code></pre><p>The above command reads the first 100 api requests from siege.txt file in memory and fire them simultaneously single time.</p><p>This is how a sample result may look like:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*dxRk2cyy-5x7wbPUjp8tXQ.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>However there are some <strong>pitfalls</strong> one needs to be aware of:</p><ul><li>Siege results also account for network latency, i.e the time it took for your requests to reach the server. Hence it is only useful to test your apis locally or get a general idea about the behaviour and relative (compared to other apis on the same server) throughput.</li><li>It only supports HTTP/1.0 and 1.1 protocols, the GET and POST directives, cookies, transaction logging, and basic authentication. If one plans to go beyond that for their tests, one needs to consider a more advanced tool.</li></ul><h4 id=\"apache-jmeter\">Apache jMeter</h4><p>The above limitation prompted us to switch to <strong>jMeter</strong>, as most other open source load testing tools don’t support PUT, DELETE apis. Apache jMeter is a GUI based load testing tool based on Java. It offers a lot of flexibility with what you can test, and how you can interpret results.</p><p>But as with all things that offer more, it can be confusing to get started with (<em>don’t quote me on that</em>), and might take some time to do the initial on boarding. Some of the <strong>advantages</strong> are:</p><ul><li>Allows to get more accurate test results without accounting for network latency. By turning on KeepAlive option in HTTP requests, one can keep the same TCP connection open and hence calculate the real throughput of your server. More on it <a href=\"https://blog.worldline.tech/2018/01/29/keepalive.html\" rel=\"noopener\">here</a>.</li><li>It offers multiple ways to analyse your results.</li><li>It has extension capabilities using plugins. e.g. creating random numbers</li></ul><p>This is how a sample aggregate report from jMeter looks like:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*SWX_23pgllvYvTE3QiGybQ.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><h4 id=\"conclusion\">Conclusion:</h4><p>If you’re looking for a getting a general idea for your api behaviour and have very less time, siege is a good option. However if you want to do some advanced testing and have some time to invest for initial learning, go ahead and use jMeter.</p>","comment_id":"60b858b6fda06e0001f837dd","plaintext":"Why load test?\nImagine a scenario where people are waiting at the airport to do the COVID-19\ntest before they could get home. And the only way they can register for that\ntest is by using your app. Now imagine their frustration if your app fails to\nload, or some of your apis fail because they are not performant enough.\n\nHad you had load tested your services, you would have an idea how much load your\nservices could handle and what steps you would need to take in order to enable\nyour services to handle bigger load while optimising the cost of resources.\nRemember, the solution for every performance problem might not always be to\nincrease the number of servers, at least not always the optimum and a cost\neffective solution.\n\nHow?\nWhen I started load testing our apis at my company, we started using siege.\n\nSiege [https://github.com/JoeDog/siege]\nIt is a simple command line load testing tool.\n\n> It can stress test a single URL with a user defined number of simulated users,\nor it can read many URLs into memory and stress them simultaneously. The program\nreports the total number of hits recorded, bytes transferred, response time,\nconcurrency, and return status.\nIt is a great tool if you want to get a general idea of how your apis behave,\nwithout having to invest a lot of time.\n\nBelow are some advantages from a practical perspective.\n\n * Lightweight and easy to get started with. One can simply download the tool\n   and start testing their apis using simple command like:\n\nsiege -c5 -t1m -d3 https://your.server.api\n\nThe above command triggers 5 users making the requests to the server for a time\nperiod of 1 minute, with a delay of 3 seconds between requests.\n\n * Supports triggering of multiple concurrent requests with different URLs or\n   different request bodies using a file.\n\nsiege -c100 -r1 -H ‘content-type: application/json’ -f siege.txt\n\nThe above command reads the first 100 api requests from siege.txt file in memory\nand fire them simultaneously single time.\n\nThis is how a sample result may look like:\n\nHowever there are some pitfalls one needs to be aware of:\n\n * Siege results also account for network latency, i.e the time it took for your\n   requests to reach the server. Hence it is only useful to test your apis\n   locally or get a general idea about the behaviour and relative (compared to\n   other apis on the same server) throughput.\n * It only supports HTTP/1.0 and 1.1 protocols, the GET and POST directives,\n   cookies, transaction logging, and basic authentication. If one plans to go\n   beyond that for their tests, one needs to consider a more advanced tool.\n\nApache jMeter\nThe above limitation prompted us to switch to jMeter, as most other open source\nload testing tools don’t support PUT, DELETE apis. Apache jMeter is a GUI based\nload testing tool based on Java. It offers a lot of flexibility with what you\ncan test, and how you can interpret results.\n\nBut as with all things that offer more, it can be confusing to get started with\n(don’t quote me on that), and might take some time to do the initial on\nboarding. Some of the advantages are:\n\n * Allows to get more accurate test results without accounting for network\n   latency. By turning on KeepAlive option in HTTP requests, one can keep the\n   same TCP connection open and hence calculate the real throughput of your\n   server. More on it here\n   [https://blog.worldline.tech/2018/01/29/keepalive.html].\n * It offers multiple ways to analyse your results.\n * It has extension capabilities using plugins. e.g. creating random numbers\n\nThis is how a sample aggregate report from jMeter looks like:\n\nConclusion:\nIf you’re looking for a getting a general idea for your api behaviour and have\nvery less time, siege is a good option. However if you want to do some advanced\ntesting and have some time to invest for initial learning, go ahead and use\njMeter.","feature_image":"__GHOST_URL__/content/images/2021/06/1_ynocDZtSxrmxLsG3GLBhpg.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T04:21:10.000Z","updated_at":"2021-06-03T04:23:20.000Z","published_at":"2021-03-24T04:23:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60b859a9fda06e0001f837f5","uuid":"5212230b-e24b-428a-907d-3f60db781b1e","title":"Gracefully shutdown Go API server connected to Database.","slug":"gracefully-shutdown-go-api-server-connected-to-database","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/8b81b6d3b591684a33098dfeff2d74a3.js\\\"></script>\"}],[\"code\",{\"code\":\"go func() {\\n    panic(srv.ListenAndServe())\\n}()\",\"language\":\"golang\"}],[\"code\",{\"code\":\"stop := make(chan os.Signal, 1)\\nsignal.Notify(stop, os.Interrupt)\\nsignal.Notify(stop, syscall.SIGTERM)\",\"language\":\"golang\"}],[\"code\",{\"code\":\"<-stop\",\"language\":\"golang\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/657166f32bc63c0c3994f88d6a935fb6.js\\\"></script>\"}]],\"markups\":[[\"code\"],[\"a\",[\"href\",\"https://godoc.org/net/http#Server.Shutdown\",\"rel\",\"nofollow noopener noopener\"]],[\"a\",[\"href\",\"https://godoc.org/database/sql#DB.Close\",\"rel\",\"nofollow noopener noopener\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Go offers scalable and robust mechanism for creating API servers. As most of APIs talk to some database, on a scaled infrastructure receiving a lot of requests per second, it is important to shutdown gracefully in order to prevent goroutine leaks. This means closing two things in our case.\"]]],[1,\"p\",[[0,[],0,\"HTTP Server and DB connection.\"]]],[1,\"p\",[[0,[],0,\"Go \"],[0,[0],1,\"net/http\"],[0,[],0,\" package offers \"],[0,[0],1,\"Shutdown\"],[0,[],0,\" function to gracefully shutdown your http server. \"],[0,[1],1,\"https://godoc.org/net/http#Server.Shutdown\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Go \"],[0,[0],1,\"database/sql\"],[0,[],0,\" package offers \"],[0,[0],1,\"Close\"],[0,[],0,\" function to gracefully close the connection to sql. \"],[0,[2],1,\"https://godoc.org/database/sql#DB.Close\"]]],[1,\"h4\",[[0,[],0,\"1) Create a minimal HTTP API service using a Database.\"]]],[10,0],[1,\"h4\",[[0,[],0,\"2) Change your server function “srv.ListenAndServe()” to run in a goroutine.\"]]],[10,1],[1,\"p\",[[0,[],0,\"This is done to ensure that \"],[0,[0],1,\"main\"],[0,[],0,\" will not exit when \"],[0,[0],1,\"srv.ListenAndServe() \"],[0,[],0,\"returns on stop signal and waits for database to be closed. Note that we’re using \"],[0,[0],1,\"panic\"],[0,[],0,\" rather than \"],[0,[0],1,\"log.Fatal \"],[0,[],0,\"to capture errors from \"],[0,[0],1,\"srv.ListenAndServe()\"],[0,[],0,\" in order to be able to call defer in next steps.\"]]],[1,\"h4\",[[0,[],0,\"3) Create a channel to receive stop signals.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Bonus point: If you’re using kubernetes, note that it sends \"],[0,[0],1,\"SIGTERM\"],[0,[],0,\" signal to its pods for shutting down. \"],[0,[0],1,\"Interrupt\"],[0,[],0,\" is normally sent\"]]],[1,\"h4\",[[0,[],0,\"4) Listen to the stop signals and handle the shutdown of your HTTP Server. You can use defer to close your db connection.\"]]],[10,3],[10,4],[1,\"p\",[[0,[],0,\"Note that we are shutting down HTTP server before closing db because server will need database to handle its API requests.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Go offers scalable and robust mechanism for creating API servers. As most of APIs talk to some database, on a scaled infrastructure receiving a lot of requests per second, it is important to shutdown gracefully in order to prevent goroutine leaks. This means closing two things in our case.</p><p>HTTP Server and DB connection.</p><p>Go <code>net/http</code> package offers <code>Shutdown</code> function to gracefully shutdown your http server. <a href=\"https://godoc.org/net/http#Server.Shutdown\" rel=\"nofollow noopener noopener\">https://godoc.org/net/http#Server.Shutdown</a>.</p><p>Go <code>database/sql</code> package offers <code>Close</code> function to gracefully close the connection to sql. <a href=\"https://godoc.org/database/sql#DB.Close\" rel=\"nofollow noopener noopener\">https://godoc.org/database/sql#DB.Close</a></p><h4 id=\"1-create-a-minimal-http-api-service-using-a-database\">1) Create a minimal HTTP API service using a Database.</h4><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/8b81b6d3b591684a33098dfeff2d74a3.js\"></script><!--kg-card-end: html--><h4 id=\"2-change-your-server-function-%E2%80%9Csrvlistenandserve%E2%80%9D-to-run-in-a-goroutine\">2) Change your server function “srv.ListenAndServe()” to run in a goroutine.</h4><pre><code class=\"language-golang\">go func() {\n    panic(srv.ListenAndServe())\n}()</code></pre><p>This is done to ensure that <code>main</code> will not exit when <code>srv.ListenAndServe() </code>returns on stop signal and waits for database to be closed. Note that we’re using <code>panic</code> rather than <code>log.Fatal </code>to capture errors from <code>srv.ListenAndServe()</code> in order to be able to call defer in next steps.</p><h4 id=\"3-create-a-channel-to-receive-stop-signals\">3) Create a channel to receive stop signals.</h4><pre><code class=\"language-golang\">stop := make(chan os.Signal, 1)\nsignal.Notify(stop, os.Interrupt)\nsignal.Notify(stop, syscall.SIGTERM)</code></pre><p>Bonus point: If you’re using kubernetes, note that it sends <code>SIGTERM</code> signal to its pods for shutting down. <code>Interrupt</code> is normally sent</p><h4 id=\"4-listen-to-the-stop-signals-and-handle-the-shutdown-of-your-http-server-you-can-use-defer-to-close-your-db-connection\">4) Listen to the stop signals and handle the shutdown of your HTTP Server. You can use defer to close your db connection.</h4><pre><code class=\"language-golang\">&lt;-stop</code></pre><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/657166f32bc63c0c3994f88d6a935fb6.js\"></script><!--kg-card-end: html--><p>Note that we are shutting down HTTP server before closing db because server will need database to handle its API requests.</p>","comment_id":"60b859a9fda06e0001f837f5","plaintext":"Go offers scalable and robust mechanism for creating API servers. As most of\nAPIs talk to some database, on a scaled infrastructure receiving a lot of\nrequests per second, it is important to shutdown gracefully in order to prevent\ngoroutine leaks. This means closing two things in our case.\n\nHTTP Server and DB connection.\n\nGo net/http package offers Shutdown function to gracefully shutdown your http\nserver. https://godoc.org/net/http#Server.Shutdown.\n\nGo database/sql package offers Close function to gracefully close the connection\nto sql. https://godoc.org/database/sql#DB.Close\n\n1) Create a minimal HTTP API service using a Database.\n2) Change your server function “srv.ListenAndServe()” to run in a goroutine.\ngo func() {\n    panic(srv.ListenAndServe())\n}()\n\nThis is done to ensure that main will not exit when srv.ListenAndServe() returns\non stop signal and waits for database to be closed. Note that we’re using panic \nrather than log.Fatal to capture errors from srv.ListenAndServe() in order to be\nable to call defer in next steps.\n\n3) Create a channel to receive stop signals.\nstop := make(chan os.Signal, 1)\nsignal.Notify(stop, os.Interrupt)\nsignal.Notify(stop, syscall.SIGTERM)\n\nBonus point: If you’re using kubernetes, note that it sends SIGTERM signal to\nits pods for shutting down. Interrupt is normally sent\n\n4) Listen to the stop signals and handle the shutdown of your HTTP Server. You\ncan use defer to close your db connection.\n<-stop\n\nNote that we are shutting down HTTP server before closing db because server will\nneed database to handle its API requests.","feature_image":"__GHOST_URL__/content/images/2021/06/install-latest-golang-centos7-ubuntu-18.04-01.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-03T04:25:13.000Z","updated_at":"2021-06-03T04:30:33.000Z","published_at":"2020-04-13T04:29:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60bbba10fda06e0001f83831","uuid":"7ba3114c-3780-469d-ac58-cdfbf13081b5","title":"My takeaways from Kent Beck's Test-Driven Development - Part I","slug":"my-takeaways-from-kent-becks-test-driven-development","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/06/image-1.png\",\"width\":791,\"height\":591,\"caption\":\"Test Driven Development Cycle\"}]],\"markups\":[[\"a\",[\"href\",\"https://amzn.to/3DMm1tV\"]],[\"strong\"],[\"em\"]],\"sections\":[[1,\"p\",[[0,[],0,\"I have been reading Kent Beck's Book: \"],[0,[0],1,\"TEST-DRIVEN DEVELOPMENT By Example\"],[0,[],0,\" for some time and am writing my takeaways from it.\"]]],[1,\"p\",[[0,[],0,\"Before saying anything, I would like to answer the question Why?  Why TDD at first place? It's a very BASIC question, but it's a general observation that people who often ask WHY tends to stay in the right direction rather than going down the rabbit hole without a sense of purpose at all. Hence, whatever your job may be, whether it's business, programming, following the rabbit to the hole or what not, never be afraid to ask WHY. Now you're wondering why I am talking about the rabbits when I came here to talk about Test Driven Development. Yes your're asking the right question now. WHY.\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"So why? \"]]],[3,\"ol\",[[[0,[1],1,\"It makes your code predictable\"],[0,[],0,\". Mr. Beck starts with an example where an external client comes to the engineering team who have been handling money in Dollars, if their code could handle multi-currency. And no one could confidently tell the client YES Or NO. Had they had tests for the code, they would know the answer. \"],[1,[],0,2]],[[0,[1],1,\"It makes extending the code easier\"],[0,[],0,\". When you're extending or refactoring a code, if you don't have tests, the confidence level that you've not broken something is quite low. The idea is that even if you don't have tests when you try to extend the code, you write the tests first. This will also improve your test coverage and increase your confidence level. Without tests you have to reason and decide whether you should refactor code in a certain way, with tests you can simply ask the computer.\"]],[[0,[1],1,\"It breaks down the problem into small manageable chunks\"],[0,[],0,\". While the above two points are valid for any kind of test strategy, the main advantage of test driven development is that it helps break down your problem into small manageable chunks. \"],[0,[2],1,\"Start small or not at all\"],[0,[],0,\". It enables you to solve complicated problems even if you're an average programmer like me. It gives you control over the size of the steps.\"]],[[0,[1],1,\"It helps build confidence towards your code and tests.\"],[0,[],0,\"  Every testing strategy comes with a baggage about how reliable is the test. Is the test passing even though the code is incorrect because there is a bug in the test? Well it's not very practical to write test for the test. Having a failing test first and passing it after writing the code gives you confidence that your code is the reason the test has passed. It is not full proof but it is better than writing tests after the code. \"]],[[0,[1],1,\"It helps in modularization of code.\"],[0,[],0,\"  When you're doing TDD, you are looking at how an external package/ party is going to use your code as a first step. You're also thinking of making your code testable early on. This results in better abstraction layers and module formations. \"]]]],[1,\"p\",[[0,[],0,\"Now that we're clear with the big WHY, Mr. Kent goes on to explain HOW in most part of the book with examples. Below are the key steps of TDD:\"]]],[10,0],[1,\"p\",[[0,[],0,\"Below are some things to keep in mind while doing TDD.\"]]],[1,\"p\",[[0,[],0,\"1. Formulate a TODO list of some basic requirements that your code should fulfill. Pick an item in the list and translate the requirement in the form of a failing test. Remember, don't write the code first, write the test first.\"]]],[1,\"p\",[[0,[],0,\"2. The key idea is to write small incremental tests. Don't write the most complicated test first. Start with the smallest manageable chunk and build on top of that. If a thought strikes about another new test while you're in the middle of a TDD cycle, write it as an item in the TODO list and continue writing and implementing the current cycle first.\"]]],[1,\"p\",[[0,[],0,\"3. One may come across some code ugliness that may come in forms of code duplication, unprotected methods etc. , specially during the step 3 of the TDD cycle. However don't be afraid of it. You're only supposed to show your code to your spouse, only if they are a bit tolerant like mine, until all the steps of TDD cycle are completed.\"]]],[1,\"p\",[[0,[],0,\"4. It can feel a bit restrictive to have only small incremental steps. Hence, to be practical during TDD, if you want to make minor code change that you know you'll eventually need to do anyways, and it will only cause a brief interruption to your flow, it makes sense to do the change. If a teeny tiny step feels restrictive, take bigger steps. If you feel unsure in big steps, take smaller steps.\"]]],[1,\"p\",[[0,[],0,\"5. There can also be exceptions where you write the code first before writing test when the risk of adding code is low and there is already a failing test that needs to be passed first. For example, one can argue to skip writing test for a log statement, because the \"]]],[3,\"ul\",[[[0,[],0,\"The risk of breaking something is low.\"]],[[0,[],0,\"The log actually shows what's happening in the code.\"],[1,[],0,3]]]],[1,\"p\",[[0,[],0,\"So far, the above mentioned points covers my understanding in terms of summary with the first half of the book. I'll write the summary of second half in the next post.\"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>I have been reading Kent Beck's Book: <a href=\"https://amzn.to/3DMm1tV\">TEST-DRIVEN DEVELOPMENT By Example</a> for some time and am writing my takeaways from it.</p><p>Before saying anything, I would like to answer the question Why?  Why TDD at first place? It's a very BASIC question, but it's a general observation that people who often ask WHY tends to stay in the right direction rather than going down the rabbit hole without a sense of purpose at all. Hence, whatever your job may be, whether it's business, programming, following the rabbit to the hole or what not, never be afraid to ask WHY. Now you're wondering why I am talking about the rabbits when I came here to talk about Test Driven Development. Yes your're asking the right question now. WHY.<br><br>So why? </p><ol><li><strong>It makes your code predictable</strong>. Mr. Beck starts with an example where an external client comes to the engineering team who have been handling money in Dollars, if their code could handle multi-currency. And no one could confidently tell the client YES Or NO. Had they had tests for the code, they would know the answer. <br></li><li><strong>It makes extending the code easier</strong>. When you're extending or refactoring a code, if you don't have tests, the confidence level that you've not broken something is quite low. The idea is that even if you don't have tests when you try to extend the code, you write the tests first. This will also improve your test coverage and increase your confidence level. Without tests you have to reason and decide whether you should refactor code in a certain way, with tests you can simply ask the computer.</li><li><strong>It breaks down the problem into small manageable chunks</strong>. While the above two points are valid for any kind of test strategy, the main advantage of test driven development is that it helps break down your problem into small manageable chunks. <em>Start small or not at all</em>. It enables you to solve complicated problems even if you're an average programmer like me. It gives you control over the size of the steps.</li><li><strong>It helps build confidence towards your code and tests.</strong>  Every testing strategy comes with a baggage about how reliable is the test. Is the test passing even though the code is incorrect because there is a bug in the test? Well it's not very practical to write test for the test. Having a failing test first and passing it after writing the code gives you confidence that your code is the reason the test has passed. It is not full proof but it is better than writing tests after the code. </li><li><strong>It helps in modularization of code.</strong>  When you're doing TDD, you are looking at how an external package/ party is going to use your code as a first step. You're also thinking of making your code testable early on. This results in better abstraction layers and module formations. </li></ol><p>Now that we're clear with the big WHY, Mr. Kent goes on to explain HOW in most part of the book with examples. Below are the key steps of TDD:</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2021/06/image-1.png\" class=\"kg-image\" alt loading=\"lazy\" width=\"791\" height=\"591\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/06/image-1.png 600w, __GHOST_URL__/content/images/2021/06/image-1.png 791w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Test Driven Development Cycle</figcaption></figure><p>Below are some things to keep in mind while doing TDD.</p><p>1. Formulate a TODO list of some basic requirements that your code should fulfill. Pick an item in the list and translate the requirement in the form of a failing test. Remember, don't write the code first, write the test first.</p><p>2. The key idea is to write small incremental tests. Don't write the most complicated test first. Start with the smallest manageable chunk and build on top of that. If a thought strikes about another new test while you're in the middle of a TDD cycle, write it as an item in the TODO list and continue writing and implementing the current cycle first.</p><p>3. One may come across some code ugliness that may come in forms of code duplication, unprotected methods etc. , specially during the step 3 of the TDD cycle. However don't be afraid of it. You're only supposed to show your code to your spouse, only if they are a bit tolerant like mine, until all the steps of TDD cycle are completed.</p><p>4. It can feel a bit restrictive to have only small incremental steps. Hence, to be practical during TDD, if you want to make minor code change that you know you'll eventually need to do anyways, and it will only cause a brief interruption to your flow, it makes sense to do the change. If a teeny tiny step feels restrictive, take bigger steps. If you feel unsure in big steps, take smaller steps.</p><p>5. There can also be exceptions where you write the code first before writing test when the risk of adding code is low and there is already a failing test that needs to be passed first. For example, one can argue to skip writing test for a log statement, because the </p><ul><li>The risk of breaking something is low.</li><li>The log actually shows what's happening in the code.<br></li></ul><p>So far, the above mentioned points covers my understanding in terms of summary with the first half of the book. I'll write the summary of second half in the next post.</p>","comment_id":"60bbba10fda06e0001f83831","plaintext":"I have been reading Kent Beck's Book: TEST-DRIVEN DEVELOPMENT By Example\n[https://amzn.to/3DMm1tV] for some time and am writing my takeaways from it.\n\nBefore saying anything, I would like to answer the question Why?  Why TDD at\nfirst place? It's a very BASIC question, but it's a general observation that\npeople who often ask WHY tends to stay in the right direction rather than going\ndown the rabbit hole without a sense of purpose at all. Hence, whatever your job\nmay be, whether it's business, programming, following the rabbit to the hole or\nwhat not, never be afraid to ask WHY. Now you're wondering why I am talking\nabout the rabbits when I came here to talk about Test Driven Development. Yes\nyour're asking the right question now. WHY.\n\nSo why? \n\n 1. It makes your code predictable. Mr. Beck starts with an example where an\n    external client comes to the engineering team who have been handling money\n    in Dollars, if their code could handle multi-currency. And no one could\n    confidently tell the client YES Or NO. Had they had tests for the code, they\n    would know the answer. \n    \n 2. It makes extending the code easier. When you're extending or refactoring a\n    code, if you don't have tests, the confidence level that you've not broken\n    something is quite low. The idea is that even if you don't have tests when\n    you try to extend the code, you write the tests first. This will also\n    improve your test coverage and increase your confidence level. Without tests\n    you have to reason and decide whether you should refactor code in a certain\n    way, with tests you can simply ask the computer.\n 3. It breaks down the problem into small manageable chunks. While the above two\n    points are valid for any kind of test strategy, the main advantage of test\n    driven development is that it helps break down your problem into small\n    manageable chunks. Start small or not at all. It enables you to solve\n    complicated problems even if you're an average programmer like me. It gives\n    you control over the size of the steps.\n 4. It helps build confidence towards your code and tests. Every testing\n    strategy comes with a baggage about how reliable is the test. Is the test\n    passing even though the code is incorrect because there is a bug in the\n    test? Well it's not very practical to write test for the test. Having a\n    failing test first and passing it after writing the code gives you\n    confidence that your code is the reason the test has passed. It is not full\n    proof but it is better than writing tests after the code. \n 5. It helps in modularization of code. When you're doing TDD, you are looking\n    at how an external package/ party is going to use your code as a first step.\n    You're also thinking of making your code testable early on. This results in\n    better abstraction layers and module formations. \n\nNow that we're clear with the big WHY, Mr. Kent goes on to explain HOW in most\npart of the book with examples. Below are the key steps of TDD:\n\nTest Driven Development CycleBelow are some things to keep in mind while doing\nTDD.\n\n1. Formulate a TODO list of some basic requirements that your code should\nfulfill. Pick an item in the list and translate the requirement in the form of a\nfailing test. Remember, don't write the code first, write the test first.\n\n2. The key idea is to write small incremental tests. Don't write the most\ncomplicated test first. Start with the smallest manageable chunk and build on\ntop of that. If a thought strikes about another new test while you're in the\nmiddle of a TDD cycle, write it as an item in the TODO list and continue writing\nand implementing the current cycle first.\n\n3. One may come across some code ugliness that may come in forms of code\nduplication, unprotected methods etc. , specially during the step 3 of the TDD\ncycle. However don't be afraid of it. You're only supposed to show your code to\nyour spouse, only if they are a bit tolerant like mine, until all the steps of\nTDD cycle are completed.\n\n4. It can feel a bit restrictive to have only small incremental steps. Hence, to\nbe practical during TDD, if you want to make minor code change that you know\nyou'll eventually need to do anyways, and it will only cause a brief\ninterruption to your flow, it makes sense to do the change. If a teeny tiny step\nfeels restrictive, take bigger steps. If you feel unsure in big steps, take\nsmaller steps.\n\n5. There can also be exceptions where you write the code first before writing\ntest when the risk of adding code is low and there is already a failing test\nthat needs to be passed first. For example, one can argue to skip writing test\nfor a log statement, because the \n\n * The risk of breaking something is low.\n * The log actually shows what's happening in the code.\n   \n\nSo far, the above mentioned points covers my understanding in terms of summary\nwith the first half of the book. I'll write the summary of second half in the\nnext post.","feature_image":"__GHOST_URL__/content/images/2021/06/Test.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-05T17:53:20.000Z","updated_at":"2022-03-29T01:21:00.000Z","published_at":"2021-06-13T21:48:04.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60bbbb26fda06e0001f83847","uuid":"c6cfc256-d809-4dd6-9e54-c1898004418d","title":"The Power of WHY","slug":"the-power-of-why","mobiledoc":"{\"version\":\"0.3.1\",\"ghostVersion\":\"4.0\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":"","comment_id":"60bbbb26fda06e0001f83847","plaintext":"","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-05T17:57:58.000Z","updated_at":"2021-06-05T17:57:58.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60c02961fda06e0001f83945","uuid":"19edc36a-d470-4d14-a90c-85867cfb64b8","title":"How to add default parameters in","slug":"how-to-add-default-parameters-in","mobiledoc":"{\"version\":\"0.3.1\",\"ghostVersion\":\"4.0\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":"","comment_id":"60c02961fda06e0001f83945","plaintext":"","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-09T02:37:21.000Z","updated_at":"2021-06-09T02:37:21.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"60c6b291fda06e0001f83a5c","uuid":"52c1debb-ebd7-497d-a36d-40b473850c0d","title":"JWTs as Email Verification Tokens","slug":"jwts-as-email-verification-tokens","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/87f17e539f37fce0aaf820f2b4372c97.js\\\"></script>\"}]],\"markups\":[[\"strong\"],[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://jwt.io\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[],0,\"Email verification is one of the basic things most websites use to make sure that the users that registers with them are real. The user is normally asked to click a link sent on his email. When the user clicks the link, the server recognises the user based on the information passed in link and marks it as verified. There are many ways to do so, two major of them being:\"]]],[1,\"p\",[[0,[],0,\"1. Generate a unique code specific to user and save it in your database corresponding to that user. Send the code in the email link sent to user. When the email is clicked, recognise the user based on the code in link and mark it as verified. Secure, but requires persistent storage. \"],[1,[],0,0],[0,[],0,\"2. Encode the userId, email, link expiry information and cryptographically sign it by server. Requires no persistence, fairly secure.\"]]],[1,\"p\",[[0,[],0,\"JWTs happen to be perfect way to achieve the second option.\"]]],[1,\"h4\",[[0,[0],1,\"Advantages\"]]],[1,\"p\",[[0,[],0,\"1. Widely available libraries in every language.\"],[1,[],0,1],[0,[],0,\"2. Out of box integrity protection.\"]]],[1,\"h4\",[[0,[],0,\"Code Sample\"]]],[1,\"p\",[[0,[],0,\"If you already use JWTs for\"],[0,[1],1,\" \"],[0,[2],1,\"Authorization \"],[0,[],0,\"tokens, it is easy to reimplement it for emails. I have used \"],[0,[2],1,\"go\"],[0,[],0,\" to achieve this.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The given code pretty much wraps the token generation and verification. The subject and email returned from \"],[0,[2],1,\"Verify\"],[0,[],0,\" function can be used to mark the user as validated in database. The JWT generated looks something like this:\"]]],[1,\"p\",[[0,[2],1,\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InRlc3RAZXhhbXBsZS5jb20iLCJleHAiOjE1MzczNzI2ODcsImlhdCI6MTUzNzM1NzY4NywibmJmIjoxNTM3MzU3Njg3LCJzdWIiOiIzODM0MTUzMC02ODg0LTRiMTAtYjFhMy00NGRlN2YxNGQ2YzkifQ.gomG2SgLvxNJxTpgCfccyc_2Ft3VJOe5TUFLWUiydlq46ZOQI6KmSoQrFMYEiADfDYqpWs50ui41OUPhOEU5TcH2k_JqVHOnH1MiPJXqB7wydFVxEFkHnlRQvdRYoov4JXdYBUIL7-NUQ4RGyJ8poPS0zrTU-7jCYHgkTxvj4NpvxL2b1j7gQcmQNpZlmo1QrB9ZDHafdFuIQGxnsWxlrK8LW8BbuME-URQx4okHNYCp3FDc7ZbQI0xtBpYK58OzrMFx0vfqCKV_qpvv5OONiw6K61dyDifz4Co0uc4n-BpiGIuih1aIjobpz6k4mocYbEqLu9wSalbiRo36bl2Ojw\"]]],[1,\"p\",[[0,[],0,\"You can use \"],[0,[3],1,\"https://jwt.io\"],[0,[],0,\" to decode and check the content.\"]]],[1,\"p\",[[0,[0],1,\"Disadvantages\"]]],[1,\"p\",[[0,[],0,\"JWT tokens are usually large. In plaintext emails , it can look ugly. It can be avoided by wrapping the email link with html button.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p>Email verification is one of the basic things most websites use to make sure that the users that registers with them are real. The user is normally asked to click a link sent on his email. When the user clicks the link, the server recognises the user based on the information passed in link and marks it as verified. There are many ways to do so, two major of them being:</p><p>1. Generate a unique code specific to user and save it in your database corresponding to that user. Send the code in the email link sent to user. When the email is clicked, recognise the user based on the code in link and mark it as verified. Secure, but requires persistent storage. <br>2. Encode the userId, email, link expiry information and cryptographically sign it by server. Requires no persistence, fairly secure.</p><p>JWTs happen to be perfect way to achieve the second option.</p><h4 id=\"advantages\"><strong>Advantages</strong></h4><p>1. Widely available libraries in every language.<br>2. Out of box integrity protection.</p><h4 id=\"code-sample\">Code Sample</h4><p>If you already use JWTs for<em> </em><code>Authorization </code>tokens, it is easy to reimplement it for emails. I have used <code>go</code> to achieve this.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/87f17e539f37fce0aaf820f2b4372c97.js\"></script><!--kg-card-end: html--><p>The given code pretty much wraps the token generation and verification. The subject and email returned from <code>Verify</code> function can be used to mark the user as validated in database. The JWT generated looks something like this:</p><p><code>eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InRlc3RAZXhhbXBsZS5jb20iLCJleHAiOjE1MzczNzI2ODcsImlhdCI6MTUzNzM1NzY4NywibmJmIjoxNTM3MzU3Njg3LCJzdWIiOiIzODM0MTUzMC02ODg0LTRiMTAtYjFhMy00NGRlN2YxNGQ2YzkifQ.gomG2SgLvxNJxTpgCfccyc_2Ft3VJOe5TUFLWUiydlq46ZOQI6KmSoQrFMYEiADfDYqpWs50ui41OUPhOEU5TcH2k_JqVHOnH1MiPJXqB7wydFVxEFkHnlRQvdRYoov4JXdYBUIL7-NUQ4RGyJ8poPS0zrTU-7jCYHgkTxvj4NpvxL2b1j7gQcmQNpZlmo1QrB9ZDHafdFuIQGxnsWxlrK8LW8BbuME-URQx4okHNYCp3FDc7ZbQI0xtBpYK58OzrMFx0vfqCKV_qpvv5OONiw6K61dyDifz4Co0uc4n-BpiGIuih1aIjobpz6k4mocYbEqLu9wSalbiRo36bl2Ojw</code></p><p>You can use <a href=\"https://jwt.io\" rel=\"noopener\">https://jwt.io</a> to decode and check the content.</p><p><strong>Disadvantages</strong></p><p>JWT tokens are usually large. In plaintext emails , it can look ugly. It can be avoided by wrapping the email link with html button.</p>","comment_id":"60c6b291fda06e0001f83a5c","plaintext":"\nEmail verification is one of the basic things most websites use to make sure\nthat the users that registers with them are real. The user is normally asked to\nclick a link sent on his email. When the user clicks the link, the server\nrecognises the user based on the information passed in link and marks it as\nverified. There are many ways to do so, two major of them being:\n\n1. Generate a unique code specific to user and save it in your database\ncorresponding to that user. Send the code in the email link sent to user. When\nthe email is clicked, recognise the user based on the code in link and mark it\nas verified. Secure, but requires persistent storage. \n2. Encode the userId, email, link expiry information and cryptographically sign\nit by server. Requires no persistence, fairly secure.\n\nJWTs happen to be perfect way to achieve the second option.\n\nAdvantages\n1. Widely available libraries in every language.\n2. Out of box integrity protection.\n\nCode Sample\nIf you already use JWTs for Authorization tokens, it is easy to reimplement it\nfor emails. I have used go to achieve this.\n\nThe given code pretty much wraps the token generation and verification. The\nsubject and email returned from Verify function can be used to mark the user as\nvalidated in database. The JWT generated looks something like this:\n\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InRlc3RAZXhhbXBsZS5jb20iLCJleHAiOjE1MzczNzI2ODcsImlhdCI6MTUzNzM1NzY4NywibmJmIjoxNTM3MzU3Njg3LCJzdWIiOiIzODM0MTUzMC02ODg0LTRiMTAtYjFhMy00NGRlN2YxNGQ2YzkifQ.gomG2SgLvxNJxTpgCfccyc_2Ft3VJOe5TUFLWUiydlq46ZOQI6KmSoQrFMYEiADfDYqpWs50ui41OUPhOEU5TcH2k_JqVHOnH1MiPJXqB7wydFVxEFkHnlRQvdRYoov4JXdYBUIL7-NUQ4RGyJ8poPS0zrTU-7jCYHgkTxvj4NpvxL2b1j7gQcmQNpZlmo1QrB9ZDHafdFuIQGxnsWxlrK8LW8BbuME-URQx4okHNYCp3FDc7ZbQI0xtBpYK58OzrMFx0vfqCKV_qpvv5OONiw6K61dyDifz4Co0uc4n-BpiGIuih1aIjobpz6k4mocYbEqLu9wSalbiRo36bl2Ojw\n\nYou can use https://jwt.io to decode and check the content.\n\nDisadvantages\n\nJWT tokens are usually large. In plaintext emails , it can look ugly. It can be\navoided by wrapping the email link with html button.","feature_image":"__GHOST_URL__/content/images/2021/06/Email.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-06-14T01:36:17.000Z","updated_at":"2021-06-14T01:40:47.000Z","published_at":"2018-09-13T01:39:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"612bd5acfda06e0001f83a8a","uuid":"d85436f6-cbfe-4593-8b0d-8311fed6c850","title":"How to setup IPP (Internet Printing Protocol) Printer on Windows.","slug":"how-to-setup-ipp-internet-printing-protocol-printer-on-windows","mobiledoc":"{\"version\":\"0.3.1\",\"ghostVersion\":\"4.0\",\"markups\":[],\"atoms\":[],\"cards\":[],\"sections\":[[1,\"p\",[[0,[],0,\"\"]]]]}","html":"","comment_id":"612bd5acfda06e0001f83a8a","plaintext":"","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-08-29T18:45:00.000Z","updated_at":"2021-10-17T23:04:02.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"61900fc7f65b490001261aec","uuid":"b95dade1-44ae-4a6a-8b57-47b983957089","title":"Add log rotation for a service running in linux using Ansible","slug":"add-logrotate-to-a-machine-using-ansible","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/40de6cb867008e80dc6b5d1c39f24da6.js\\\"></script>\"}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=1id6ERvfozo&ab_channel=TechWorldwithNana\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/System_administration\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Computer_data_logging\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Archive\"]],[\"a\",[\"href\",\"https://linux.die.net/man/8/logrotate\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Ansible\"],[0,[],0,\" is an IT automation tool that can automate cloud provisioning, configuration management, application deployment and other automation needs. In order to have a brief overview of how Ansible works and how is it different from other automation tools, one can watch \"],[0,[1],1,\"this\"],[0,[],0,\" video. \"]]],[1,\"p\",[[0,[0],1,\"Log rotation\"],[0,[],0,\" is an automated process used in \"],[0,[2],1,\"system administration\"],[0,[],0,\" in which \"],[0,[3],1,\"log files\"],[0,[],0,\" are compressed, moved (\"],[0,[4],1,\"archived\"],[0,[],0,\"), renamed or deleted once they are too old or too big (there can be other metrics that can apply here). New incoming log data is directed into a new fresh file (at the same location).\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"In Linux, log rotation is typically performed using the \"],[0,[5],1,\"logrotate\"],[0,[],0,\" command. \"],[0,[0,0],2,\"logrotate\"],[0,[],0,\" reads everything about the log files it should be handling from the series of configuration files specified on the command line. Each configuration file can set global options (local definitions override global ones, and later definitions override earlier ones) and specify logfiles to rotate. \"]]],[1,\"p\",[[0,[],0,\"In the below example, we create a logfile, which needs to be rotated and create the logrotate configuration for that file in the yaml file for configuration.\"]]],[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Ansible</strong> is an IT automation tool that can automate cloud provisioning, configuration management, application deployment and other automation needs. In order to have a brief overview of how Ansible works and how is it different from other automation tools, one can watch <a href=\"https://www.youtube.com/watch?v=1id6ERvfozo&amp;ab_channel=TechWorldwithNana\">this</a> video. </p><p><strong>Log rotation</strong> is an automated process used in <a href=\"https://en.wikipedia.org/wiki/System_administration\">system administration</a> in which <a href=\"https://en.wikipedia.org/wiki/Computer_data_logging\">log files</a> are compressed, moved (<a href=\"https://en.wikipedia.org/wiki/Archive\">archived</a>), renamed or deleted once they are too old or too big (there can be other metrics that can apply here). New incoming log data is directed into a new fresh file (at the same location).<br><br>In Linux, log rotation is typically performed using the <a href=\"https://linux.die.net/man/8/logrotate\">logrotate</a> command. <strong><strong>logrotate</strong></strong> reads everything about the log files it should be handling from the series of configuration files specified on the command line. Each configuration file can set global options (local definitions override global ones, and later definitions override earlier ones) and specify logfiles to rotate. </p><p>In the below example, we create a logfile, which needs to be rotated and create the logrotate configuration for that file in the yaml file for configuration.</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/40de6cb867008e80dc6b5d1c39f24da6.js\"></script><!--kg-card-end: html-->","comment_id":"61900fc7f65b490001261aec","plaintext":"Ansible is an IT automation tool that can automate cloud provisioning,\nconfiguration management, application deployment and other automation needs. In\norder to have a brief overview of how Ansible works and how is it different from\nother automation tools, one can watch this\n[https://www.youtube.com/watch?v=1id6ERvfozo&ab_channel=TechWorldwithNana] \nvideo. \n\nLog rotation is an automated process used in system administration\n[https://en.wikipedia.org/wiki/System_administration] in which log files\n[https://en.wikipedia.org/wiki/Computer_data_logging] are compressed, moved (\narchived [https://en.wikipedia.org/wiki/Archive]), renamed or deleted once they\nare too old or too big (there can be other metrics that can apply here). New\nincoming log data is directed into a new fresh file (at the same location).\n\nIn Linux, log rotation is typically performed using the logrotate\n[https://linux.die.net/man/8/logrotate] command. logrotate reads everything\nabout the log files it should be handling from the series of configuration files\nspecified on the command line. Each configuration file can set global options\n(local definitions override global ones, and later definitions override earlier\nones) and specify logfiles to rotate. \n\nIn the below example, we create a logfile, which needs to be rotated and create\nthe logrotate configuration for that file in the yaml file for configuration.","feature_image":"__GHOST_URL__/content/images/2021/11/ansible.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-11-13T19:19:35.000Z","updated_at":"2022-07-04T21:36:57.000Z","published_at":"2021-11-13T21:37:10.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"61b6a32df65b490001261b8a","uuid":"661d77ec-9449-4e81-8e47-d70dcc085c52","title":"Home","slug":"home","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<!-- Start of LiveChat (www.livechatinc.com) code -->\\n<script>\\n    window.__lc = window.__lc || {};\\n    window.__lc.license = 13506288;\\n    ;(function(n,t,c){function i(n){return e._h?e._h.apply(null,n):e._q.push(n)}var e={_q:[],_h:null,_v:\\\"2.0\\\",on:function(){i([\\\"on\\\",c.call(arguments)])},once:function(){i([\\\"once\\\",c.call(arguments)])},off:function(){i([\\\"off\\\",c.call(arguments)])},get:function(){if(!e._h)throw new Error(\\\"[LiveChatWidget] You can't use getters before load.\\\");return i([\\\"get\\\",c.call(arguments)])},call:function(){i([\\\"call\\\",c.call(arguments)])},init:function(){var n=t.createElement(\\\"script\\\");n.async=!0,n.type=\\\"text/javascript\\\",n.src=\\\"https://cdn.livechatinc.com/tracking.js\\\",t.head.appendChild(n)}};!n.__lc.asyncInit&&e.init(),n.LiveChatWidget=n.LiveChatWidget||e}(window,document,[].slice))\\n</script>\\n<noscript><a href=\\\"https://www.livechatinc.com/chat-with/13506288/\\\" rel=\\\"nofollow\\\">Chat with us</a>, powered by <a href=\\\"https://www.livechatinc.com/?welcome\\\" rel=\\\"noopener nofollow\\\" target=\\\"_blank\\\">LiveChat</a></noscript>\\n<!-- End of LiveChat code -->\\n\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<!--kg-card-begin: html--><!-- Start of LiveChat (www.livechatinc.com) code -->\n<script>\n    window.__lc = window.__lc || {};\n    window.__lc.license = 13506288;\n    ;(function(n,t,c){function i(n){return e._h?e._h.apply(null,n):e._q.push(n)}var e={_q:[],_h:null,_v:\"2.0\",on:function(){i([\"on\",c.call(arguments)])},once:function(){i([\"once\",c.call(arguments)])},off:function(){i([\"off\",c.call(arguments)])},get:function(){if(!e._h)throw new Error(\"[LiveChatWidget] You can't use getters before load.\");return i([\"get\",c.call(arguments)])},call:function(){i([\"call\",c.call(arguments)])},init:function(){var n=t.createElement(\"script\");n.async=!0,n.type=\"text/javascript\",n.src=\"https://cdn.livechatinc.com/tracking.js\",t.head.appendChild(n)}};!n.__lc.asyncInit&&e.init(),n.LiveChatWidget=n.LiveChatWidget||e}(window,document,[].slice))\n</script>\n<noscript><a href=\"https://www.livechatinc.com/chat-with/13506288/\" rel=\"nofollow\">Chat with us</a>, powered by <a href=\"https://www.livechatinc.com/?welcome\" rel=\"noopener nofollow\" target=\"_blank\">LiveChat</a></noscript>\n<!-- End of LiveChat code -->\n<!--kg-card-end: html-->","comment_id":"61b6a32df65b490001261b8a","plaintext":"Chat with us [https://www.livechatinc.com/chat-with/13506288/], powered by \nLiveChat [https://www.livechatinc.com/?welcome]","feature_image":null,"featured":0,"type":"page","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-12-13T01:34:37.000Z","updated_at":"2022-02-07T01:18:05.000Z","published_at":"2021-12-13T01:34:53.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"61ca21f8f65b490001261ba8","uuid":"40a7a3dd-7322-41be-a6a8-9aaaa325ff40","title":"How to debug a docker image.","slug":"how-to-debug-a-docker-image","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/dd88c940661e79ed2582c5ea26ff1571.js\\\"></script>\"}],[\"markdown\",{\"markdown\":\"```shell\\n> docker build -t test_image\\n```\"}],[\"markdown\",{\"markdown\":\"<span style=\\\"color:red\\\">   \\ngo: github.com/org-name/private-repo/v2@v2.11.0: reading github.com/org-name/private-repo/go.mod at revision v2.11.0: unknown revision v2.11.0   \\n</span>\"}],[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/67199188cf1caaac212f163c063481e7.js\\\"></script>\"}],[\"markdown\",{\"markdown\":\"```shell\\n> docker run -it --rm test_image sh\\n```\"}],[\"markdown\",{\"markdown\":\"```shell\\n> git config --global --list\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nurl.https://:x-oauth-basic@github.com.insteadof=https://github.com\\n```\"}],[\"markdown\",{\"markdown\":\"```shell\\n> docker build --build-arg GITHUB_TOKEN=dFtrRShVIzBvJHBGQk1wJE9f -t test_image\\n```\"}]],\"markups\":[[\"a\",[\"href\",\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\"]],[\"a\",[\"href\",\"https://pkg.go.dev/cmd/go#hdr-Compile_packages_and_dependencies\"]],[\"strong\"],[\"a\",[\"href\",\"https://docs.docker.com/engine/reference/commandline/run/\"]],[\"code\"]],\"sections\":[[1,\"p\",[[0,[],0,\"Ever came across a time when a docker image just wouldn't build successfully? Even after putting band-aids and patches all over the place and dropping jargon of list and print commands in between, you're not able to see where exactly it went wrong. I had been through the same situation for some time, before I decided to approach the situation differently. And that's what I am going to share with you now.\"]]],[1,\"p\",[[0,[],0,\"I'll start with a simple example where we will debug the Dockerfile together.\"]]],[1,\"p\",[[0,[],0,\"Consider the Dockerfile below:\"]]],[10,0],[1,\"p\",[[0,[],0,\"In the above file, we are simply trying to build a docker image for a Golang service. It involves pulling some packages from a private repository. Golang, by default, uses https to pull packages. In line 11, we're setting the git URL to use our \"],[0,[0],1,\"personal access token\"],[0,[],0,\" so that it's also able to access the private repositories. In the `go build` \"],[0,[1],1,\"command\"],[0,[],0,\" in line 14, Golang will pull the required repositories and build the executable service binary. \"]]],[1,\"p\",[[0,[],0,\"Let's try to build the docker image now. I will use the command: \"]]],[10,1],[1,\"p\",[[0,[],0,\"However it returned the error shown below. \"]]],[10,2],[1,\"p\",[[0,[],0,\"It seems like the error is coming from an unknown revision, either GOPRIVATE is not being set, or the given git URL is not able to access the private repository.\"]]],[1,\"p\",[[0,[],0,\"Let's try to find out what the issue is. For this, I will\"]]],[3,\"ol\",[[[0,[],0,\"Comment some lines in the docker image and build that image.\"]],[[0,[],0,\"Create the docker container and get into the shell of the running container.\"]],[[0,[],0,\"Check the environment inside the docker container setup.\"]],[[0,[],0,\"If necessary execute each commented commands manually, and see the results.\"]]]],[1,\"h3\",[[0,[2],1,\"Step 1: Comment some lines in the Dockerfile and build the image.\"]]],[10,3],[1,\"p\",[[0,[],0,\"We commented out some lines of code in the above Dockerfile and tried to build the image again. The docker image \"],[0,[2],1,\"builds successfully now\"],[0,[],0,\", but some commands are missing.\"]]],[1,\"h3\",[[0,[2],1,\"Step 2: Get into the shell/ bash of the created docker container from the image.\"]]],[1,\"p\",[[0,[],0,\"The good news is that now we can run the docker image, go inside the shell ourselves and run the commands manually, while checking the environment where commands are being run. The below command \"],[0,[3],1,\"runs\"],[0,[],0,\" a docker container with the image and opens the shell inside the container.\"]]],[10,4],[1,\"h3\",[[0,[2],1,\"Step 3: Check the environment inside the docker container setup.\"]]],[1,\"p\",[[0,[],0,\"Now that we're inside the shell, let's see the status of the container as of now. \"]]],[1,\"p\",[[0,[],0,\"Checking \"],[0,[4],1,\"go env\"],[0,[],0,\" shows a properly set GOPRIVATE variable.\"]]],[1,\"p\",[[0,[],0,\"Let's check that the global git url for our org is properly set as described by the last docker command in line 11.\"]]],[10,5],[1,\"p\",[[0,[],0,\"The result looks like this:\"]]],[10,6],[1,\"p\",[[0,[],0,\"The global URL seems weird as it seems to be setting an empty \"],[0,[4],1,\"GITHUB_TOKEN\"],[0,[],0,\" (refer line 11 in the Dockerfile). It seems like the build arg was not properly passed while building the docker image. Passing a proper \"],[0,[4],1,\"GITHUB_TOKEN\"],[0,[],0,\" to the docker build command should resolve the issue in this case.\"]]],[10,7],[1,\"p\",[[0,[],0,\"HURRAH!!!! The docker image is built successfully now. If needed, we could also run the individual commented commands from the shell of the running container to see what happens after each command. \"]]],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<p>Ever came across a time when a docker image just wouldn't build successfully? Even after putting band-aids and patches all over the place and dropping jargon of list and print commands in between, you're not able to see where exactly it went wrong. I had been through the same situation for some time, before I decided to approach the situation differently. And that's what I am going to share with you now.</p><p>I'll start with a simple example where we will debug the Dockerfile together.</p><p>Consider the Dockerfile below:</p><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/dd88c940661e79ed2582c5ea26ff1571.js\"></script><!--kg-card-end: html--><p>In the above file, we are simply trying to build a docker image for a Golang service. It involves pulling some packages from a private repository. Golang, by default, uses https to pull packages. In line 11, we're setting the git URL to use our <a href=\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token\">personal access token</a> so that it's also able to access the private repositories. In the `go build` <a href=\"https://pkg.go.dev/cmd/go#hdr-Compile_packages_and_dependencies\">command</a> in line 14, Golang will pull the required repositories and build the executable service binary. </p><p>Let's try to build the docker image now. I will use the command: </p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">&gt; docker build -t test_image\n</code></pre>\n<!--kg-card-end: markdown--><p>However it returned the error shown below. </p><!--kg-card-begin: markdown--><span style=\"color:red\">   \ngo: github.com/org-name/private-repo/v2@v2.11.0: reading github.com/org-name/private-repo/go.mod at revision v2.11.0: unknown revision v2.11.0   \n</span><!--kg-card-end: markdown--><p>It seems like the error is coming from an unknown revision, either GOPRIVATE is not being set, or the given git URL is not able to access the private repository.</p><p>Let's try to find out what the issue is. For this, I will</p><ol><li>Comment some lines in the docker image and build that image.</li><li>Create the docker container and get into the shell of the running container.</li><li>Check the environment inside the docker container setup.</li><li>If necessary execute each commented commands manually, and see the results.</li></ol><h3 id=\"step-1-comment-some-lines-in-the-dockerfile-and-build-the-image\"><strong>Step 1: Comment some lines in the Dockerfile and build the image.</strong></h3><!--kg-card-begin: html--><script src=\"https://gist.github.com/Harsimran1/67199188cf1caaac212f163c063481e7.js\"></script><!--kg-card-end: html--><p>We commented out some lines of code in the above Dockerfile and tried to build the image again. The docker image <strong>builds successfully now</strong>, but some commands are missing.</p><h3 id=\"step-2-get-into-the-shell-bash-of-the-created-docker-container-from-the-image\"><strong>Step 2: Get into the shell/ bash of the created docker container from the image.</strong></h3><p>The good news is that now we can run the docker image, go inside the shell ourselves and run the commands manually, while checking the environment where commands are being run. The below command <a href=\"https://docs.docker.com/engine/reference/commandline/run/\">runs</a> a docker container with the image and opens the shell inside the container.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">&gt; docker run -it --rm test_image sh\n</code></pre>\n<!--kg-card-end: markdown--><h3 id=\"step-3-check-the-environment-inside-the-docker-container-setup\"><strong>Step 3: Check the environment inside the docker container setup.</strong></h3><p>Now that we're inside the shell, let's see the status of the container as of now. </p><p>Checking <code>go env</code> shows a properly set GOPRIVATE variable.</p><p>Let's check that the global git url for our org is properly set as described by the last docker command in line 11.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">&gt; git config --global --list\n</code></pre>\n<!--kg-card-end: markdown--><p>The result looks like this:</p><!--kg-card-begin: markdown--><pre><code>url.https://:x-oauth-basic@github.com.insteadof=https://github.com\n</code></pre>\n<!--kg-card-end: markdown--><p>The global URL seems weird as it seems to be setting an empty <code>GITHUB_TOKEN</code> (refer line 11 in the Dockerfile). It seems like the build arg was not properly passed while building the docker image. Passing a proper <code>GITHUB_TOKEN</code> to the docker build command should resolve the issue in this case.</p><!--kg-card-begin: markdown--><pre><code class=\"language-shell\">&gt; docker build --build-arg GITHUB_TOKEN=dFtrRShVIzBvJHBGQk1wJE9f -t test_image\n</code></pre>\n<!--kg-card-end: markdown--><p>HURRAH!!!! The docker image is built successfully now. If needed, we could also run the individual commented commands from the shell of the running container to see what happens after each command. </p>","comment_id":"61ca21f8f65b490001261ba8","plaintext":"Ever came across a time when a docker image just wouldn't build successfully?\nEven after putting band-aids and patches all over the place and dropping jargon\nof list and print commands in between, you're not able to see where exactly it\nwent wrong. I had been through the same situation for some time, before I\ndecided to approach the situation differently. And that's what I am going to\nshare with you now.\n\nI'll start with a simple example where we will debug the Dockerfile together.\n\nConsider the Dockerfile below:\n\nIn the above file, we are simply trying to build a docker image for a Golang\nservice. It involves pulling some packages from a private repository. Golang, by\ndefault, uses https to pull packages. In line 11, we're setting the git URL to\nuse our personal access token\n[https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token] \nso that it's also able to access the private repositories. In the `go build` \ncommand [https://pkg.go.dev/cmd/go#hdr-Compile_packages_and_dependencies] in\nline 14, Golang will pull the required repositories and build the executable\nservice binary. \n\nLet's try to build the docker image now. I will use the command: \n\n> docker build -t test_image\n\n\nHowever it returned the error shown below. \n\ngo: github.com/org-name/private-repo/v2@v2.11.0: reading\ngithub.com/org-name/private-repo/go.mod at revision v2.11.0: unknown revision\nv2.11.0It seems like the error is coming from an unknown revision, either\nGOPRIVATE is not being set, or the given git URL is not able to access the\nprivate repository.\n\nLet's try to find out what the issue is. For this, I will\n\n 1. Comment some lines in the docker image and build that image.\n 2. Create the docker container and get into the shell of the running container.\n 3. Check the environment inside the docker container setup.\n 4. If necessary execute each commented commands manually, and see the results.\n\nStep 1: Comment some lines in the Dockerfile and build the image.\nWe commented out some lines of code in the above Dockerfile and tried to build\nthe image again. The docker image builds successfully now, but some commands are\nmissing.\n\nStep 2: Get into the shell/ bash of the created docker container from the image.\nThe good news is that now we can run the docker image, go inside the shell\nourselves and run the commands manually, while checking the environment where\ncommands are being run. The below command runs\n[https://docs.docker.com/engine/reference/commandline/run/] a docker container\nwith the image and opens the shell inside the container.\n\n> docker run -it --rm test_image sh\n\n\nStep 3: Check the environment inside the docker container setup.\nNow that we're inside the shell, let's see the status of the container as of\nnow. \n\nChecking go env shows a properly set GOPRIVATE variable.\n\nLet's check that the global git url for our org is properly set as described by\nthe last docker command in line 11.\n\n> git config --global --list\n\n\nThe result looks like this:\n\nurl.https://:x-oauth-basic@github.com.insteadof=https://github.com\n\n\nThe global URL seems weird as it seems to be setting an empty GITHUB_TOKEN \n(refer line 11 in the Dockerfile). It seems like the build arg was not properly\npassed while building the docker image. Passing a proper GITHUB_TOKEN to the\ndocker build command should resolve the issue in this case.\n\n> docker build --build-arg GITHUB_TOKEN=dFtrRShVIzBvJHBGQk1wJE9f -t test_image\n\n\nHURRAH!!!! The docker image is built successfully now. If needed, we could also\nrun the individual commented commands from the shell of the running container to\nsee what happens after each command.","feature_image":"__GHOST_URL__/content/images/2021/12/wordpress-debug.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2021-12-27T20:28:40.000Z","updated_at":"2022-07-04T21:37:04.000Z","published_at":"2021-12-29T22:00:51.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"61dfad30f65b490001261eaa","uuid":"f2b881be-5aac-48d8-9d8a-3333435c7d07","title":"How I brought my dead computer back to life.","slug":"how-i-brought-my-dead-computer-back-to-life","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.32.29.jpeg\",\"width\":1024,\"height\":441,\"caption\":\"Initial startup screen when my arch linux was stuck for more than 30 minutes.\",\"cardWidth\":\"\",\"alt\":\"Initial startup screen when my arch linux was stuck for more than 30 minutes.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.30.17.jpeg\",\"width\":1024,\"height\":372,\"cardWidth\":\"\",\"alt\":\"GRUB bootloader script for linux\",\"caption\":\"GRUB boot-loader script for linux with highlighted quiet flag.\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.39.36.jpeg\",\"width\":1024,\"height\":488,\"cardWidth\":\"\",\"alt\":\"Inital startup screen after enabling boot logs.\",\"caption\":\"Inital startup screen after enabling boot logs.\"}],[\"markdown\",{\"markdown\":\"```\\nfsck /dev/mapper/volgroup0-lv_home\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nmkfs.ext4 /dev/volgroup0/lv_home\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nmount /dev/volgroup0/lv_home /home\\n```\"}],[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/01/Screenshot-from-2022-01-22-15-17-17.png\",\"width\":1896,\"height\":401,\"caption\":\"List of all the block devices in arch linux with their UUIDs - the output of blkid command.\",\"alt\":\"List of all the block devices in arch linux with their UUIDs - the output of blkid command.\"}]],\"markups\":[[\"a\",[\"href\",\"https://wiki.archlinux.org/title/LVM\"]],[\"a\",[\"href\",\"https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LVM_on_LUKS\"]],[\"strong\"],[\"em\"],[\"code\"],[\"a\",[\"href\",\"https://wiki.archlinux.org/title/LVM#Resizing_the_logical_volume_and_file_system_separately\"]],[\"a\",[\"href\",\"https://linoxide.com/linux-command-lsblk-blkid/#:~:text=The%20blkid%20program%20is%20a,e.g.%20LABEL%20or%20UUID%20fields).\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have a great relationship with the New Year. I am not much of a planner, but nature has its way of planning a bumpy ride for me for the first few days of every year.\"]]],[1,\"p\",[[0,[],0,\"On my second day at work as the year started, my computer refused to start. And since it was an Arch Linux OS, and boot debug logs were disabled, it won't tell me WHY. \"]]],[1,\"p\",[[0,[],0,\"To give you a bit of context, I was not surprised. As I am not much of a planner, on the previous evening (Monday), I had decided to resize the \"],[0,[0],1,\"Logical Volumes\"],[0,[],0,\" of my main partition. I have \"],[0,[1],1,\"disc encryption enabled on my partition\"],[0,[],0,\". So I was a bit worried that bootloader is not able to decrypt the partition properly. But luckily I had only resized the logical volumes and not the whole partition. This is what I did to fix my broken boot-loader.\"]]],[1,\"h3\",[[0,[],0,\"Getting the visibility on the culprit.\"]]],[1,\"p\",[[0,[],0,\"The next step was to get some eyes on what was going on during the boot.  This is what my boot-loader looked like I had been staring at it to move forward. \"]]],[10,0],[1,\"p\",[[0,[],0,\"As they say, \"]]],[1,\"blockquote\",[[0,[],0,\"All things come to those who wait.\"]]],[1,\"p\",[[0,[],0,\"I would have believed that quote, if it was a weekend and I had all the day to wait. But right then, staring at the boot screen which seemed to be stuck for forever, wasn't helping me much.\"]]],[1,\"p\",[[0,[],0,\"That's when I changed the boot-loader script to enable the debug logging. \"]]],[1,\"p\",[[0,[2,3],2,\"How: \"],[0,[],0,\"On startup screen, where it asks to choose the OS to start with, pressing \"],[0,[4],1,\"e\"],[0,[],0,\" takes one to the editor with GRUB boot-loader script, which is normally present in \"],[0,[4],1,\"/boot/grub/grub.cfg\"],[0,[],0,\" in linux system.\"]]],[1,\"p\",[[0,[],0,\"I removed the \"],[0,[4],1,\"quiet\"],[0,[],0,\" flag  (highlighted in the image below) from the boot-loader script, re-initiated the boot again, and let nature take its course.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Now I could finally see the logs of what was happening.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Huh!!! From what it looks like, it seems like there is a start job for File System Check that seems to be taking forever to finish. \"]]],[1,\"p\",[[0,[],0,\"That made me wonder, how could I bypass this file system check during the boot. This would at-least enable me to access my File System, make necessary changes and hopefully fix the problem. \"]]],[1,\"h3\",[[0,[],0,\"Skipping the blocking job during boot.\"]]],[1,\"p\",[[0,[],0,\"After a bit of googling, I added \"],[0,[4],1,\"fsck.mode=skip\"],[0,[],0,\" flag into my boot-loader script. This would enable skipping the File System Checks during the boot. This was added in the same line, the \"],[0,[4],1,\"quiet\"],[0,[],0,\" flag was removed in previous step. I initiated the boot again and waited for something to happen. \"]]],[1,\"p\",[[0,[],0,\"This time it took me into maintainers mode where I could finally access my device as root. \"]]],[1,\"h3\",[[0,[],0,\"Righting the wrongs.\"]]],[1,\"p\",[[0,[],0,\"From what I could see, my \"],[0,[4],1,\"/dev/mapper/volgroup0-lv_home\"],[0,[],0,\" volume was not mounted. Running File System Check for the volume also failed.\"]]],[10,3],[1,\"p\",[[0,[],0,\"This definitely proved that there is something wrong with the \"],[0,[4],1,\"home\"],[0,[],0,\" volume.\"]]],[1,\"p\",[[0,[2,3],1,\"What went wrong:\"],[0,[],1,\" \"],[0,[],0,\"This has most probably happened, when the day before, I increased the size of the root logical volume and shrank the size of my home logical volume, referring \"],[0,[5],1,\"this\"],[0,[],0,\" doc. I must have done some mistake in calculating the sizes of the resulting volumes. In my defense, it was already 10 p.m. at night when I did that.\"]]],[1,\"p\",[[0,[],0,\"I was in a bit of rush, as the next morning, I had to start for work again and it wouldn't help if I was still not able to fix my computer. Hence I ended up formatting my \"],[0,[4],1,\"/dev/mapper/volgroup0-lv_home\"],[0,[],0,\" logical volume using the following command: \"]]],[10,4],[1,\"p\",[[0,[],0,\"Running the File System Check on the new volume succeeded this time.\"]]],[1,\"p\",[[0,[],0,\"Then I mounted the newly created logical volume on the home directory using:\"]]],[10,5],[1,\"p\",[[0,[],0,\"One would think that this would be all, but there's still one step missing.\"]]],[1,\"p\",[[0,[],0,\"The UUID of the \"],[0,[4],1,\"/dev/volgroup0/lv_home\"],[0,[],0,\" logical volume has changed because I had created a new File System.  We need to tell the File System to check for new UUID during boot. Runnig \"],[0,[4],1,\"blkid\"],[0,[],0,\" \"],[0,[6],1,\"command\"],[0,[],0,\" gives the list of all the block devices in the machine with their UUIDs.\"]]],[10,6],[1,\"p\",[[0,[],0,\"I had to copy over the UUID of \"],[0,[4],1,\"/dev/mapper/volgroup0-lv_home\"],[0,[],0,\" volume and replace it with the UUID in \"],[0,[4],1,\"/etc/fstab\"],[0,[],0,\" file for the given volume group.\"]]],[1,\"h3\",[[0,[],0,\"Rebooting the machine.\"]]],[1,\"p\",[[0,[],0,\"HURRAY! This time rebooting the machine actually took me to the home login screen.\"]]],[1,\"p\",[[0,[],0,\"I had to however create a new user, as all the config files for the apps, which were present in the `/home` directory for my old user didn't exist anymore. But that was an OKish compromise. The good thing was that I didn't have to re-install all the packages as the root volume was still intact.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>I have a great relationship with the New Year. I am not much of a planner, but nature has its way of planning a bumpy ride for me for the first few days of every year.</p><p>On my second day at work as the year started, my computer refused to start. And since it was an Arch Linux OS, and boot debug logs were disabled, it won't tell me WHY. </p><p>To give you a bit of context, I was not surprised. As I am not much of a planner, on the previous evening (Monday), I had decided to resize the <a href=\"https://wiki.archlinux.org/title/LVM\">Logical Volumes</a> of my main partition. I have <a href=\"https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LVM_on_LUKS\">disc encryption enabled on my partition</a>. So I was a bit worried that bootloader is not able to decrypt the partition properly. But luckily I had only resized the logical volumes and not the whole partition. This is what I did to fix my broken boot-loader.</p><h3 id=\"getting-the-visibility-on-the-culprit\">Getting the visibility on the culprit.</h3><p>The next step was to get some eyes on what was going on during the boot.  This is what my boot-loader looked like I had been staring at it to move forward. </p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.32.29.jpeg\" class=\"kg-image\" alt=\"Initial startup screen when my arch linux was stuck for more than 30 minutes.\" loading=\"lazy\" width=\"1024\" height=\"441\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/WhatsApp-Image-2022-01-22-at-13.32.29.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2022/01/WhatsApp-Image-2022-01-22-at-13.32.29.jpeg 1000w, __GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.32.29.jpeg 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Initial startup screen when my arch linux was stuck for more than 30 minutes.</figcaption></figure><p>As they say, </p><blockquote>All things come to those who wait.</blockquote><p>I would have believed that quote, if it was a weekend and I had all the day to wait. But right then, staring at the boot screen which seemed to be stuck for forever, wasn't helping me much.</p><p>That's when I changed the boot-loader script to enable the debug logging. </p><p><strong><em>How: </em></strong>On startup screen, where it asks to choose the OS to start with, pressing <code>e</code> takes one to the editor with GRUB boot-loader script, which is normally present in <code>/boot/grub/grub.cfg</code> in linux system.</p><p>I removed the <code>quiet</code> flag  (highlighted in the image below) from the boot-loader script, re-initiated the boot again, and let nature take its course.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.30.17.jpeg\" class=\"kg-image\" alt=\"GRUB bootloader script for linux\" loading=\"lazy\" width=\"1024\" height=\"372\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/WhatsApp-Image-2022-01-22-at-13.30.17.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2022/01/WhatsApp-Image-2022-01-22-at-13.30.17.jpeg 1000w, __GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.30.17.jpeg 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>GRUB boot-loader script for linux with highlighted quiet flag.</figcaption></figure><p>Now I could finally see the logs of what was happening.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.39.36.jpeg\" class=\"kg-image\" alt=\"Inital startup screen after enabling boot logs.\" loading=\"lazy\" width=\"1024\" height=\"488\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/WhatsApp-Image-2022-01-22-at-13.39.36.jpeg 600w, __GHOST_URL__/content/images/size/w1000/2022/01/WhatsApp-Image-2022-01-22-at-13.39.36.jpeg 1000w, __GHOST_URL__/content/images/2022/01/WhatsApp-Image-2022-01-22-at-13.39.36.jpeg 1024w\" sizes=\"(min-width: 720px) 720px\"><figcaption>Inital startup screen after enabling boot logs.</figcaption></figure><p>Huh!!! From what it looks like, it seems like there is a start job for File System Check that seems to be taking forever to finish. </p><p>That made me wonder, how could I bypass this file system check during the boot. This would at-least enable me to access my File System, make necessary changes and hopefully fix the problem. </p><h3 id=\"skipping-the-blocking-job-during-boot\">Skipping the blocking job during boot.</h3><p>After a bit of googling, I added <code>fsck.mode=skip</code> flag into my boot-loader script. This would enable skipping the File System Checks during the boot. This was added in the same line, the <code>quiet</code> flag was removed in previous step. I initiated the boot again and waited for something to happen. </p><p>This time it took me into maintainers mode where I could finally access my device as root. </p><h3 id=\"righting-the-wrongs\">Righting the wrongs.</h3><p>From what I could see, my <code>/dev/mapper/volgroup0-lv_home</code> volume was not mounted. Running File System Check for the volume also failed.</p><!--kg-card-begin: markdown--><pre><code>fsck /dev/mapper/volgroup0-lv_home\n</code></pre>\n<!--kg-card-end: markdown--><p>This definitely proved that there is something wrong with the <code>home</code> volume.</p><p><strong><em>What went wrong:</em> </strong>This has most probably happened, when the day before, I increased the size of the root logical volume and shrank the size of my home logical volume, referring <a href=\"https://wiki.archlinux.org/title/LVM#Resizing_the_logical_volume_and_file_system_separately\">this</a> doc. I must have done some mistake in calculating the sizes of the resulting volumes. In my defense, it was already 10 p.m. at night when I did that.</p><p>I was in a bit of rush, as the next morning, I had to start for work again and it wouldn't help if I was still not able to fix my computer. Hence I ended up formatting my <code>/dev/mapper/volgroup0-lv_home</code> logical volume using the following command: </p><!--kg-card-begin: markdown--><pre><code>mkfs.ext4 /dev/volgroup0/lv_home\n</code></pre>\n<!--kg-card-end: markdown--><p>Running the File System Check on the new volume succeeded this time.</p><p>Then I mounted the newly created logical volume on the home directory using:</p><!--kg-card-begin: markdown--><pre><code>mount /dev/volgroup0/lv_home /home\n</code></pre>\n<!--kg-card-end: markdown--><p>One would think that this would be all, but there's still one step missing.</p><p>The UUID of the <code>/dev/volgroup0/lv_home</code> logical volume has changed because I had created a new File System.  We need to tell the File System to check for new UUID during boot. Runnig <code>blkid</code> <a href=\"https://linoxide.com/linux-command-lsblk-blkid/#:~:text=The%20blkid%20program%20is%20a,e.g.%20LABEL%20or%20UUID%20fields).\">command</a> gives the list of all the block devices in the machine with their UUIDs.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"__GHOST_URL__/content/images/2022/01/Screenshot-from-2022-01-22-15-17-17.png\" class=\"kg-image\" alt=\"List of all the block devices in arch linux with their UUIDs - the output of blkid command.\" loading=\"lazy\" width=\"1896\" height=\"401\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/01/Screenshot-from-2022-01-22-15-17-17.png 600w, __GHOST_URL__/content/images/size/w1000/2022/01/Screenshot-from-2022-01-22-15-17-17.png 1000w, __GHOST_URL__/content/images/size/w1600/2022/01/Screenshot-from-2022-01-22-15-17-17.png 1600w, __GHOST_URL__/content/images/2022/01/Screenshot-from-2022-01-22-15-17-17.png 1896w\" sizes=\"(min-width: 720px) 720px\"><figcaption>List of all the block devices in arch linux with their UUIDs - the output of blkid command.</figcaption></figure><p>I had to copy over the UUID of <code>/dev/mapper/volgroup0-lv_home</code> volume and replace it with the UUID in <code>/etc/fstab</code> file for the given volume group.</p><h3 id=\"rebooting-the-machine\">Rebooting the machine.</h3><p>HURRAY! This time rebooting the machine actually took me to the home login screen.</p><p>I had to however create a new user, as all the config files for the apps, which were present in the `/home` directory for my old user didn't exist anymore. But that was an OKish compromise. The good thing was that I didn't have to re-install all the packages as the root volume was still intact.</p>","comment_id":"61dfad30f65b490001261eaa","plaintext":"I have a great relationship with the New Year. I am not much of a planner, but\nnature has its way of planning a bumpy ride for me for the first few days of\nevery year.\n\nOn my second day at work as the year started, my computer refused to start. And\nsince it was an Arch Linux OS, and boot debug logs were disabled, it won't tell\nme WHY. \n\nTo give you a bit of context, I was not surprised. As I am not much of a\nplanner, on the previous evening (Monday), I had decided to resize the Logical\nVolumes [https://wiki.archlinux.org/title/LVM] of my main partition. I have \ndisc\nencryption enabled on my partition\n[https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LVM_on_LUKS]\n. So I was a bit worried that bootloader is not able to decrypt the partition\nproperly. But luckily I had only resized the logical volumes and not the whole\npartition. This is what I did to fix my broken boot-loader.\n\nGetting the visibility on the culprit.\nThe next step was to get some eyes on what was going on during the boot.  This\nis what my boot-loader looked like I had been staring at it to move forward. \n\nInitial startup screen when my arch linux was stuck for more than 30 minutes.As\nthey say, \n\n> All things come to those who wait.\nI would have believed that quote, if it was a weekend and I had all the day to\nwait. But right then, staring at the boot screen which seemed to be stuck for\nforever, wasn't helping me much.\n\nThat's when I changed the boot-loader script to enable the debug logging. \n\nHow: On startup screen, where it asks to choose the OS to start with, pressing e \ntakes one to the editor with GRUB boot-loader script, which is normally present\nin /boot/grub/grub.cfg in linux system.\n\nI removed the quiet flag  (highlighted in the image below) from the boot-loader\nscript, re-initiated the boot again, and let nature take its course.\n\nGRUB boot-loader script for linux with highlighted quiet flag.Now I could\nfinally see the logs of what was happening.\n\nInital startup screen after enabling boot logs.Huh!!! From what it looks like,\nit seems like there is a start job for File System Check that seems to be taking\nforever to finish. \n\nThat made me wonder, how could I bypass this file system check during the boot.\nThis would at-least enable me to access my File System, make necessary changes\nand hopefully fix the problem. \n\nSkipping the blocking job during boot.\nAfter a bit of googling, I added fsck.mode=skip flag into my boot-loader script.\nThis would enable skipping the File System Checks during the boot. This was\nadded in the same line, the quiet flag was removed in previous step. I initiated\nthe boot again and waited for something to happen. \n\nThis time it took me into maintainers mode where I could finally access my\ndevice as root. \n\nRighting the wrongs.\nFrom what I could see, my /dev/mapper/volgroup0-lv_home volume was not mounted.\nRunning File System Check for the volume also failed.\n\nfsck /dev/mapper/volgroup0-lv_home\n\n\nThis definitely proved that there is something wrong with the home volume.\n\nWhat went wrong: This has most probably happened, when the day before, I\nincreased the size of the root logical volume and shrank the size of my home\nlogical volume, referring this\n[https://wiki.archlinux.org/title/LVM#Resizing_the_logical_volume_and_file_system_separately] \ndoc. I must have done some mistake in calculating the sizes of the resulting\nvolumes. In my defense, it was already 10 p.m. at night when I did that.\n\nI was in a bit of rush, as the next morning, I had to start for work again and\nit wouldn't help if I was still not able to fix my computer. Hence I ended up\nformatting my /dev/mapper/volgroup0-lv_home logical volume using the following\ncommand: \n\nmkfs.ext4 /dev/volgroup0/lv_home\n\n\nRunning the File System Check on the new volume succeeded this time.\n\nThen I mounted the newly created logical volume on the home directory using:\n\nmount /dev/volgroup0/lv_home /home\n\n\nOne would think that this would be all, but there's still one step missing.\n\nThe UUID of the /dev/volgroup0/lv_home logical volume has changed because I had\ncreated a new File System.  We need to tell the File System to check for new\nUUID during boot. Runnig blkid command\n[https://linoxide.com/linux-command-lsblk-blkid/#:~:text=The%20blkid%20program%20is%20a,e.g.%20LABEL%20or%20UUID%20fields).] \ngives the list of all the block devices in the machine with their UUIDs.\n\nList of all the block devices in arch linux with their UUIDs - the output of\nblkid command.I had to copy over the UUID of /dev/mapper/volgroup0-lv_home volume and replace\nit with the UUID in /etc/fstab file for the given volume group.\n\nRebooting the machine.\nHURRAY! This time rebooting the machine actually took me to the home login\nscreen.\n\nI had to however create a new user, as all the config files for the apps, which\nwere present in the `/home` directory for my old user didn't exist anymore. But\nthat was an OKish compromise. The good thing was that I didn't have to\nre-install all the packages as the root volume was still intact.","feature_image":"__GHOST_URL__/content/images/2022/01/dead-computer-monitor.jpg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-01-13T04:40:16.000Z","updated_at":"2022-07-04T21:37:21.000Z","published_at":"2022-01-23T18:17:18.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"61fcc2ac021a27000172f42d","uuid":"a1dfb1b9-4eaf-40d8-9578-0536fa05da5a","title":"Load Testing with Apache JMeter.","slug":"load-testing-with-apache-jmeter","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*mpcQxJWwaewkC5k7EFgZXA.png\",\"alt\":\"\",\"title\":\"\",\"caption\":\"Creating apache jmeter test plan\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*4C6xUVNxFauxKRCnDyuaqw.png\",\"alt\":\"Instructions for apache jmeter test plan.\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*F9CE4Xdt4OJ-9EaNE3PSNg.png\",\"alt\":\"Setting up thread groups for Apache JMeter test plan.\",\"title\":\"\",\"caption\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*BbwYHEgNxx8K8m88GE3V0w.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*NAQVsME41lZqkUBDTQvJyA.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*Gx6m6rxStv7VG-RcLxiLfg.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*9EzLudJaJ_adNOJSjTPLcw.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*DKvoKRDZIoEzXDbkJKWwHA.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*Nf_hf8mPMUs6wozLLDl26w.png\",\"alt\":\"\",\"title\":\"\"}],[\"image\",{\"src\":\"https://cdn-images-1.medium.com/max/800/1*gTxafeDyepYkqeD_57AerA.png\",\"alt\":\"\",\"title\":\"\"}]],\"markups\":[[\"a\",[\"href\",\"https://medium.com/@kaur.harsimran301/load-testing-tool-comparison-siege-vs-apache-jmeter-b74d9493493c\"]],[\"strong\"],[\"a\",[\"href\",\"http://jmeter.apache.org/download_jmeter.cgi\",\"rel\",\"noopener\"]],[\"a\",[\"href\",\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Keep-Alive\",\"rel\",\"noopener\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"Now that we have learnt about the difference between Siege and Apache JMeter in the \"],[0,[0],1,\"previous article\"],[0,[],0,\", it’s time to talk about running basic HTTP api load tests with JMeter.\"]]],[1,\"p\",[[0,[],0,\"JMeter is a \"],[0,[1],1,\"pure Java\"],[0,[],0,\" application and should run correctly on any system that has a compatible Java implementation.\"]]],[1,\"p\",[[0,[],0,\"One can download and install apache JMeter from \"],[0,[2],1,\"here\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"Open it and let’s get started. You’ll see an empty Test Plan as below.\"]]],[10,0],[1,\"p\",[[0,[1],1,\"Test Plan\"],[0,[],0,\" is where you can describe the name of your tests and add comments about it. We’ll keep the name Test Plan for the sake of reference.\"]]],[1,\"p\",[[0,[],0,\"Right click on the \"],[0,[1],1,\"Test Plan\"],[0,[],0,\" and add \"],[0,[1],1,\"Thread Groups\"],[0,[],0,\" as shown below:\"]]],[10,1],[1,\"p\",[[0,[],0,\"A Thread Group is where you specify the number of users that you want to simulate. One thread = one user.\"]]],[1,\"p\",[[0,[1],1,\"Number of threads\"],[0,[],0,\": It is where you specify the number of users that you want to simulate. One thread = one user.\"]]],[1,\"p\",[[0,[1],1,\"Ramp Up Period\"],[0,[],0,\": By setting the ramp-up period, you can also tell JMeter how long it should take to reach all of the threads that you’ve chosen.\"]]],[1,\"p\",[[0,[1],1,\"Loop Count\"],[0,[],0,\": You can set the number of iterations for each user in the group using Loop Count.\"]]],[10,2],[1,\"p\",[[0,[],0,\"For example, the above configurations with:\"]]],[1,\"p\",[[0,[],0,\"Number of threads: 10, Ramp-Up period: 10 seconds and Loop Count: 1 means that JMeter will take 10 seconds to get all the 10 threads up and running. Each thread will start after 1 (10/10) second after the previous thread had begun. A total of 10 requests will be made, since loop count is 1.\"]]],[1,\"p\",[[0,[],0,\"Note that the ramp up period is obeyed only for the first loop. If the loop count was 2, the second request for each thread would start after the response of first one is received.\"]]],[1,\"p\",[[0,[],0,\"Each thread will wait for the response from previous call and start the next request till the number of loop counts are over.\"]]],[1,\"p\",[[0,[],0,\"The loop count can be infinite and one can run concurrent threads for a specific amount of time by checking on \"],[0,[1],1,\"Specify Thread Lifetime\"],[0,[],0,\" and adding \"],[0,[1],1,\"Duration \"],[0,[],0,\"say 60 seconds.\"]]],[1,\"p\",[[0,[],0,\"Now that we have specified Thread Group, we can specify what that thread is actually going to execute, say an HTTP request to a server in our case.\"]]],[10,3],[1,\"p\",[[0,[],0,\"One can configure a simple HTTP request as given below.\"]]],[10,4],[1,\"p\",[[0,[1],1,\"Use KeepAlive \"],[0,[],0,\"sets header \"],[0,[3],1,\"Keep-Alive\"],[0,[],0,\" in http request, which would mean that only initial request will have latency involved with connection setup, the subsequent requests will use the same TCP socket. Hence the tests will check the real load the application can handle, without taking into account the latencies to establish a TCP handshake.\"]]],[1,\"p\",[[0,[],0,\"In case one wants to add http header like an Authorization token, one can do so by adding a \"],[0,[1],1,\"Config Element\"],[0,[],0,\" like \"],[0,[1],1,\"HTTP Header Manager.\"]]],[10,5],[10,6],[1,\"p\",[[0,[],0,\"Now that we have an HTTP Request ready, we need a setup to analyse our test results. JMeter by default has a number of options for listeners. \"],[0,[1],1,\"View Results Tree\"],[0,[],0,\" is used very frequently since it shows the response of each request and hence makes it easier to detect in case something goes wrong. In addition to it, one can use \"],[0,[1],1,\"Aggregate or Summary Report\"],[0,[],0,\" to get a general idea about overall throughput and performance of the apis.\"]]],[10,7],[10,8],[1,\"p\",[[0,[],0,\"Now that we have 2 listeners set up, we can start the test by clicking on start button. Once the tests are over, the results can be seen in the listener sections.\"]]],[10,9],[1,\"p\",[[0,[],0,\"Hope this acts as a good introduction to JMeter. Reach out to me in the comments section in case you have further queries.\"]]],[1,\"p\",[[0,[],0,\"Happy Load Testing!\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Now that we have learnt about the difference between Siege and Apache JMeter in the <a href=\"https://medium.com/@kaur.harsimran301/load-testing-tool-comparison-siege-vs-apache-jmeter-b74d9493493c\">previous article</a>, it’s time to talk about running basic HTTP api load tests with JMeter.</p><p>JMeter is a <strong>pure Java</strong> application and should run correctly on any system that has a compatible Java implementation.</p><p>One can download and install apache JMeter from <a href=\"http://jmeter.apache.org/download_jmeter.cgi\" rel=\"noopener\">here</a>.</p><p>Open it and let’s get started. You’ll see an empty Test Plan as below.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://cdn-images-1.medium.com/max/800/1*mpcQxJWwaewkC5k7EFgZXA.png\" class=\"kg-image\" alt loading=\"lazy\"><figcaption>Creating apache jmeter test plan</figcaption></figure><p><strong>Test Plan</strong> is where you can describe the name of your tests and add comments about it. We’ll keep the name Test Plan for the sake of reference.</p><p>Right click on the <strong>Test Plan</strong> and add <strong>Thread Groups</strong> as shown below:</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*4C6xUVNxFauxKRCnDyuaqw.png\" class=\"kg-image\" alt=\"Instructions for apache jmeter test plan.\" loading=\"lazy\"></figure><p>A Thread Group is where you specify the number of users that you want to simulate. One thread = one user.</p><p><strong>Number of threads</strong>: It is where you specify the number of users that you want to simulate. One thread = one user.</p><p><strong>Ramp Up Period</strong>: By setting the ramp-up period, you can also tell JMeter how long it should take to reach all of the threads that you’ve chosen.</p><p><strong>Loop Count</strong>: You can set the number of iterations for each user in the group using Loop Count.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*F9CE4Xdt4OJ-9EaNE3PSNg.png\" class=\"kg-image\" alt=\"Setting up thread groups for Apache JMeter test plan.\" loading=\"lazy\"></figure><p>For example, the above configurations with:</p><p>Number of threads: 10, Ramp-Up period: 10 seconds and Loop Count: 1 means that JMeter will take 10 seconds to get all the 10 threads up and running. Each thread will start after 1 (10/10) second after the previous thread had begun. A total of 10 requests will be made, since loop count is 1.</p><p>Note that the ramp up period is obeyed only for the first loop. If the loop count was 2, the second request for each thread would start after the response of first one is received.</p><p>Each thread will wait for the response from previous call and start the next request till the number of loop counts are over.</p><p>The loop count can be infinite and one can run concurrent threads for a specific amount of time by checking on <strong>Specify Thread Lifetime</strong> and adding <strong>Duration </strong>say 60 seconds.</p><p>Now that we have specified Thread Group, we can specify what that thread is actually going to execute, say an HTTP request to a server in our case.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*BbwYHEgNxx8K8m88GE3V0w.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>One can configure a simple HTTP request as given below.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*NAQVsME41lZqkUBDTQvJyA.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p><strong>Use KeepAlive </strong>sets header <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Keep-Alive\" rel=\"noopener\">Keep-Alive</a> in http request, which would mean that only initial request will have latency involved with connection setup, the subsequent requests will use the same TCP socket. Hence the tests will check the real load the application can handle, without taking into account the latencies to establish a TCP handshake.</p><p>In case one wants to add http header like an Authorization token, one can do so by adding a <strong>Config Element</strong> like <strong>HTTP Header Manager.</strong></p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*Gx6m6rxStv7VG-RcLxiLfg.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*9EzLudJaJ_adNOJSjTPLcw.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now that we have an HTTP Request ready, we need a setup to analyse our test results. JMeter by default has a number of options for listeners. <strong>View Results Tree</strong> is used very frequently since it shows the response of each request and hence makes it easier to detect in case something goes wrong. In addition to it, one can use <strong>Aggregate or Summary Report</strong> to get a general idea about overall throughput and performance of the apis.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*DKvoKRDZIoEzXDbkJKWwHA.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*Nf_hf8mPMUs6wozLLDl26w.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Now that we have 2 listeners set up, we can start the test by clicking on start button. Once the tests are over, the results can be seen in the listener sections.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://cdn-images-1.medium.com/max/800/1*gTxafeDyepYkqeD_57AerA.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><p>Hope this acts as a good introduction to JMeter. Reach out to me in the comments section in case you have further queries.</p><p>Happy Load Testing!</p>","comment_id":"61fcc2ac021a27000172f42d","plaintext":"Now that we have learnt about the difference between Siege and Apache JMeter in\nthe previous article\n[https://medium.com/@kaur.harsimran301/load-testing-tool-comparison-siege-vs-apache-jmeter-b74d9493493c]\n, it’s time to talk about running basic HTTP api load tests with JMeter.\n\nJMeter is a pure Java application and should run correctly on any system that\nhas a compatible Java implementation.\n\nOne can download and install apache JMeter from here\n[http://jmeter.apache.org/download_jmeter.cgi].\n\nOpen it and let’s get started. You’ll see an empty Test Plan as below.\n\nCreating apache jmeter test planTest Plan is where you can describe the name of your tests and add comments\nabout it. We’ll keep the name Test Plan for the sake of reference.\n\nRight click on the Test Plan and add Thread Groups as shown below:\n\nA Thread Group is where you specify the number of users that you want to\nsimulate. One thread = one user.\n\nNumber of threads: It is where you specify the number of users that you want to\nsimulate. One thread = one user.\n\nRamp Up Period: By setting the ramp-up period, you can also tell JMeter how long\nit should take to reach all of the threads that you’ve chosen.\n\nLoop Count: You can set the number of iterations for each user in the group\nusing Loop Count.\n\nFor example, the above configurations with:\n\nNumber of threads: 10, Ramp-Up period: 10 seconds and Loop Count: 1 means that\nJMeter will take 10 seconds to get all the 10 threads up and running. Each\nthread will start after 1 (10/10) second after the previous thread had begun. A\ntotal of 10 requests will be made, since loop count is 1.\n\nNote that the ramp up period is obeyed only for the first loop. If the loop\ncount was 2, the second request for each thread would start after the response\nof first one is received.\n\nEach thread will wait for the response from previous call and start the next\nrequest till the number of loop counts are over.\n\nThe loop count can be infinite and one can run concurrent threads for a specific\namount of time by checking on Specify Thread Lifetime and adding Duration say 60\nseconds.\n\nNow that we have specified Thread Group, we can specify what that thread is\nactually going to execute, say an HTTP request to a server in our case.\n\nOne can configure a simple HTTP request as given below.\n\nUse KeepAlive sets header Keep-Alive\n[https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Keep-Alive] in http\nrequest, which would mean that only initial request will have latency involved\nwith connection setup, the subsequent requests will use the same TCP socket.\nHence the tests will check the real load the application can handle, without\ntaking into account the latencies to establish a TCP handshake.\n\nIn case one wants to add http header like an Authorization token, one can do so\nby adding a Config Element like HTTP Header Manager.\n\nNow that we have an HTTP Request ready, we need a setup to analyse our test\nresults. JMeter by default has a number of options for listeners. View Results\nTree is used very frequently since it shows the response of each request and\nhence makes it easier to detect in case something goes wrong. In addition to it,\none can use Aggregate or Summary Report to get a general idea about overall\nthroughput and performance of the apis.\n\nNow that we have 2 listeners set up, we can start the test by clicking on start\nbutton. Once the tests are over, the results can be seen in the listener\nsections.\n\nHope this acts as a good introduction to JMeter. Reach out to me in the comments\nsection in case you have further queries.\n\nHappy Load Testing!","feature_image":"__GHOST_URL__/content/images/2022/02/load_testing.jpeg","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-02-04T06:07:40.000Z","updated_at":"2022-02-04T06:13:31.000Z","published_at":"2020-10-18T06:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"620078c9021a27000172f488","uuid":"b8c0fad8-5c1c-429d-a6ae-74960eb18589","title":"Point Of Sales Solutions","slug":"point-of-sales-solutions","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2022/02/POS.jpg\",\"width\":3840,\"height\":1762,\"cardWidth\":\"wide\"}]],\"markups\":[],\"sections\":[[10,0],[1,\"p\",[]]],\"ghostVersion\":\"4.0\"}","html":"<figure class=\"kg-card kg-image-card kg-width-wide\"><img src=\"__GHOST_URL__/content/images/2022/02/POS.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"2000\" height=\"918\" srcset=\"__GHOST_URL__/content/images/size/w600/2022/02/POS.jpg 600w, __GHOST_URL__/content/images/size/w1000/2022/02/POS.jpg 1000w, __GHOST_URL__/content/images/size/w1600/2022/02/POS.jpg 1600w, __GHOST_URL__/content/images/size/w2400/2022/02/POS.jpg 2400w\" sizes=\"(min-width: 1200px) 1200px\"></figure>","comment_id":"620078c9021a27000172f488","plaintext":"","feature_image":null,"featured":0,"type":"page","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-02-07T01:41:29.000Z","updated_at":"2022-02-07T19:59:43.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"62c35d9b4866590001862307","uuid":"6d130b96-80de-4621-a868-6f3a0456199b","title":"Learning Scala","slug":"learning-scala","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Here are some of the problems I faced and how I solved them:\"],[1,[],0,0],[1,[],0,1],[0,[],0,\"Even though IntelliJ is dope, I like Visual Studio Code more..\"]]],[1,\"p\",[[0,[],0,\"However I faced some issues with indexing and autosuggestions for Scala.\"]]],[1,\"p\",[[0,[],0,\"Visual studio codes comes with Metals plugin for Scala, which internally uses bloop to create a build server and compile and run code. However if a bloop project from some other project has run before and now you're opening another project, indexing and autocompletion will not work unless you kill that lingering process. Closing VSC alone doesn't shuts down that process, I had to restart my machine in order to fix that.\"]]],[1,\"p\",[]],[1,\"p\",[[0,[],0,\"Monoid's => What are they?\"]]],[1,\"p\",[[0,[],0,\"Reduction Tree => Read about\"]]],[1,\"p\",[[0,[],0,\"TrieMap => \"]]],[1,\"p\",[]],[1,\"p\",[[1,[],0,2]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Here are some of the problems I faced and how I solved them:<br><br>Even though IntelliJ is dope, I like Visual Studio Code more..</p><p>However I faced some issues with indexing and autosuggestions for Scala.</p><p>Visual studio codes comes with Metals plugin for Scala, which internally uses bloop to create a build server and compile and run code. However if a bloop project from some other project has run before and now you're opening another project, indexing and autocompletion will not work unless you kill that lingering process. Closing VSC alone doesn't shuts down that process, I had to restart my machine in order to fix that.</p><p></p><p>Monoid's =&gt; What are they?</p><p>Reduction Tree =&gt; Read about</p><p>TrieMap =&gt; </p><p></p>","comment_id":"62c35d9b4866590001862307","plaintext":"Here are some of the problems I faced and how I solved them:\n\nEven though IntelliJ is dope, I like Visual Studio Code more..\n\nHowever I faced some issues with indexing and autosuggestions for Scala.\n\nVisual studio codes comes with Metals plugin for Scala, which internally uses\nbloop to create a build server and compile and run code. However if a bloop\nproject from some other project has run before and now you're opening another\nproject, indexing and autocompletion will not work unless you kill that\nlingering process. Closing VSC alone doesn't shuts down that process, I had to\nrestart my machine in order to fix that.\n\n\n\nMonoid's => What are they?\n\nReduction Tree => Read about\n\nTrieMap =>","feature_image":null,"featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2022-07-04T21:37:31.000Z","updated_at":"2022-07-07T15:21:56.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64a5eff82f69e4000100a52b","uuid":"0504bdb3-acb6-46a6-8b54-983f3ba9d394","title":"Book Copy and Cover Creation Automation Tool","slug":"copy-and-cover-creation-automation-tool","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}]],\"markups\":[[\"strong\"]],\"sections\":[[1,\"h3\",[]],[1,\"p\",[[0,[0],1,\"Client Overview\"],[1,[],0,0],[0,[],0,\"Our client is an independent contractor who collaborates closely with a publisher, specializing in creating book covers. They sought a solution to streamline the writing of web copy and book covers by automating the data intake process from emails.\"]]],[10,0],[1,\"p\",[[0,[0],1,\"Challenge\"],[1,[],0,1],[0,[],0,\"The client faced challenges in efficiently extracting relevant information from semi-structured email data to create compelling book cover designs and copy. The manual process was time-consuming and prone to errors, leading to delays in project delivery.\"]]],[1,\"p\",[[0,[0],1,\"Solution\"],[1,[],0,2],[0,[],0,\"To address these challenges, we developed a robust automation tool utilizing \"],[0,[0],1,\"Electron.js\"],[0,[],0,\" to create a cross-platform desktop application. This solution ensured compatibility with various operating systems, catering to the client's preferences.\"]]],[1,\"p\",[[0,[0],1,\"Key Features of the Solution:\"]]],[3,\"ol\",[[[0,[0],1,\"Email Parsing\"],[0,[],0,\":\"],[1,[],0,3],[0,[],0,\"We integrated \"],[0,[0],1,\"Google Email APIs\"],[0,[],0,\" to capture and parse incoming emails that met specific criteria. This allowed us to automatically extract relevant attachments containing deliverable data.\"]],[[0,[0],1,\"Data Extraction and Organization\"],[0,[],0,\":\"],[1,[],0,4],[0,[],0,\"The tool was designed to download and organize files from email attachments, extracting critical information such as due dates and types of copy required based on filenames. The extracted data was organized into unique folders for easy access.\"]],[[0,[0],1,\"Document Classification\"],[0,[],0,\":\"],[1,[],0,5],[0,[],0,\"We implemented a system to classify source documents into categories (e.g., Tipsheets, Author Information Sheets, Proposals). This classification enabled efficient data handling and retrieval.\"]],[[0,[0],1,\"Database Integration\"],[0,[],0,\":\"],[1,[],0,6],[0,[],0,\"The relevant information, including ISBN, book titles, author details, and pricing, was populated into a structured format within a database or spreadsheet, ensuring easy management and accessibility.\"]],[[0,[0],1,\"Copy Generation with OpenAI\"],[0,[],0,\":\"],[1,[],0,7],[0,[],0,\"We utilized \"],[0,[0],1,\"OpenAI APIs\"],[0,[],0,\" to generate the initial draft of the copy. By feeding the system appropriate prompts and data, we leveraged AI capabilities to create compelling copy based on specific templates tailored to each project.\"]],[[0,[0],1,\"Template Population\"],[0,[],0,\":\"],[1,[],0,8],[0,[],0,\"The application automated the process of populating various templates with the generated copy, ensuring that each book cover met the specific requirements provided by the client.\"]]]],[1,\"p\",[[0,[0],1,\"Results\"],[1,[],0,9],[0,[],0,\"The implementation of the automation tool significantly streamlined the client's workflow, reducing the time spent on manual data extraction and copy creation. The automated solution provided the following benefits:\"]]],[3,\"ul\",[[[0,[0],1,\"Increased Efficiency\"],[0,[],0,\": The client experienced a substantial reduction in project turnaround times, allowing for more projects to be completed within shorter timeframes.\"]],[[0,[0],1,\"Enhanced Accuracy\"],[0,[],0,\": Automation minimized human error in data extraction and copy generation, ensuring higher quality output.\"]],[[0,[0],1,\"Improved Collaboration\"],[0,[],0,\": The structured data organization facilitated better communication and collaboration between the client and the publisher, fostering a smoother workflow.\"]]]],[1,\"p\",[[0,[0],1,\"Conclusion\"],[1,[],0,10],[0,[],0,\"By developing a comprehensive copy and cover creation automation tool, we empowered our client to overcome their operational challenges and enhance their productivity. The integration of advanced technologies like Electron.js and OpenAI APIs not only streamlined their processes but also set the stage for future scalability and growth in their publishing endeavors.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<h3></h3><p><strong>Client Overview</strong><br>Our client is an independent contractor who collaborates closely with a publisher, specializing in creating book covers. They sought a solution to streamline the writing of web copy and book covers by automating the data intake process from emails.</p><hr><p><strong>Challenge</strong><br>The client faced challenges in efficiently extracting relevant information from semi-structured email data to create compelling book cover designs and copy. The manual process was time-consuming and prone to errors, leading to delays in project delivery.</p><p><strong>Solution</strong><br>To address these challenges, we developed a robust automation tool utilizing <strong>Electron.js</strong> to create a cross-platform desktop application. This solution ensured compatibility with various operating systems, catering to the client's preferences.</p><p><strong>Key Features of the Solution:</strong></p><ol><li><strong>Email Parsing</strong>:<br>We integrated <strong>Google Email APIs</strong> to capture and parse incoming emails that met specific criteria. This allowed us to automatically extract relevant attachments containing deliverable data.</li><li><strong>Data Extraction and Organization</strong>:<br>The tool was designed to download and organize files from email attachments, extracting critical information such as due dates and types of copy required based on filenames. The extracted data was organized into unique folders for easy access.</li><li><strong>Document Classification</strong>:<br>We implemented a system to classify source documents into categories (e.g., Tipsheets, Author Information Sheets, Proposals). This classification enabled efficient data handling and retrieval.</li><li><strong>Database Integration</strong>:<br>The relevant information, including ISBN, book titles, author details, and pricing, was populated into a structured format within a database or spreadsheet, ensuring easy management and accessibility.</li><li><strong>Copy Generation with OpenAI</strong>:<br>We utilized <strong>OpenAI APIs</strong> to generate the initial draft of the copy. By feeding the system appropriate prompts and data, we leveraged AI capabilities to create compelling copy based on specific templates tailored to each project.</li><li><strong>Template Population</strong>:<br>The application automated the process of populating various templates with the generated copy, ensuring that each book cover met the specific requirements provided by the client.</li></ol><p><strong>Results</strong><br>The implementation of the automation tool significantly streamlined the client's workflow, reducing the time spent on manual data extraction and copy creation. The automated solution provided the following benefits:</p><ul><li><strong>Increased Efficiency</strong>: The client experienced a substantial reduction in project turnaround times, allowing for more projects to be completed within shorter timeframes.</li><li><strong>Enhanced Accuracy</strong>: Automation minimized human error in data extraction and copy generation, ensuring higher quality output.</li><li><strong>Improved Collaboration</strong>: The structured data organization facilitated better communication and collaboration between the client and the publisher, fostering a smoother workflow.</li></ul><p><strong>Conclusion</strong><br>By developing a comprehensive copy and cover creation automation tool, we empowered our client to overcome their operational challenges and enhance their productivity. The integration of advanced technologies like Electron.js and OpenAI APIs not only streamlined their processes but also set the stage for future scalability and growth in their publishing endeavors.</p>","comment_id":"64a5eff82f69e4000100a52b","plaintext":"\nClient Overview\nOur client is an independent contractor who collaborates closely with a\npublisher, specializing in creating book covers. They sought a solution to\nstreamline the writing of web copy and book covers by automating the data intake\nprocess from emails.\n\n\n--------------------------------------------------------------------------------\n\nChallenge\nThe client faced challenges in efficiently extracting relevant information from\nsemi-structured email data to create compelling book cover designs and copy. The\nmanual process was time-consuming and prone to errors, leading to delays in\nproject delivery.\n\nSolution\nTo address these challenges, we developed a robust automation tool utilizing \nElectron.js to create a cross-platform desktop application. This solution\nensured compatibility with various operating systems, catering to the client's\npreferences.\n\nKey Features of the Solution:\n\n 1. Email Parsing:\n    We integrated Google Email APIs to capture and parse incoming emails that\n    met specific criteria. This allowed us to automatically extract relevant\n    attachments containing deliverable data.\n 2. Data Extraction and Organization:\n    The tool was designed to download and organize files from email attachments,\n    extracting critical information such as due dates and types of copy required\n    based on filenames. The extracted data was organized into unique folders for\n    easy access.\n 3. Document Classification:\n    We implemented a system to classify source documents into categories (e.g.,\n    Tipsheets, Author Information Sheets, Proposals). This classification\n    enabled efficient data handling and retrieval.\n 4. Database Integration:\n    The relevant information, including ISBN, book titles, author details, and\n    pricing, was populated into a structured format within a database or\n    spreadsheet, ensuring easy management and accessibility.\n 5. Copy Generation with OpenAI:\n    We utilized OpenAI APIs to generate the initial draft of the copy. By\n    feeding the system appropriate prompts and data, we leveraged AI\n    capabilities to create compelling copy based on specific templates tailored\n    to each project.\n 6. Template Population:\n    The application automated the process of populating various templates with\n    the generated copy, ensuring that each book cover met the specific\n    requirements provided by the client.\n\nResults\nThe implementation of the automation tool significantly streamlined the client's\nworkflow, reducing the time spent on manual data extraction and copy creation.\nThe automated solution provided the following benefits:\n\n * Increased Efficiency: The client experienced a substantial reduction in\n   project turnaround times, allowing for more projects to be completed within\n   shorter timeframes.\n * Enhanced Accuracy: Automation minimized human error in data extraction and\n   copy generation, ensuring higher quality output.\n * Improved Collaboration: The structured data organization facilitated better\n   communication and collaboration between the client and the publisher,\n   fostering a smoother workflow.\n\nConclusion\nBy developing a comprehensive copy and cover creation automation tool, we\nempowered our client to overcome their operational challenges and enhance their\nproductivity. The integration of advanced technologies like Electron.js and\nOpenAI APIs not only streamlined their processes but also set the stage for\nfuture scalability and growth in their publishing endeavors.","feature_image":"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-06-at-6.27.17-PM.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-07-05T22:34:32.000Z","updated_at":"2024-10-07T00:27:34.000Z","published_at":"2024-02-25T01:26:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64a9e5fa2f69e4000100a530","uuid":"f6b91916-bda8-460e-bed1-38941351ee24","title":"(Untitled)","slug":"case-studies","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"h2\",[]]],\"ghostVersion\":\"4.0\"}","html":"<h2></h2>","comment_id":"64a9e5fa2f69e4000100a530","plaintext":"","feature_image":null,"featured":0,"type":"page","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-07-08T22:40:58.000Z","updated_at":"2023-07-08T22:54:14.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"64a9e91b2f69e4000100a560","uuid":"95e3deaa-251f-47c5-8197-ff23d675bf90","title":"Revolutionizing Insurance Quoting with an Intuitive App","slug":"insurance-quoting-system","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://www.sgscoverage.com/\"]],[\"a\",[\"href\",\"https://docs.compulife.com/\"]],[\"strong\"]],\"sections\":[[1,\"blockquote\",[[0,[],0,\"A&H Solutions, through its founder Harsimran, did a fantastic job helping to configure a life insurance quote engine for my insurance brokerage.  Their responsiveness, patience, and diligence is top-notch.\"],[1,[],0,0],[0,[],0,\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t   - Clay Owens, \"],[0,[0],1,\"https://www.sgscoverage.com/\"]]],[1,\"h3\",[[1,[],0,1],[0,[],0,\"Introduction\"]]],[1,\"p\",[[0,[],0,\"In this case study, we will explore how our team developed and implemented an Insurance Quoting System for an insurance provider using a single-page React app. We will discuss the problem our product solved and the significant impact it had on their business operations.\"]]],[1,\"h3\",[[0,[],0,\"Problem Statement\"]]],[1,\"p\",[[0,[],0,\"Our client, an insurance provider, faced challenges in providing accurate and timely insurance quotes to their customers. The existing manual process was time-consuming, error-prone, and lacked the ability to quickly retrieve quotes from multiple insurance providers. This hindered the company's ability to offer competitive rates and meet customer expectations.\"]]],[1,\"h3\",[[0,[],0,\"Solution\"]]],[1,\"p\",[[0,[],0,\"To address the client's problem, our team developed a cutting-edge Insurance Quoting System using React, a popular JavaScript library for building user interfaces. The system streamlined the quoting process by automating data collection, integrating with \"],[0,[1],1,\"Compulife APIs\"],[0,[],0,\" for instant quote retrieval, and presenting the results to the user in a clear and user-friendly manner.\"]]],[1,\"h3\",[[0,[],0,\"Key Features\"]]],[1,\"p\",[[0,[2],1,\"User Information Collection\"],[1,[],0,2],[0,[],0,\"The app allowed users to input their personal information and insurance requirements through an intuitive interface. We expanded the system to include a contact form that captured additional user information, such as name, email address, and phone number. This enhanced the user experience and allowed the insurance provider to follow up with potential customers effectively.\"]]],[1,\"p\",[[0,[2],1,\"Compulife API Integration\"],[1,[],0,3],[0,[],0,\"Our system seamlessly integrated with Compulife APIs to retrieve real-time insurance quotes from multiple providers. This integration ensured that users received accurate and up-to-date information, enhancing the trustworthiness of the quotes provided.\"]]],[1,\"p\",[[0,[2],1,\"Instant Quote Generation\"],[1,[],0,4],[0,[],0,\"By leveraging the power of the Compulife APIs, our app rapidly generated quotes based on the user's input, providing instant feedback and reducing the time required for customers to obtain insurance quotes.\"]]],[1,\"p\",[[0,[2],1,\"User-Friendly Interface\"],[1,[],0,5],[0,[],0,\" The single-page React app offered a responsive and visually appealing user interface that guided users through the quoting process. It presented the results in a comprehensible format, enabling users to make informed decisions about their insurance needs.\"]]],[1,\"h3\",[[0,[],0,\"Implementation and Impact\"],[1,[],0,6]]],[1,\"p\",[[0,[2],1,\"Development Effort\"],[1,[],0,7],[0,[],0,\"Our team dedicated a total of 40 hours to develop and refine the Insurance Quoting System. This included designing the user interface, integrating the Compulife APIs, implementing data validation, creating the contact form, and conducting thorough testing.\"]]],[1,\"p\",[[0,[2],1,\"Enhanced Efficiency\"],[1,[],0,8],[0,[],0,\"The introduction of the Insurance Quoting System significantly improved the efficiency of the insurance provider's operations. By automating the quoting process and reducing manual data entry, the app saved valuable time for both the customers and the insurance provider's staff.\"]]],[1,\"p\",[[0,[2],1,\"Increased Customer Satisfaction\"],[1,[],0,9],[0,[],0,\"The system's ability to provide instant, accurate quotes empowered customers to make informed decisions regarding their insurance coverage. Customers appreciated the quick turnaround time and the transparency in the quoting process, leading to higher customer satisfaction rates. Additionally, the inclusion of a contact form allowed the insurance provider to gather additional user information and effectively follow up with potential customers, further enhancing the overall customer experience.\"]]],[1,\"p\",[[0,[2],1,\"Competitive Advantage\"],[1,[],0,10],[0,[],0,\"With the Insurance Quoting System in place, the insurance provider gained a competitive edge in the market. The ability to offer real-time quotes from multiple providers gave them a broader range of options to present to their customers. This resulted in increased conversions and revenue for the company.\"]]],[1,\"p\",[[0,[2],1,\"Scalability and Adaptability\"],[1,[],0,11],[0,[],0,\"The single-page React app architecture ensured scalability and adaptability for future enhancements and feature additions. The system could easily be extended to include additional insurance products, integrate with other APIs, or incorporate more sophisticated algorithms for quote calculation.\"]]],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"Through the development and implementation of our Insurance Quoting System, our client experienced a transformative impact on their business. By automating the quoting process, integrating with Compulife APIs, providing an intuitive user interface, and adding a contact form, the system enhanced efficiency, increased customer satisfaction, and gave the insurance provider a competitive advantage in the market. The single-page React app architecture ensured scalability, setting the stage for future growth and innovation in the insurance industry.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<blockquote>A&amp;H Solutions, through its founder Harsimran, did a fantastic job helping to configure a life insurance quote engine for my insurance brokerage.  Their responsiveness, patience, and diligence is top-notch.<br>\t\t\t\t\t\t\t\t\t\t\t\t   - Clay Owens, <a href=\"https://www.sgscoverage.com/\">https://www.sgscoverage.com/</a></blockquote><h3 id=\"introduction\"><br>Introduction</h3><p>In this case study, we will explore how our team developed and implemented an Insurance Quoting System for an insurance provider using a single-page React app. We will discuss the problem our product solved and the significant impact it had on their business operations.</p><h3 id=\"problem-statement\">Problem Statement</h3><p>Our client, an insurance provider, faced challenges in providing accurate and timely insurance quotes to their customers. The existing manual process was time-consuming, error-prone, and lacked the ability to quickly retrieve quotes from multiple insurance providers. This hindered the company's ability to offer competitive rates and meet customer expectations.</p><h3 id=\"solution\">Solution</h3><p>To address the client's problem, our team developed a cutting-edge Insurance Quoting System using React, a popular JavaScript library for building user interfaces. The system streamlined the quoting process by automating data collection, integrating with <a href=\"https://docs.compulife.com/\">Compulife APIs</a> for instant quote retrieval, and presenting the results to the user in a clear and user-friendly manner.</p><h3 id=\"key-features\">Key Features</h3><p><strong>User Information Collection</strong><br>The app allowed users to input their personal information and insurance requirements through an intuitive interface. We expanded the system to include a contact form that captured additional user information, such as name, email address, and phone number. This enhanced the user experience and allowed the insurance provider to follow up with potential customers effectively.</p><p><strong>Compulife API Integration</strong><br>Our system seamlessly integrated with Compulife APIs to retrieve real-time insurance quotes from multiple providers. This integration ensured that users received accurate and up-to-date information, enhancing the trustworthiness of the quotes provided.</p><p><strong>Instant Quote Generation</strong><br>By leveraging the power of the Compulife APIs, our app rapidly generated quotes based on the user's input, providing instant feedback and reducing the time required for customers to obtain insurance quotes.</p><p><strong>User-Friendly Interface</strong><br> The single-page React app offered a responsive and visually appealing user interface that guided users through the quoting process. It presented the results in a comprehensible format, enabling users to make informed decisions about their insurance needs.</p><h3 id=\"implementation-and-impact\">Implementation and Impact<br></h3><p><strong>Development Effort</strong><br>Our team dedicated a total of 40 hours to develop and refine the Insurance Quoting System. This included designing the user interface, integrating the Compulife APIs, implementing data validation, creating the contact form, and conducting thorough testing.</p><p><strong>Enhanced Efficiency</strong><br>The introduction of the Insurance Quoting System significantly improved the efficiency of the insurance provider's operations. By automating the quoting process and reducing manual data entry, the app saved valuable time for both the customers and the insurance provider's staff.</p><p><strong>Increased Customer Satisfaction</strong><br>The system's ability to provide instant, accurate quotes empowered customers to make informed decisions regarding their insurance coverage. Customers appreciated the quick turnaround time and the transparency in the quoting process, leading to higher customer satisfaction rates. Additionally, the inclusion of a contact form allowed the insurance provider to gather additional user information and effectively follow up with potential customers, further enhancing the overall customer experience.</p><p><strong>Competitive Advantage</strong><br>With the Insurance Quoting System in place, the insurance provider gained a competitive edge in the market. The ability to offer real-time quotes from multiple providers gave them a broader range of options to present to their customers. This resulted in increased conversions and revenue for the company.</p><p><strong>Scalability and Adaptability</strong><br>The single-page React app architecture ensured scalability and adaptability for future enhancements and feature additions. The system could easily be extended to include additional insurance products, integrate with other APIs, or incorporate more sophisticated algorithms for quote calculation.</p><h3 id=\"conclusion\">Conclusion</h3><p>Through the development and implementation of our Insurance Quoting System, our client experienced a transformative impact on their business. By automating the quoting process, integrating with Compulife APIs, providing an intuitive user interface, and adding a contact form, the system enhanced efficiency, increased customer satisfaction, and gave the insurance provider a competitive advantage in the market. The single-page React app architecture ensured scalability, setting the stage for future growth and innovation in the insurance industry.</p>","comment_id":"64a9e91b2f69e4000100a560","plaintext":"> A&H Solutions, through its founder Harsimran, did a fantastic job helping to\nconfigure a life insurance quote engine for my insurance brokerage.  Their\nresponsiveness, patience, and diligence is top-notch.\n- Clay Owens, https://www.sgscoverage.com/\n\nIntroduction\nIn this case study, we will explore how our team developed and implemented an\nInsurance Quoting System for an insurance provider using a single-page React\napp. We will discuss the problem our product solved and the significant impact\nit had on their business operations.\n\nProblem Statement\nOur client, an insurance provider, faced challenges in providing accurate and\ntimely insurance quotes to their customers. The existing manual process was\ntime-consuming, error-prone, and lacked the ability to quickly retrieve quotes\nfrom multiple insurance providers. This hindered the company's ability to offer\ncompetitive rates and meet customer expectations.\n\nSolution\nTo address the client's problem, our team developed a cutting-edge Insurance\nQuoting System using React, a popular JavaScript library for building user\ninterfaces. The system streamlined the quoting process by automating data\ncollection, integrating with Compulife APIs [https://docs.compulife.com/] for\ninstant quote retrieval, and presenting the results to the user in a clear and\nuser-friendly manner.\n\nKey Features\nUser Information Collection\nThe app allowed users to input their personal information and insurance\nrequirements through an intuitive interface. We expanded the system to include a\ncontact form that captured additional user information, such as name, email\naddress, and phone number. This enhanced the user experience and allowed the\ninsurance provider to follow up with potential customers effectively.\n\nCompulife API Integration\nOur system seamlessly integrated with Compulife APIs to retrieve real-time\ninsurance quotes from multiple providers. This integration ensured that users\nreceived accurate and up-to-date information, enhancing the trustworthiness of\nthe quotes provided.\n\nInstant Quote Generation\nBy leveraging the power of the Compulife APIs, our app rapidly generated quotes\nbased on the user's input, providing instant feedback and reducing the time\nrequired for customers to obtain insurance quotes.\n\nUser-Friendly Interface\nThe single-page React app offered a responsive and visually appealing user\ninterface that guided users through the quoting process. It presented the\nresults in a comprehensible format, enabling users to make informed decisions\nabout their insurance needs.\n\nImplementation and Impact\n\nDevelopment Effort\nOur team dedicated a total of 40 hours to develop and refine the Insurance\nQuoting System. This included designing the user interface, integrating the\nCompulife APIs, implementing data validation, creating the contact form, and\nconducting thorough testing.\n\nEnhanced Efficiency\nThe introduction of the Insurance Quoting System significantly improved the\nefficiency of the insurance provider's operations. By automating the quoting\nprocess and reducing manual data entry, the app saved valuable time for both the\ncustomers and the insurance provider's staff.\n\nIncreased Customer Satisfaction\nThe system's ability to provide instant, accurate quotes empowered customers to\nmake informed decisions regarding their insurance coverage. Customers\nappreciated the quick turnaround time and the transparency in the quoting\nprocess, leading to higher customer satisfaction rates. Additionally, the\ninclusion of a contact form allowed the insurance provider to gather additional\nuser information and effectively follow up with potential customers, further\nenhancing the overall customer experience.\n\nCompetitive Advantage\nWith the Insurance Quoting System in place, the insurance provider gained a\ncompetitive edge in the market. The ability to offer real-time quotes from\nmultiple providers gave them a broader range of options to present to their\ncustomers. This resulted in increased conversions and revenue for the company.\n\nScalability and Adaptability\nThe single-page React app architecture ensured scalability and adaptability for\nfuture enhancements and feature additions. The system could easily be extended\nto include additional insurance products, integrate with other APIs, or\nincorporate more sophisticated algorithms for quote calculation.\n\nConclusion\nThrough the development and implementation of our Insurance Quoting System, our\nclient experienced a transformative impact on their business. By automating the\nquoting process, integrating with Compulife APIs, providing an intuitive user\ninterface, and adding a contact form, the system enhanced efficiency, increased\ncustomer satisfaction, and gave the insurance provider a competitive advantage\nin the market. The single-page React app architecture ensured scalability,\nsetting the stage for future growth and innovation in the insurance industry.","feature_image":"__GHOST_URL__/content/images/2023/08/Screenshot-2023-08-08-at-9.39.03-AM.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2023-07-08T22:54:19.000Z","updated_at":"2023-08-08T15:40:10.000Z","published_at":"2023-07-08T23:13:32.000Z","custom_excerpt":"Revolutionizing insurance quoting with our intuitive React app. Streamlined data collection, real-time quotes, increased efficiency, customer satisfaction, and competitiveness. Case study inside","codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"66f234ebde6f0100018c3a87","uuid":"13d5dc8e-0e58-4bcf-8f91-b811c66f3fd8","title":"Evaluating Data Integration with Oracle Autonomous Data Warehouse","slug":"evaluating-data-integration-with-oracle-autonomous-data-warehouse","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2024/09/Oracle-POC-Report.jpg\",\"width\":720,\"height\":540}]],\"markups\":[[\"strong\"],[\"a\",[\"href\",\"https://www.oracle.com/ca-en/autonomous-database/autonomous-data-warehouse/\"]],[\"a\",[\"href\",\"__GHOST_URL__/\"]]],\"sections\":[[1,\"p\",[[0,[0],1,\"Client:\"],[0,[],0,\" Aquanomix\"],[1,[],0,0],[0,[0],1,\"Industry:\"],[0,[],0,\" Technology/Water Management Services\"],[1,[],0,1],[0,[0],1,\"Project:\"],[0,[],0,\" Oracle \"],[0,[1],1,\"Autonomous Data Warehouse\"],[0,[],0,\" (ADW) Integration with MongoDB\"],[1,[],0,2],[0,[0],1,\"Duration:\"],[0,[],0,\" 15 days\"]]],[1,\"h3\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"Our client, Aquanomix, required a solution to data warehousing solution that integrated well with their existing production databases (Primarily MongoDB). their existing MongoDB database with Oracle Autonomous Data Warehouse (ADW) to enhance their data analytics capabilities. \"],[0,[2],1,\"A & H Solutions\"],[0,[],0,\" was engaged to conduct a Proof of Concept (POC) to evaluate the feasibility and performance of this integration.\"]]],[1,\"h3\",[[0,[],0,\"Objectives\"]]],[3,\"ul\",[[[0,[],0,\"Evaluate the process of importing MongoDB tables into Oracle ADW.\"]],[[0,[],0,\"Measure the performance and scalability of Oracle’s tools for data integration.\"]],[[0,[],0,\"Identify and address any challenges encountered during the integration process.\"]],[[0,[],0,\"Assess Oracle Cloud as a potential long-term partner for Aquanomix.\"]]]],[1,\"h3\",[[0,[],0,\"Process\"]]],[1,\"h4\",[[0,[],0,\"1. Environment Setup\"]]],[1,\"p\",[[0,[],0,\"We began by configuring the Oracle Autonomous Data Warehouse and establishing a connection with the existing MongoDB environment. This initial setup was crucial to ensure a stable and secure data flow.\"]]],[1,\"h4\",[[0,[],0,\"2. Table Selection and Data Migration\"]]],[1,\"p\",[[0,[],0,\"Three tables were selected from MongoDB for the integration based on their relevance. Due to the lack of a native MongoDB connector in Oracle's Data Integration Service, custom scripts were developed to handle the data migration. Oracle Data Loader was used for direct file import into the ADW, complementing our custom approach. Below diagrams represents the approach taken, vs the proposed long term approach and the ideal solution.\"],[1,[],0,3],[1,[],0,4]]],[10,0],[1,\"h4\",[[0,[],0,\"3. Collaboration with Oracle’s Internal Team\"]]],[1,\"p\",[[0,[],0,\"Given that Oracle’s platform is still evolving, particularly in areas like UI and client compatibility, we had to seek guidance from Oracle’s internal team. Their support was vital in addressing specific challenges and ensuring the integration was executed smoothly.\"]]],[1,\"h4\",[[0,[],0,\"4. Documentation and Analysis\"]]],[1,\"p\",[[0,[],0,\"The entire process was thoroughly documented, focusing on the challenges encountered and the solutions implemented. This provided a clear record of the integration process and offered insights for future projects.\"]]],[1,\"h3\",[[0,[],0,\"Results\"]]],[3,\"ul\",[[[0,[0],1,\"Successful Data Migration:\"],[0,[],0,\" All selected MongoDB tables were successfully imported into Oracle ADW.\"]],[[0,[0],1,\"Identification of Platform Limitations:\"],[0,[],0,\" The project highlighted areas where Oracle’s platform is still developing, particularly the absence of a MongoDB connector and some UI challenges.\"]]]],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"While the POC demonstrated that integrating MongoDB with Oracle ADW is feasible, the limitations encountered during the process—particularly the lack of a MongoDB connector and other UI issues — led to the decision not to proceed with Oracle as a long-term solution. The collaboration with Oracle’s internal team was helpful in navigating these challenges, but ultimately, the platform did not feel mature enough for a smooth developer experience. \"]]]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Client:</strong> Aquanomix<br><strong>Industry:</strong> Technology/Water Management Services<br><strong>Project:</strong> Oracle <a href=\"https://www.oracle.com/ca-en/autonomous-database/autonomous-data-warehouse/\">Autonomous Data Warehouse</a> (ADW) Integration with MongoDB<br><strong>Duration:</strong> 15 days</p><h3 id=\"overview\">Overview</h3><p>Our client, Aquanomix, required a solution to data warehousing solution that integrated well with their existing production databases (Primarily MongoDB). their existing MongoDB database with Oracle Autonomous Data Warehouse (ADW) to enhance their data analytics capabilities. <a href=\"__GHOST_URL__/\">A &amp; H Solutions</a> was engaged to conduct a Proof of Concept (POC) to evaluate the feasibility and performance of this integration.</p><h3 id=\"objectives\">Objectives</h3><ul><li>Evaluate the process of importing MongoDB tables into Oracle ADW.</li><li>Measure the performance and scalability of Oracle’s tools for data integration.</li><li>Identify and address any challenges encountered during the integration process.</li><li>Assess Oracle Cloud as a potential long-term partner for Aquanomix.</li></ul><h3 id=\"process\">Process</h3><h4 id=\"1-environment-setup\">1. Environment Setup</h4><p>We began by configuring the Oracle Autonomous Data Warehouse and establishing a connection with the existing MongoDB environment. This initial setup was crucial to ensure a stable and secure data flow.</p><h4 id=\"2-table-selection-and-data-migration\">2. Table Selection and Data Migration</h4><p>Three tables were selected from MongoDB for the integration based on their relevance. Due to the lack of a native MongoDB connector in Oracle's Data Integration Service, custom scripts were developed to handle the data migration. Oracle Data Loader was used for direct file import into the ADW, complementing our custom approach. Below diagrams represents the approach taken, vs the proposed long term approach and the ideal solution.<br><br></p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2024/09/Oracle-POC-Report.jpg\" class=\"kg-image\" alt loading=\"lazy\" width=\"720\" height=\"540\" srcset=\"__GHOST_URL__/content/images/size/w600/2024/09/Oracle-POC-Report.jpg 600w, __GHOST_URL__/content/images/2024/09/Oracle-POC-Report.jpg 720w\" sizes=\"(min-width: 720px) 720px\"></figure><h4 id=\"3-collaboration-with-oracle%E2%80%99s-internal-team\">3. Collaboration with Oracle’s Internal Team</h4><p>Given that Oracle’s platform is still evolving, particularly in areas like UI and client compatibility, we had to seek guidance from Oracle’s internal team. Their support was vital in addressing specific challenges and ensuring the integration was executed smoothly.</p><h4 id=\"4-documentation-and-analysis\">4. Documentation and Analysis</h4><p>The entire process was thoroughly documented, focusing on the challenges encountered and the solutions implemented. This provided a clear record of the integration process and offered insights for future projects.</p><h3 id=\"results\">Results</h3><ul><li><strong>Successful Data Migration:</strong> All selected MongoDB tables were successfully imported into Oracle ADW.</li><li><strong>Identification of Platform Limitations:</strong> The project highlighted areas where Oracle’s platform is still developing, particularly the absence of a MongoDB connector and some UI challenges.</li></ul><h3 id=\"conclusion\">Conclusion</h3><p>While the POC demonstrated that integrating MongoDB with Oracle ADW is feasible, the limitations encountered during the process—particularly the lack of a MongoDB connector and other UI issues — led to the decision not to proceed with Oracle as a long-term solution. The collaboration with Oracle’s internal team was helpful in navigating these challenges, but ultimately, the platform did not feel mature enough for a smooth developer experience. </p>","comment_id":"66f234ebde6f0100018c3a87","plaintext":"Client: Aquanomix\nIndustry: Technology/Water Management Services\nProject: Oracle Autonomous Data Warehouse\n[https://www.oracle.com/ca-en/autonomous-database/autonomous-data-warehouse/] \n(ADW) Integration with MongoDB\nDuration: 15 days\n\nOverview\nOur client, Aquanomix, required a solution to data warehousing solution that\nintegrated well with their existing production databases (Primarily MongoDB).\ntheir existing MongoDB database with Oracle Autonomous Data Warehouse (ADW) to\nenhance their data analytics capabilities. A & H Solutions [__GHOST_URL__/] was\nengaged to conduct a Proof of Concept (POC) to evaluate the feasibility and\nperformance of this integration.\n\nObjectives\n * Evaluate the process of importing MongoDB tables into Oracle ADW.\n * Measure the performance and scalability of Oracle’s tools for data\n   integration.\n * Identify and address any challenges encountered during the integration\n   process.\n * Assess Oracle Cloud as a potential long-term partner for Aquanomix.\n\nProcess\n1. Environment Setup\nWe began by configuring the Oracle Autonomous Data Warehouse and establishing a\nconnection with the existing MongoDB environment. This initial setup was crucial\nto ensure a stable and secure data flow.\n\n2. Table Selection and Data Migration\nThree tables were selected from MongoDB for the integration based on their\nrelevance. Due to the lack of a native MongoDB connector in Oracle's Data\nIntegration Service, custom scripts were developed to handle the data migration.\nOracle Data Loader was used for direct file import into the ADW, complementing\nour custom approach. Below diagrams represents the approach taken, vs the\nproposed long term approach and the ideal solution.\n\n\n\n3. Collaboration with Oracle’s Internal Team\nGiven that Oracle’s platform is still evolving, particularly in areas like UI\nand client compatibility, we had to seek guidance from Oracle’s internal team.\nTheir support was vital in addressing specific challenges and ensuring the\nintegration was executed smoothly.\n\n4. Documentation and Analysis\nThe entire process was thoroughly documented, focusing on the challenges\nencountered and the solutions implemented. This provided a clear record of the\nintegration process and offered insights for future projects.\n\nResults\n * Successful Data Migration: All selected MongoDB tables were successfully\n   imported into Oracle ADW.\n * Identification of Platform Limitations: The project highlighted areas where\n   Oracle’s platform is still developing, particularly the absence of a MongoDB\n   connector and some UI challenges.\n\nConclusion\nWhile the POC demonstrated that integrating MongoDB with Oracle ADW is feasible,\nthe limitations encountered during the process—particularly the lack of a\nMongoDB connector and other UI issues — led to the decision not to proceed with\nOracle as a long-term solution. The collaboration with Oracle’s internal team\nwas helpful in navigating these challenges, but ultimately, the platform did not\nfeel mature enough for a smooth developer experience.","feature_image":"__GHOST_URL__/content/images/2024/09/Data-Warehouse.png","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-09-24T03:41:31.000Z","updated_at":"2025-05-28T21:34:14.000Z","published_at":"2024-09-24T03:56:45.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"67031d9fde6f0100018c3abf","uuid":"aeead4b1-f0a7-425b-bdbf-6bb564af52c7","title":"Real Estate Investment App for Small-Scale Investors","slug":"real-estate-investment-app-for-small-scale-investors","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}]],\"markups\":[[\"strong\"]],\"sections\":[[1,\"p\",[[0,[0],1,\"Client:\"],[0,[],0,\" A real estate investment company building an app for small-scale investors\"]]],[1,\"p\",[[0,[0],1,\"Team:\"],[0,[],0,\" Full-stack Development Team (React Native & Golang)\"]]],[1,\"p\",[[0,[0],1,\"Project Overview:\"],[0,[],0,\"The client sought to develop a mobile application that would allow small-scale investors to manage their real estate portfolios with ease. The app needed to provide a comprehensive view of portfolio performance, track fund statuses in real-time, and present this data in an intuitive way. Our team was brought in to support the client with frontend and backend development using React Native and Golang, ensuring the project met its aggressive deadlines.\"]]],[1,\"p\",[[0,[0],1,\"Challenges:\"]]],[3,\"ol\",[[[0,[0],1,\"Real-Time Data Integration:\"],[1,[],0,0],[0,[],0,\"Investors needed to track their portfolio and investment performance in real time, making it necessary to build a highly responsive system capable of processing large volumes of data.\"]],[[0,[0],1,\"User Experience:\"],[1,[],0,1],[0,[],0,\"Since the target audience was non-technical small-scale investors, creating a user-friendly interface with easy navigation was critical.\"]],[[0,[0],1,\"Scalable Infrastructure:\"],[1,[],0,2],[0,[],0,\"With anticipated growth in the user base, the backend had to be scalable and performant, ensuring the system could handle increasing loads as the app expanded.\"]]]],[1,\"p\",[[0,[0],1,\"Solution:\"]]],[1,\"p\",[[0,[0],1,\"React Native for Cross-Platform Development:\"],[0,[],0,\"We leveraged React Native to create a mobile app that worked seamlessly across both iOS and Android platforms. React Native’s flexibility allowed us to maintain a single codebase for both platforms, which streamlined development and ensured consistent performance.\"]]],[3,\"ul\",[[[0,[0],1,\"Portfolio Dashboard:\"],[0,[],0,\" We developed a user-friendly dashboard that offered a comprehensive view of an investor’s portfolio. This included visual data representations like charts and graphs to show growth over time and property-specific details.\"]],[[0,[0],1,\"Fund Tracking:\"],[0,[],0,\" A dedicated section was created to display the status of users’ funds, including pending transactions, earnings, and overall returns. This helped investors make informed decisions with real-time information at their fingertips.\"]]]],[1,\"p\",[[0,[0],1,\"Golang for Backend Services:\"],[0,[],0,\"Our team utilized Golang to build a robust and efficient backend. Golang’s powerful concurrency model allowed us to manage real-time updates efficiently and ensure high performance under increasing data loads.\"]]],[3,\"ul\",[[[0,[0],1,\"API Development:\"],[0,[],0,\" We built secure and efficient RESTful APIs to handle user portfolio management, transaction history, and investment performance metrics. These APIs powered the core functionalities of the app and facilitated smooth data flow between the frontend and backend.\"]],[[0,[0],1,\"Database Integration:\"],[0,[],0,\" We integrated a PostgreSQL database to securely store user data, property details, and transactional information. The database design allowed for fast retrieval and updating of information, ensuring a smooth user experience.\"]],[[0,[0],1,\"Authentication & Security:\"],[0,[],0,\" Our team implemented a robust authentication system using JWT tokens, ensuring secure user login and data privacy throughout the app.\"]]]],[3,\"ol\",[[[0,[0],1,\"Real-Time Updates:\"],[0,[],0,\"To provide real-time investment performance updates, we implemented WebSocket technology, allowing instant notifications and updates on portfolio changes. This ensured users could track their investments in real time and respond quickly to market shifts.\"]],[[0,[0],1,\"Testing & Optimization:\"],[0,[],0,\"We employed comprehensive testing strategies, including unit tests, integration tests, and end-to-end testing to ensure the app functioned seamlessly under varying traffic loads. Additionally, we set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline to automate testing and deployment, enabling faster iterations and ensuring the app was always up-to-date.\"]]]],[1,\"p\",[[0,[0],1,\"Outcome:\"]]],[3,\"ul\",[[[0,[0],1,\"Timely Delivery:\"],[0,[],0,\" Our team successfully delivered the app within the client's timeline, ensuring they could meet their launch deadlines.\"]],[[0,[0],1,\"Positive User Reception:\"],[0,[],0,\" Early users praised the app for its intuitive design, ease of use, and the real-time updates, which allowed them to stay informed about their investments at all times.\"]],[[0,[0],1,\"Scalability and Performance:\"],[0,[],0,\" The app’s backend architecture proved to be highly scalable, easily handling the increasing number of users and transactions as the app’s user base grew post-launch.\"]]]],[1,\"p\",[[0,[0],1,\"Conclusion:\"],[0,[],0,\"By working closely with the client, we successfully delivered a scalable and user-friendly real estate investment app. Our use of React Native and Golang ensured a responsive and secure platform where small-scale investors could monitor and grow their portfolios with confidence. The project’s success enabled the client to meet their launch goals and set the foundation for future growth and development.\"]]],[10,0]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Client:</strong> A real estate investment company building an app for small-scale investors</p><p><strong>Team:</strong> Full-stack Development Team (React Native &amp; Golang)</p><p><strong>Project Overview:</strong>The client sought to develop a mobile application that would allow small-scale investors to manage their real estate portfolios with ease. The app needed to provide a comprehensive view of portfolio performance, track fund statuses in real-time, and present this data in an intuitive way. Our team was brought in to support the client with frontend and backend development using React Native and Golang, ensuring the project met its aggressive deadlines.</p><p><strong>Challenges:</strong></p><ol><li><strong>Real-Time Data Integration:</strong><br>Investors needed to track their portfolio and investment performance in real time, making it necessary to build a highly responsive system capable of processing large volumes of data.</li><li><strong>User Experience:</strong><br>Since the target audience was non-technical small-scale investors, creating a user-friendly interface with easy navigation was critical.</li><li><strong>Scalable Infrastructure:</strong><br>With anticipated growth in the user base, the backend had to be scalable and performant, ensuring the system could handle increasing loads as the app expanded.</li></ol><p><strong>Solution:</strong></p><p><strong>React Native for Cross-Platform Development:</strong>We leveraged React Native to create a mobile app that worked seamlessly across both iOS and Android platforms. React Native’s flexibility allowed us to maintain a single codebase for both platforms, which streamlined development and ensured consistent performance.</p><ul><li><strong>Portfolio Dashboard:</strong> We developed a user-friendly dashboard that offered a comprehensive view of an investor’s portfolio. This included visual data representations like charts and graphs to show growth over time and property-specific details.</li><li><strong>Fund Tracking:</strong> A dedicated section was created to display the status of users’ funds, including pending transactions, earnings, and overall returns. This helped investors make informed decisions with real-time information at their fingertips.</li></ul><p><strong>Golang for Backend Services:</strong>Our team utilized Golang to build a robust and efficient backend. Golang’s powerful concurrency model allowed us to manage real-time updates efficiently and ensure high performance under increasing data loads.</p><ul><li><strong>API Development:</strong> We built secure and efficient RESTful APIs to handle user portfolio management, transaction history, and investment performance metrics. These APIs powered the core functionalities of the app and facilitated smooth data flow between the frontend and backend.</li><li><strong>Database Integration:</strong> We integrated a PostgreSQL database to securely store user data, property details, and transactional information. The database design allowed for fast retrieval and updating of information, ensuring a smooth user experience.</li><li><strong>Authentication &amp; Security:</strong> Our team implemented a robust authentication system using JWT tokens, ensuring secure user login and data privacy throughout the app.</li></ul><ol><li><strong>Real-Time Updates:</strong>To provide real-time investment performance updates, we implemented WebSocket technology, allowing instant notifications and updates on portfolio changes. This ensured users could track their investments in real time and respond quickly to market shifts.</li><li><strong>Testing &amp; Optimization:</strong>We employed comprehensive testing strategies, including unit tests, integration tests, and end-to-end testing to ensure the app functioned seamlessly under varying traffic loads. Additionally, we set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline to automate testing and deployment, enabling faster iterations and ensuring the app was always up-to-date.</li></ol><p><strong>Outcome:</strong></p><ul><li><strong>Timely Delivery:</strong> Our team successfully delivered the app within the client's timeline, ensuring they could meet their launch deadlines.</li><li><strong>Positive User Reception:</strong> Early users praised the app for its intuitive design, ease of use, and the real-time updates, which allowed them to stay informed about their investments at all times.</li><li><strong>Scalability and Performance:</strong> The app’s backend architecture proved to be highly scalable, easily handling the increasing number of users and transactions as the app’s user base grew post-launch.</li></ul><p><strong>Conclusion:</strong>By working closely with the client, we successfully delivered a scalable and user-friendly real estate investment app. Our use of React Native and Golang ensured a responsive and secure platform where small-scale investors could monitor and grow their portfolios with confidence. The project’s success enabled the client to meet their launch goals and set the foundation for future growth and development.</p><hr>","comment_id":"67031d9fde6f0100018c3abf","plaintext":"Client: A real estate investment company building an app for small-scale\ninvestors\n\nTeam: Full-stack Development Team (React Native & Golang)\n\nProject Overview:The client sought to develop a mobile application that would\nallow small-scale investors to manage their real estate portfolios with ease.\nThe app needed to provide a comprehensive view of portfolio performance, track\nfund statuses in real-time, and present this data in an intuitive way. Our team\nwas brought in to support the client with frontend and backend development using\nReact Native and Golang, ensuring the project met its aggressive deadlines.\n\nChallenges:\n\n 1. Real-Time Data Integration:\n    Investors needed to track their portfolio and investment performance in real\n    time, making it necessary to build a highly responsive system capable of\n    processing large volumes of data.\n 2. User Experience:\n    Since the target audience was non-technical small-scale investors, creating\n    a user-friendly interface with easy navigation was critical.\n 3. Scalable Infrastructure:\n    With anticipated growth in the user base, the backend had to be scalable and\n    performant, ensuring the system could handle increasing loads as the app\n    expanded.\n\nSolution:\n\nReact Native for Cross-Platform Development:We leveraged React Native to create\na mobile app that worked seamlessly across both iOS and Android platforms. React\nNative’s flexibility allowed us to maintain a single codebase for both\nplatforms, which streamlined development and ensured consistent performance.\n\n * Portfolio Dashboard: We developed a user-friendly dashboard that offered a\n   comprehensive view of an investor’s portfolio. This included visual data\n   representations like charts and graphs to show growth over time and\n   property-specific details.\n * Fund Tracking: A dedicated section was created to display the status of\n   users’ funds, including pending transactions, earnings, and overall returns.\n   This helped investors make informed decisions with real-time information at\n   their fingertips.\n\nGolang for Backend Services:Our team utilized Golang to build a robust and\nefficient backend. Golang’s powerful concurrency model allowed us to manage\nreal-time updates efficiently and ensure high performance under increasing data\nloads.\n\n * API Development: We built secure and efficient RESTful APIs to handle user\n   portfolio management, transaction history, and investment performance\n   metrics. These APIs powered the core functionalities of the app and\n   facilitated smooth data flow between the frontend and backend.\n * Database Integration: We integrated a PostgreSQL database to securely store\n   user data, property details, and transactional information. The database\n   design allowed for fast retrieval and updating of information, ensuring a\n   smooth user experience.\n * Authentication & Security: Our team implemented a robust authentication\n   system using JWT tokens, ensuring secure user login and data privacy\n   throughout the app.\n\n 1. Real-Time Updates:To provide real-time investment performance updates, we\n    implemented WebSocket technology, allowing instant notifications and updates\n    on portfolio changes. This ensured users could track their investments in\n    real time and respond quickly to market shifts.\n 2. Testing & Optimization:We employed comprehensive testing strategies,\n    including unit tests, integration tests, and end-to-end testing to ensure\n    the app functioned seamlessly under varying traffic loads. Additionally, we\n    set up a Continuous Integration/Continuous Deployment (CI/CD) pipeline to\n    automate testing and deployment, enabling faster iterations and ensuring the\n    app was always up-to-date.\n\nOutcome:\n\n * Timely Delivery: Our team successfully delivered the app within the client's\n   timeline, ensuring they could meet their launch deadlines.\n * Positive User Reception: Early users praised the app for its intuitive\n   design, ease of use, and the real-time updates, which allowed them to stay\n   informed about their investments at all times.\n * Scalability and Performance: The app’s backend architecture proved to be\n   highly scalable, easily handling the increasing number of users and\n   transactions as the app’s user base grew post-launch.\n\nConclusion:By working closely with the client, we successfully delivered a\nscalable and user-friendly real estate investment app. Our use of React Native\nand Golang ensured a responsive and secure platform where small-scale investors\ncould monitor and grow their portfolios with confidence. The project’s success\nenabled the client to meet their launch goals and set the foundation for future\ngrowth and development.\n\n\n--------------------------------------------------------------------------------","feature_image":"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-06-at-5.47.02-PM.png","featured":1,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-10-06T23:30:39.000Z","updated_at":"2024-10-06T23:50:14.000Z","published_at":"2023-06-06T23:48:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"67032421de6f0100018c3acf","uuid":"fca4fec2-a749-4d7b-b710-a89d6325571d","title":"Remotely Controlling Water Treatment Systems For Commercial Buildings","slug":"remotely-controlling-water-treatment-systems-for-commercial-buildings","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"hr\",{}]],\"markups\":[[\"strong\"],[\"em\"]],\"sections\":[[1,\"p\",[[0,[0],1,\"Client\"],[0,[],0,\": Aquanomix, a New York-based company specializing in resource optimizations for water treatment solutions.\"]]],[1,\"h3\",[[0,[],0,\"Overview\"]]],[1,\"p\",[[0,[],0,\"Aquanomix, a leader in maximising resource utilization for water treatment systems, approached us with a requirement to develop a system that would enable remote control and monitoring of their water treatment operations across multiple sites. Their challenge was to manage various controllers located in dispersed locations effectively and ensure that operators could access and control these systems from anywhere. To solve this, we designed and developed the \"],[0,[0],1,\"Controlling the Controller (CTC)\"],[0,[],0,\" system.\"]]],[1,\"h3\",[[0,[],0,\"Project Goals\"]]],[1,\"p\",[[0,[],0,\"The primary objectives were to:\"]]],[3,\"ol\",[[[0,[],0,\"Enable remote operations for Aquanomix's water treatment systems.\"]],[[0,[],0,\"Provide a centralized platform for managing controllers on various sites.\"]],[[0,[],0,\"Implement secure and scalable architecture for data exchange between controllers and the dashboard.\"]]]],[1,\"h3\",[[0,[],0,\"Solution\"]]],[1,\"p\",[[0,[],0,\"We designed and implemented a comprehensive \"],[0,[0],1,\"CTC system\"],[0,[],0,\" consisting of multiple components for remote operation and control of water treatment systems. The system architecture comprises the following key components:\"]]],[3,\"ol\",[[[0,[0],1,\"JACE (Java Application Control Engine)\"],[0,[],0,\": Each Aquanomix site uses JACE devices to manage local water treatment systems. These devices are integrated into the system and can be controlled remotely via the Supervisor.\"]],[[0,[0],1,\"Supervisor\"],[0,[],0,\": The Supervisor manages the network of JACEs across different sites and serves as the communication hub between the CTC server and individual controllers. It connects to JACEs at various sites using MQTT protocols.\"]],[[0,[0],1,\"AWS IoT Core\"],[0,[],0,\": AWS IoT Core facilitates secure and scalable communication between the Supervisor and the CTC Server. It enables remote data exchange without managing infrastructure, thus allowing Aquanomix to focus on operations.\"]],[[0,[0],1,\"CTC Server\"],[0,[],0,\": The server, built using Node.js and TypeScript, fetches the latest data from JACEs via the Supervisor and provides an API/WebSocket-based interface for the users. The server also leverages AWS IoT Core to publish and subscribe to messages, allowing real-time communication between all components.\"]],[[0,[0],1,\"CTC Dashboard\"],[0,[],0,\": A user-friendly React-based dashboard enables site admins to monitor and update JACE states. It provides features like data point updates and the ability to trigger specific routines remotely.\"]]]],[1,\"h3\",[[0,[],0,\"Key Features\"]]],[3,\"ul\",[[[0,[0],1,\"Remote Control\"],[0,[],0,\": The CTC system allows users to view real-time data and update settings on any JACE device, regardless of location.\"]],[[0,[0],1,\"Routine Automation\"],[0,[],0,\": One of the routines included is the \"],[0,[1],1,\"CalculateWaterVolumeAndBleedRate\"],[0,[],0,\". This routine injects a tracer chemical into the system, allowing the system to calculate and monitor water volume based on the concentration of the tracer.\"]],[[0,[0],1,\"Secure Communication\"],[0,[],0,\": Communication between the CTC server and JACE devices is secured via AWS IoT Core and authenticated using JWT tokens for access control.\"]],[[0,[0],1,\"Scalable Architecture\"],[0,[],0,\": The system can handle numerous Aquanomix sites across various locations without degradation in performance, thanks to the use of cloud-based infrastructure (AWS IoT Core and AWS EC2).\"]]]],[1,\"h3\",[[0,[],0,\"Technical Details\"]]],[3,\"ul\",[[[0,[0],1,\"Node.js & TypeScript\"],[0,[],0,\": The server-side components were implemented using Node.js, ensuring asynchronous handling of tasks, while TypeScript provided type safety.\"]],[[0,[0],1,\"AWS IoT Core\"],[0,[],0,\": Used for managing communication between devices (JACE, Supervisor, CTC Server) through MQTT topics.\"]],[[0,[0],1,\"React & WebSockets\"],[0,[],0,\": The dashboard is built with React and uses WebSockets for real-time updates, ensuring users are always aware of changes to their systems.\"]],[[0,[0],1,\"Docker\"],[0,[],0,\": Docker was used to containerize the server and ensure consistent deployment across different environments.\"]]]],[1,\"h3\",[[0,[],0,\"Challenges Overcome\"]]],[3,\"ul\",[[[0,[0],1,\"Secure Real-time Communication\"],[0,[],0,\": Achieving secure real-time communication between cloud infrastructure and on-premise devices required careful consideration of encryption, token-based authentication, and certificate management.\"]],[[0,[0],1,\"Scalability\"],[0,[],0,\": The architecture was designed to handle the current number of Aquanomix sites while allowing for easy scalability in the future without requiring significant changes.\"]]]],[1,\"h3\",[[0,[],0,\"Results\"]]],[1,\"p\",[[0,[],0,\"With the implementation of the CTC system, Aquanomix successfully achieved:\"]]],[3,\"ul\",[[[0,[0],1,\"Improved Efficiency\"],[0,[],0,\": Site administrators can now remotely control and monitor water treatment systems, reducing the need for on-site personnel.\"]],[[0,[0],1,\"Centralized Control\"],[0,[],0,\": The dashboard provides a single view of all sites, allowing admins to make informed decisions and trigger system actions from anywhere.\"]],[[0,[0],1,\"Increased Security\"],[0,[],0,\": The system's integration with AWS IoT Core and the use of JWT authentication ensures data is exchanged securely.\"]]]],[1,\"h3\",[[0,[],0,\"Conclusion\"]]],[1,\"p\",[[0,[],0,\"This project provided Aquanomix with the tools needed to remotely manage water treatment systems across multiple sites, improving operational efficiency and scalability. The CTC system has also laid the groundwork for future expansion as Aquanomix continues to grow its operations.\"]]],[10,0],[1,\"p\",[[0,[0],1,\"Technologies Used\"],[0,[],0,\": Node.js, TypeScript, React, AWS IoT Core, Docker, WebSockets, MQTT\"]]],[1,\"p\",[[0,[0],1,\"Key Components\"],[0,[],0,\": JACE devices, Supervisor, CTC Server, CTC Dashboard\"]]],[1,\"p\",[[0,[0],1,\"Security Features\"],[0,[],0,\": AWS IoT Core, JWT-based authentication\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Client</strong>: Aquanomix, a New York-based company specializing in resource optimizations for water treatment solutions.</p><h3 id=\"overview\">Overview</h3><p>Aquanomix, a leader in maximising resource utilization for water treatment systems, approached us with a requirement to develop a system that would enable remote control and monitoring of their water treatment operations across multiple sites. Their challenge was to manage various controllers located in dispersed locations effectively and ensure that operators could access and control these systems from anywhere. To solve this, we designed and developed the <strong>Controlling the Controller (CTC)</strong> system.</p><h3 id=\"project-goals\">Project Goals</h3><p>The primary objectives were to:</p><ol><li>Enable remote operations for Aquanomix's water treatment systems.</li><li>Provide a centralized platform for managing controllers on various sites.</li><li>Implement secure and scalable architecture for data exchange between controllers and the dashboard.</li></ol><h3 id=\"solution\">Solution</h3><p>We designed and implemented a comprehensive <strong>CTC system</strong> consisting of multiple components for remote operation and control of water treatment systems. The system architecture comprises the following key components:</p><ol><li><strong>JACE (Java Application Control Engine)</strong>: Each Aquanomix site uses JACE devices to manage local water treatment systems. These devices are integrated into the system and can be controlled remotely via the Supervisor.</li><li><strong>Supervisor</strong>: The Supervisor manages the network of JACEs across different sites and serves as the communication hub between the CTC server and individual controllers. It connects to JACEs at various sites using MQTT protocols.</li><li><strong>AWS IoT Core</strong>: AWS IoT Core facilitates secure and scalable communication between the Supervisor and the CTC Server. It enables remote data exchange without managing infrastructure, thus allowing Aquanomix to focus on operations.</li><li><strong>CTC Server</strong>: The server, built using Node.js and TypeScript, fetches the latest data from JACEs via the Supervisor and provides an API/WebSocket-based interface for the users. The server also leverages AWS IoT Core to publish and subscribe to messages, allowing real-time communication between all components.</li><li><strong>CTC Dashboard</strong>: A user-friendly React-based dashboard enables site admins to monitor and update JACE states. It provides features like data point updates and the ability to trigger specific routines remotely.</li></ol><h3 id=\"key-features\">Key Features</h3><ul><li><strong>Remote Control</strong>: The CTC system allows users to view real-time data and update settings on any JACE device, regardless of location.</li><li><strong>Routine Automation</strong>: One of the routines included is the <em>CalculateWaterVolumeAndBleedRate</em>. This routine injects a tracer chemical into the system, allowing the system to calculate and monitor water volume based on the concentration of the tracer.</li><li><strong>Secure Communication</strong>: Communication between the CTC server and JACE devices is secured via AWS IoT Core and authenticated using JWT tokens for access control.</li><li><strong>Scalable Architecture</strong>: The system can handle numerous Aquanomix sites across various locations without degradation in performance, thanks to the use of cloud-based infrastructure (AWS IoT Core and AWS EC2).</li></ul><h3 id=\"technical-details\">Technical Details</h3><ul><li><strong>Node.js &amp; TypeScript</strong>: The server-side components were implemented using Node.js, ensuring asynchronous handling of tasks, while TypeScript provided type safety.</li><li><strong>AWS IoT Core</strong>: Used for managing communication between devices (JACE, Supervisor, CTC Server) through MQTT topics.</li><li><strong>React &amp; WebSockets</strong>: The dashboard is built with React and uses WebSockets for real-time updates, ensuring users are always aware of changes to their systems.</li><li><strong>Docker</strong>: Docker was used to containerize the server and ensure consistent deployment across different environments.</li></ul><h3 id=\"challenges-overcome\">Challenges Overcome</h3><ul><li><strong>Secure Real-time Communication</strong>: Achieving secure real-time communication between cloud infrastructure and on-premise devices required careful consideration of encryption, token-based authentication, and certificate management.</li><li><strong>Scalability</strong>: The architecture was designed to handle the current number of Aquanomix sites while allowing for easy scalability in the future without requiring significant changes.</li></ul><h3 id=\"results\">Results</h3><p>With the implementation of the CTC system, Aquanomix successfully achieved:</p><ul><li><strong>Improved Efficiency</strong>: Site administrators can now remotely control and monitor water treatment systems, reducing the need for on-site personnel.</li><li><strong>Centralized Control</strong>: The dashboard provides a single view of all sites, allowing admins to make informed decisions and trigger system actions from anywhere.</li><li><strong>Increased Security</strong>: The system's integration with AWS IoT Core and the use of JWT authentication ensures data is exchanged securely.</li></ul><h3 id=\"conclusion\">Conclusion</h3><p>This project provided Aquanomix with the tools needed to remotely manage water treatment systems across multiple sites, improving operational efficiency and scalability. The CTC system has also laid the groundwork for future expansion as Aquanomix continues to grow its operations.</p><hr><p><strong>Technologies Used</strong>: Node.js, TypeScript, React, AWS IoT Core, Docker, WebSockets, MQTT</p><p><strong>Key Components</strong>: JACE devices, Supervisor, CTC Server, CTC Dashboard</p><p><strong>Security Features</strong>: AWS IoT Core, JWT-based authentication</p>","comment_id":"67032421de6f0100018c3acf","plaintext":"Client: Aquanomix, a New York-based company specializing in resource\noptimizations for water treatment solutions.\n\nOverview\nAquanomix, a leader in maximising resource utilization for water treatment\nsystems, approached us with a requirement to develop a system that would enable\nremote control and monitoring of their water treatment operations across\nmultiple sites. Their challenge was to manage various controllers located in\ndispersed locations effectively and ensure that operators could access and\ncontrol these systems from anywhere. To solve this, we designed and developed\nthe Controlling the Controller (CTC) system.\n\nProject Goals\nThe primary objectives were to:\n\n 1. Enable remote operations for Aquanomix's water treatment systems.\n 2. Provide a centralized platform for managing controllers on various sites.\n 3. Implement secure and scalable architecture for data exchange between\n    controllers and the dashboard.\n\nSolution\nWe designed and implemented a comprehensive CTC system consisting of multiple\ncomponents for remote operation and control of water treatment systems. The\nsystem architecture comprises the following key components:\n\n 1. JACE (Java Application Control Engine): Each Aquanomix site uses JACE\n    devices to manage local water treatment systems. These devices are\n    integrated into the system and can be controlled remotely via the\n    Supervisor.\n 2. Supervisor: The Supervisor manages the network of JACEs across different\n    sites and serves as the communication hub between the CTC server and\n    individual controllers. It connects to JACEs at various sites using MQTT\n    protocols.\n 3. AWS IoT Core: AWS IoT Core facilitates secure and scalable communication\n    between the Supervisor and the CTC Server. It enables remote data exchange\n    without managing infrastructure, thus allowing Aquanomix to focus on\n    operations.\n 4. CTC Server: The server, built using Node.js and TypeScript, fetches the\n    latest data from JACEs via the Supervisor and provides an\n    API/WebSocket-based interface for the users. The server also leverages AWS\n    IoT Core to publish and subscribe to messages, allowing real-time\n    communication between all components.\n 5. CTC Dashboard: A user-friendly React-based dashboard enables site admins to\n    monitor and update JACE states. It provides features like data point updates\n    and the ability to trigger specific routines remotely.\n\nKey Features\n * Remote Control: The CTC system allows users to view real-time data and update\n   settings on any JACE device, regardless of location.\n * Routine Automation: One of the routines included is the \n   CalculateWaterVolumeAndBleedRate. This routine injects a tracer chemical into\n   the system, allowing the system to calculate and monitor water volume based\n   on the concentration of the tracer.\n * Secure Communication: Communication between the CTC server and JACE devices\n   is secured via AWS IoT Core and authenticated using JWT tokens for access\n   control.\n * Scalable Architecture: The system can handle numerous Aquanomix sites across\n   various locations without degradation in performance, thanks to the use of\n   cloud-based infrastructure (AWS IoT Core and AWS EC2).\n\nTechnical Details\n * Node.js & TypeScript: The server-side components were implemented using\n   Node.js, ensuring asynchronous handling of tasks, while TypeScript provided\n   type safety.\n * AWS IoT Core: Used for managing communication between devices (JACE,\n   Supervisor, CTC Server) through MQTT topics.\n * React & WebSockets: The dashboard is built with React and uses WebSockets for\n   real-time updates, ensuring users are always aware of changes to their\n   systems.\n * Docker: Docker was used to containerize the server and ensure consistent\n   deployment across different environments.\n\nChallenges Overcome\n * Secure Real-time Communication: Achieving secure real-time communication\n   between cloud infrastructure and on-premise devices required careful\n   consideration of encryption, token-based authentication, and certificate\n   management.\n * Scalability: The architecture was designed to handle the current number of\n   Aquanomix sites while allowing for easy scalability in the future without\n   requiring significant changes.\n\nResults\nWith the implementation of the CTC system, Aquanomix successfully achieved:\n\n * Improved Efficiency: Site administrators can now remotely control and monitor\n   water treatment systems, reducing the need for on-site personnel.\n * Centralized Control: The dashboard provides a single view of all sites,\n   allowing admins to make informed decisions and trigger system actions from\n   anywhere.\n * Increased Security: The system's integration with AWS IoT Core and the use of\n   JWT authentication ensures data is exchanged securely.\n\nConclusion\nThis project provided Aquanomix with the tools needed to remotely manage water\ntreatment systems across multiple sites, improving operational efficiency and\nscalability. The CTC system has also laid the groundwork for future expansion as\nAquanomix continues to grow its operations.\n\n\n--------------------------------------------------------------------------------\n\nTechnologies Used: Node.js, TypeScript, React, AWS IoT Core, Docker, WebSockets,\nMQTT\n\nKey Components: JACE devices, Supervisor, CTC Server, CTC Dashboard\n\nSecurity Features: AWS IoT Core, JWT-based authentication","feature_image":"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-06-at-9.41.15-PM.png","featured":0,"type":"post","status":"draft","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-10-06T23:58:25.000Z","updated_at":"2025-05-28T21:35:47.000Z","published_at":"2024-03-31T00:05:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null},{"id":"67032df3de6f0100018c3af2","uuid":"111aa4b6-fc69-4802-9841-8208779fe25a","title":"Optimizing Data Integrity for Moodiday.com","slug":"optimizing-data-integrity-for-cannabis-ecommerce-platform","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"hr\",{}],[\"hr\",{}],[\"hr\",{}],[\"hr\",{}]],\"markups\":[[\"strong\"],[\"code\"]],\"sections\":[[1,\"p\",[[0,[0],1,\"Client Overview\"],[1,[],0,0],[0,[],0,\"Moodiday.com is an innovative e-commerce platform catering to the modern cannabis consumer. The client sought to optimize their data infrastructure to ensure better data integrity, enhance efficiency, and improve security across their PostgreSQL databases hosted on Azure and Digital Ocean.\"]]],[10,0],[1,\"p\",[[0,[0],1,\"Challenge\"],[1,[],0,1],[0,[],0,\"The client faced data integrity issues primarily stemming from complex data interactions between multiple databases. Specifically, there were discrepancies in the number of rows returned when joining two key tables: \"],[0,[1],1,\"pages\"],[0,[],0,\" and \"],[0,[1],1,\"widget_in_view\"],[0,[],0,\". This inconsistency led to confusion and hindered data analysis, as the data ingestion process had compounded the issue, making it difficult to resolve existing discrepancies. The problem was particularly pronounced due to the use of a \"],[0,[1],1,\"LEFT JOIN\"],[0,[],0,\" on multiple non-unique rows, which resulted in an inflated dataset and flattened the data, complicating accurate analysis.\"]]],[10,1],[1,\"p\",[[0,[0],1,\"Solution\"],[1,[],0,2],[0,[],0,\"As the consulting team, we undertook a comprehensive review of the existing data infrastructure. Our approach involved the following steps:\"]]],[3,\"ol\",[[[0,[0],1,\"Data Analysis and Query Review\"],[0,[],0,\":\"],[1,[],0,3],[0,[],0,\"We analyzed the SQL queries used to join the \"],[0,[1],1,\"pages\"],[0,[],0,\" and \"],[0,[1],1,\"widget_in_view\"],[0,[],0,\" tables. The use of \"],[0,[1],1,\"LEFT JOIN\"],[0,[],0,\" on \"],[0,[1],1,\"anonymous_id\"],[0,[],0,\", \"],[0,[1],1,\"brand\"],[0,[],0,\", and \"],[0,[1],1,\"product_id\"],[0,[],0,\" resulted in multiple entries from both tables inflating the row count significantly. For instance, one query returned 4144 rows instead of the expected number, highlighting the need for a more effective data model.\"]],[[0,[0],1,\"Proposed Solution\"],[0,[],0,\":\"],[1,[],0,4],[0,[],0,\"To address the data integrity issues, we proposed implementing a unique session/page load identifier. This identifier would enable the accurate joining of the two tables by ensuring that we could uniquely identify \"],[0,[1],1,\"widget_in_view\"],[0,[],0,\" events corresponding to a particular page load. This approach allows us to track multiple \"],[0,[1],1,\"widget_in_view\"],[0,[],0,\" events for a page load and pinpoint a unique event based on the page load ID/session ID, effectively preventing the flattening of the data and maintaining data integrity.\"]],[[0,[0],1,\"Expert Recommendations\"],[0,[],0,\":\"],[1,[],0,5],[0,[],0,\"Throughout the consultation, we provided recommendations for optimizing data flow, improving query performance, and enhancing data integrity. This included strategies for restructuring the database schema to minimize redundancy and streamline data retrieval processes.\"]],[[0,[0],1,\"Collaboration and Communication\"],[0,[],0,\":\"],[1,[],0,6],[0,[],0,\"We maintained open lines of communication with the client, scheduling meetings to discuss findings and collaboratively brainstorm solutions. This ongoing dialogue helped clarify the client’s needs and ensured alignment on the proposed changes.\"]]]],[10,2],[1,\"p\",[[0,[0],1,\"Results\"],[1,[],0,7],[0,[],0,\"While the initial data integrity issues could not be retroactively resolved due to existing data ingestion challenges, the strategies and recommendations we put forth laid a solid foundation for future data handling and analysis:\"]]],[3,\"ul\",[[[0,[0],1,\"Enhanced Understanding\"],[0,[],0,\": The client gained valuable insights into the reasons behind their data discrepancies, enabling them to make informed decisions moving forward.\"]],[[0,[0],1,\"Improved Data Modeling\"],[0,[],0,\": By implementing a unique session identifier, the client could enhance their data model to ensure better accuracy in reporting and analytics.\"]],[[0,[0],1,\"Increased Efficiency\"],[0,[],0,\": Our proposed changes aimed to streamline data flow and improve query performance, positioning the client for future scalability and growth.\"]]]],[10,3],[1,\"p\",[[0,[0],1,\"Conclusion\"],[1,[],0,8],[0,[],0,\"Our engagement with Moodiday.com not only addressed their immediate data integrity issues but also empowered them to build a more robust data infrastructure. By focusing on optimizing their PostgreSQL databases and enhancing data interactions across multiple platforms, the client is now better equipped to support their growing e-commerce operations in the cannabis industry.\"]]]],\"ghostVersion\":\"4.0\"}","html":"<p><strong>Client Overview</strong><br>Moodiday.com is an innovative e-commerce platform catering to the modern cannabis consumer. The client sought to optimize their data infrastructure to ensure better data integrity, enhance efficiency, and improve security across their PostgreSQL databases hosted on Azure and Digital Ocean.</p><hr><p><strong>Challenge</strong><br>The client faced data integrity issues primarily stemming from complex data interactions between multiple databases. Specifically, there were discrepancies in the number of rows returned when joining two key tables: <code>pages</code> and <code>widget_in_view</code>. This inconsistency led to confusion and hindered data analysis, as the data ingestion process had compounded the issue, making it difficult to resolve existing discrepancies. The problem was particularly pronounced due to the use of a <code>LEFT JOIN</code> on multiple non-unique rows, which resulted in an inflated dataset and flattened the data, complicating accurate analysis.</p><hr><p><strong>Solution</strong><br>As the consulting team, we undertook a comprehensive review of the existing data infrastructure. Our approach involved the following steps:</p><ol><li><strong>Data Analysis and Query Review</strong>:<br>We analyzed the SQL queries used to join the <code>pages</code> and <code>widget_in_view</code> tables. The use of <code>LEFT JOIN</code> on <code>anonymous_id</code>, <code>brand</code>, and <code>product_id</code> resulted in multiple entries from both tables inflating the row count significantly. For instance, one query returned 4144 rows instead of the expected number, highlighting the need for a more effective data model.</li><li><strong>Proposed Solution</strong>:<br>To address the data integrity issues, we proposed implementing a unique session/page load identifier. This identifier would enable the accurate joining of the two tables by ensuring that we could uniquely identify <code>widget_in_view</code> events corresponding to a particular page load. This approach allows us to track multiple <code>widget_in_view</code> events for a page load and pinpoint a unique event based on the page load ID/session ID, effectively preventing the flattening of the data and maintaining data integrity.</li><li><strong>Expert Recommendations</strong>:<br>Throughout the consultation, we provided recommendations for optimizing data flow, improving query performance, and enhancing data integrity. This included strategies for restructuring the database schema to minimize redundancy and streamline data retrieval processes.</li><li><strong>Collaboration and Communication</strong>:<br>We maintained open lines of communication with the client, scheduling meetings to discuss findings and collaboratively brainstorm solutions. This ongoing dialogue helped clarify the client’s needs and ensured alignment on the proposed changes.</li></ol><hr><p><strong>Results</strong><br>While the initial data integrity issues could not be retroactively resolved due to existing data ingestion challenges, the strategies and recommendations we put forth laid a solid foundation for future data handling and analysis:</p><ul><li><strong>Enhanced Understanding</strong>: The client gained valuable insights into the reasons behind their data discrepancies, enabling them to make informed decisions moving forward.</li><li><strong>Improved Data Modeling</strong>: By implementing a unique session identifier, the client could enhance their data model to ensure better accuracy in reporting and analytics.</li><li><strong>Increased Efficiency</strong>: Our proposed changes aimed to streamline data flow and improve query performance, positioning the client for future scalability and growth.</li></ul><hr><p><strong>Conclusion</strong><br>Our engagement with Moodiday.com not only addressed their immediate data integrity issues but also empowered them to build a more robust data infrastructure. By focusing on optimizing their PostgreSQL databases and enhancing data interactions across multiple platforms, the client is now better equipped to support their growing e-commerce operations in the cannabis industry.</p>","comment_id":"67032df3de6f0100018c3af2","plaintext":"Client Overview\nMoodiday.com is an innovative e-commerce platform catering to the modern\ncannabis consumer. The client sought to optimize their data infrastructure to\nensure better data integrity, enhance efficiency, and improve security across\ntheir PostgreSQL databases hosted on Azure and Digital Ocean.\n\n\n--------------------------------------------------------------------------------\n\nChallenge\nThe client faced data integrity issues primarily stemming from complex data\ninteractions between multiple databases. Specifically, there were discrepancies\nin the number of rows returned when joining two key tables: pages and \nwidget_in_view. This inconsistency led to confusion and hindered data analysis,\nas the data ingestion process had compounded the issue, making it difficult to\nresolve existing discrepancies. The problem was particularly pronounced due to\nthe use of a LEFT JOIN on multiple non-unique rows, which resulted in an\ninflated dataset and flattened the data, complicating accurate analysis.\n\n\n--------------------------------------------------------------------------------\n\nSolution\nAs the consulting team, we undertook a comprehensive review of the existing data\ninfrastructure. Our approach involved the following steps:\n\n 1. Data Analysis and Query Review:\n    We analyzed the SQL queries used to join the pages and widget_in_view \n    tables. The use of LEFT JOIN on anonymous_id, brand, and product_id resulted\n    in multiple entries from both tables inflating the row count significantly.\n    For instance, one query returned 4144 rows instead of the expected number,\n    highlighting the need for a more effective data model.\n 2. Proposed Solution:\n    To address the data integrity issues, we proposed implementing a unique\n    session/page load identifier. This identifier would enable the accurate\n    joining of the two tables by ensuring that we could uniquely identify \n    widget_in_view events corresponding to a particular page load. This approach\n    allows us to track multiple widget_in_view events for a page load and\n    pinpoint a unique event based on the page load ID/session ID, effectively\n    preventing the flattening of the data and maintaining data integrity.\n 3. Expert Recommendations:\n    Throughout the consultation, we provided recommendations for optimizing data\n    flow, improving query performance, and enhancing data integrity. This\n    included strategies for restructuring the database schema to minimize\n    redundancy and streamline data retrieval processes.\n 4. Collaboration and Communication:\n    We maintained open lines of communication with the client, scheduling\n    meetings to discuss findings and collaboratively brainstorm solutions. This\n    ongoing dialogue helped clarify the client’s needs and ensured alignment on\n    the proposed changes.\n\n\n--------------------------------------------------------------------------------\n\nResults\nWhile the initial data integrity issues could not be retroactively resolved due\nto existing data ingestion challenges, the strategies and recommendations we put\nforth laid a solid foundation for future data handling and analysis:\n\n * Enhanced Understanding: The client gained valuable insights into the reasons\n   behind their data discrepancies, enabling them to make informed decisions\n   moving forward.\n * Improved Data Modeling: By implementing a unique session identifier, the\n   client could enhance their data model to ensure better accuracy in reporting\n   and analytics.\n * Increased Efficiency: Our proposed changes aimed to streamline data flow and\n   improve query performance, positioning the client for future scalability and\n   growth.\n\n\n--------------------------------------------------------------------------------\n\nConclusion\nOur engagement with Moodiday.com not only addressed their immediate data\nintegrity issues but also empowered them to build a more robust data\ninfrastructure. By focusing on optimizing their PostgreSQL databases and\nenhancing data interactions across multiple platforms, the client is now better\nequipped to support their growing e-commerce operations in the cannabis\nindustry.","feature_image":"__GHOST_URL__/content/images/2024/10/Screenshot-2024-10-06-at-6.41.45-PM.png","featured":0,"type":"post","status":"published","locale":null,"visibility":"public","email_recipient_filter":"none","author_id":"1","created_at":"2024-10-07T00:40:19.000Z","updated_at":"2024-11-03T19:21:12.000Z","published_at":"2024-04-02T00:41:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"newsletter_id":null}],"posts_authors":[{"id":"60b6ee631d684200019a9577","post_id":"60b6ee631d684200019a9576","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"60b6ee631d684200019a9579","post_id":"60b6ee631d684200019a9578","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"60b6ee631d684200019a957b","post_id":"60b6ee631d684200019a957a","author_id":"5951f5fca366002ebd5dbef7","sort_order":0},{"id":"60b81d4afda06e0001f83720","post_id":"60b81d4afda06e0001f8371f","author_id":"1","sort_order":0},{"id":"60b84fa0fda06e0001f8375d","post_id":"60b84fa0fda06e0001f8375c","author_id":"1","sort_order":0},{"id":"60b8524afda06e0001f8377e","post_id":"60b8524afda06e0001f8377d","author_id":"1","sort_order":0},{"id":"60b85376fda06e0001f8378e","post_id":"60b85376fda06e0001f8378d","author_id":"1","sort_order":0},{"id":"60b85707fda06e0001f837ca","post_id":"60b85707fda06e0001f837c9","author_id":"1","sort_order":0},{"id":"60b858b6fda06e0001f837de","post_id":"60b858b6fda06e0001f837dd","author_id":"1","sort_order":0},{"id":"60b859a9fda06e0001f837f6","post_id":"60b859a9fda06e0001f837f5","author_id":"1","sort_order":0},{"id":"60bbba10fda06e0001f83832","post_id":"60bbba10fda06e0001f83831","author_id":"1","sort_order":0},{"id":"60bbbb26fda06e0001f83848","post_id":"60bbbb26fda06e0001f83847","author_id":"1","sort_order":0},{"id":"60c02961fda06e0001f83946","post_id":"60c02961fda06e0001f83945","author_id":"1","sort_order":0},{"id":"60c6b291fda06e0001f83a5d","post_id":"60c6b291fda06e0001f83a5c","author_id":"1","sort_order":0},{"id":"612bd5acfda06e0001f83a8b","post_id":"612bd5acfda06e0001f83a8a","author_id":"1","sort_order":0},{"id":"61900fc7f65b490001261aed","post_id":"61900fc7f65b490001261aec","author_id":"1","sort_order":0},{"id":"61b6a32df65b490001261b8b","post_id":"61b6a32df65b490001261b8a","author_id":"1","sort_order":0},{"id":"61ca21f8f65b490001261ba9","post_id":"61ca21f8f65b490001261ba8","author_id":"1","sort_order":0},{"id":"61dfad30f65b490001261eab","post_id":"61dfad30f65b490001261eaa","author_id":"1","sort_order":0},{"id":"61fcc2ac021a27000172f42e","post_id":"61fcc2ac021a27000172f42d","author_id":"1","sort_order":0},{"id":"620078c9021a27000172f489","post_id":"620078c9021a27000172f488","author_id":"1","sort_order":0},{"id":"62c35d9b4866590001862308","post_id":"62c35d9b4866590001862307","author_id":"1","sort_order":0},{"id":"64a5eff82f69e4000100a52c","post_id":"64a5eff82f69e4000100a52b","author_id":"1","sort_order":0},{"id":"64a9e5fa2f69e4000100a531","post_id":"64a9e5fa2f69e4000100a530","author_id":"1","sort_order":0},{"id":"64a9e91b2f69e4000100a561","post_id":"64a9e91b2f69e4000100a560","author_id":"1","sort_order":0},{"id":"66f234ebde6f0100018c3a88","post_id":"66f234ebde6f0100018c3a87","author_id":"1","sort_order":0},{"id":"67031d9fde6f0100018c3ac0","post_id":"67031d9fde6f0100018c3abf","author_id":"1","sort_order":0},{"id":"67032421de6f0100018c3ad0","post_id":"67032421de6f0100018c3acf","author_id":"1","sort_order":0},{"id":"67032df3de6f0100018c3af3","post_id":"67032df3de6f0100018c3af2","author_id":"1","sort_order":0}],"posts_meta":[{"id":"61fb3b9a021a27000172f420","post_id":"61dfad30f65b490001261eaa","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":"Bringing back from the dead - An Arch Linux tale. ","meta_description":"After resizing the logical volumes in the main partition, GRUB failed to boot my arch linux. This was happening because of some file system checks that failed after resizing the partition.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0},{"id":"64a9ed532f69e4000100a5cb","post_id":"64a9e91b2f69e4000100a560","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":"Learn how our intuitive single-page React app revolutionized insurance quoting. By streamlining data collection, integrating with Compulife APIs, and presenting real-time quotes, our solution increased efficiency, customer satisfaction, and competitiveness. Discover the impact of our Insurance Quoting System in this case study.","email_subject":null,"frontmatter":null,"feature_image_alt":null,"feature_image_caption":null,"email_only":0}],"posts_products":[{"id":"6837815658c53000012f53ff","post_id":"66f234ebde6f0100018c3a87","product_id":"60b6ee611d684200019a9508","sort_order":0},{"id":"6837815658c53000012f5400","post_id":"66f234ebde6f0100018c3a87","product_id":"67b142141d62d30001b31ea7","sort_order":1},{"id":"683781b358c53000012f5402","post_id":"67032421de6f0100018c3acf","product_id":"60b6ee611d684200019a9508","sort_order":0},{"id":"683781b358c53000012f5403","post_id":"67032421de6f0100018c3acf","product_id":"67b142141d62d30001b31ea7","sort_order":1}],"posts_tags":[{"id":"60c67d04fda06e0001f83a4b","post_id":"60bbba10fda06e0001f83831","tag_id":"60c67d04fda06e0001f83a48","sort_order":0},{"id":"60c67d04fda06e0001f83a4c","post_id":"60bbba10fda06e0001f83831","tag_id":"60c67d04fda06e0001f83a49","sort_order":1},{"id":"60c67d04fda06e0001f83a4d","post_id":"60bbba10fda06e0001f83831","tag_id":"60c67d04fda06e0001f83a4a","sort_order":2},{"id":"60c67ef2fda06e0001f83a55","post_id":"60bbba10fda06e0001f83831","tag_id":"60c67ef2fda06e0001f83a53","sort_order":3},{"id":"60c67ef2fda06e0001f83a56","post_id":"60bbba10fda06e0001f83831","tag_id":"60c67ef2fda06e0001f83a54","sort_order":4},{"id":"60c6b394fda06e0001f83a71","post_id":"60c6b291fda06e0001f83a5c","tag_id":"60c6b394fda06e0001f83a6e","sort_order":1},{"id":"60c6b394fda06e0001f83a72","post_id":"60c6b291fda06e0001f83a5c","tag_id":"60c6b394fda06e0001f83a6f","sort_order":2},{"id":"60c6b394fda06e0001f83a73","post_id":"60c6b291fda06e0001f83a5c","tag_id":"60c6b394fda06e0001f83a70","sort_order":3},{"id":"61902fd3f65b490001261b5d","post_id":"61900fc7f65b490001261aec","tag_id":"61902fd3f65b490001261b59","sort_order":0},{"id":"61902fd3f65b490001261b5e","post_id":"61900fc7f65b490001261aec","tag_id":"61902fd3f65b490001261b5a","sort_order":1},{"id":"61902fd3f65b490001261b5f","post_id":"61900fc7f65b490001261aec","tag_id":"61902fd3f65b490001261b5b","sort_order":2},{"id":"61902fd3f65b490001261b60","post_id":"61900fc7f65b490001261aec","tag_id":"61902fd3f65b490001261b5c","sort_order":3},{"id":"61ccda3ef65b490001261d3e","post_id":"61ca21f8f65b490001261ba8","tag_id":"61ccda3ef65b490001261d3c","sort_order":0},{"id":"61ccda3ef65b490001261d3f","post_id":"61ca21f8f65b490001261ba8","tag_id":"61ccda3ef65b490001261d3d","sort_order":1},{"id":"61eca317d831dd00011bc351","post_id":"61dfad30f65b490001261eaa","tag_id":"61eca317d831dd00011bc34d","sort_order":5},{"id":"61eca317d831dd00011bc352","post_id":"61dfad30f65b490001261eaa","tag_id":"61eca317d831dd00011bc34e","sort_order":0},{"id":"61eca317d831dd00011bc353","post_id":"61dfad30f65b490001261eaa","tag_id":"61902fd3f65b490001261b5b","sort_order":1},{"id":"61eca317d831dd00011bc354","post_id":"61dfad30f65b490001261eaa","tag_id":"61eca317d831dd00011bc34f","sort_order":2},{"id":"61eca317d831dd00011bc355","post_id":"61dfad30f65b490001261eaa","tag_id":"61eca317d831dd00011bc350","sort_order":3},{"id":"61fcc3d7021a27000172f448","post_id":"61fcc2ac021a27000172f42d","tag_id":"61fcc3d7021a27000172f446","sort_order":1},{"id":"61fcc3d7021a27000172f449","post_id":"61fcc2ac021a27000172f42d","tag_id":"61fcc3d7021a27000172f447","sort_order":0},{"id":"61fdf404021a27000172f452","post_id":"60b84fa0fda06e0001f8375c","tag_id":"61ccda3ef65b490001261d3c","sort_order":0},{"id":"61fdf404021a27000172f453","post_id":"60b84fa0fda06e0001f8375c","tag_id":"61fdf404021a27000172f450","sort_order":1},{"id":"61fdf404021a27000172f454","post_id":"60b84fa0fda06e0001f8375c","tag_id":"61fdf404021a27000172f451","sort_order":2},{"id":"61fdf404021a27000172f455","post_id":"60b84fa0fda06e0001f8375c","tag_id":"60c6b394fda06e0001f83a70","sort_order":3},{"id":"61fdf46b021a27000172f459","post_id":"61dfad30f65b490001261eaa","tag_id":"61ccda3ef65b490001261d3d","sort_order":4},{"id":"61fdf4c4021a27000172f45e","post_id":"60b81d4afda06e0001f8371f","tag_id":"61fdf4c4021a27000172f45c","sort_order":0},{"id":"61fdf4c4021a27000172f45f","post_id":"60b81d4afda06e0001f8371f","tag_id":"61fdf4c4021a27000172f45d","sort_order":1},{"id":"61fdf50b021a27000172f464","post_id":"60b8524afda06e0001f8377d","tag_id":"61ccda3ef65b490001261d3c","sort_order":0},{"id":"61fdf50b021a27000172f465","post_id":"60b8524afda06e0001f8377d","tag_id":"61fdf50b021a27000172f463","sort_order":1},{"id":"61fe0638021a27000172f469","post_id":"60b859a9fda06e0001f837f5","tag_id":"61fdf4c4021a27000172f45c","sort_order":0},{"id":"61fe0638021a27000172f46a","post_id":"60b859a9fda06e0001f837f5","tag_id":"61fe0638021a27000172f468","sort_order":1},{"id":"61fe069a021a27000172f46e","post_id":"60b858b6fda06e0001f837dd","tag_id":"61fcc3d7021a27000172f447","sort_order":0},{"id":"61fe069a021a27000172f46f","post_id":"60b858b6fda06e0001f837dd","tag_id":"61fe069a021a27000172f46d","sort_order":1},{"id":"61fe069a021a27000172f470","post_id":"60b858b6fda06e0001f837dd","tag_id":"61fcc3d7021a27000172f446","sort_order":2},{"id":"61fe0c1a021a27000172f474","post_id":"60b85376fda06e0001f8378d","tag_id":"61fe0c1a021a27000172f473","sort_order":0},{"id":"61fe0c33021a27000172f478","post_id":"60b85707fda06e0001f837c9","tag_id":"61fe0c33021a27000172f477","sort_order":0},{"id":"6354499ab9d5a200011d8380","post_id":"60c6b291fda06e0001f83a5c","tag_id":"61fdf4c4021a27000172f45c","sort_order":0},{"id":"64a9ec9c2f69e4000100a5c5","post_id":"64a9e91b2f69e4000100a560","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"66f23801de6f0100018c3ab5","post_id":"66f234ebde6f0100018c3a87","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"670321d5de6f0100018c3ac8","post_id":"67031d9fde6f0100018c3abf","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"670321d5de6f0100018c3ac9","post_id":"67031d9fde6f0100018c3abf","tag_id":"61fdf4c4021a27000172f45c","sort_order":1},{"id":"670325cede6f0100018c3ade","post_id":"67032421de6f0100018c3acf","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"67032aedde6f0100018c3aed","post_id":"64a5eff82f69e4000100a52b","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"67032aedde6f0100018c3aee","post_id":"64a5eff82f69e4000100a52b","tag_id":"67032aedde6f0100018c3aec","sort_order":1},{"id":"67032eaade6f0100018c3aff","post_id":"67032df3de6f0100018c3af2","tag_id":"64a9ec9c2f69e4000100a5c4","sort_order":0},{"id":"67032eaade6f0100018c3b00","post_id":"67032df3de6f0100018c3af2","tag_id":"61fe0638021a27000172f468","sort_order":1},{"id":"67032eaade6f0100018c3b01","post_id":"67032df3de6f0100018c3af2","tag_id":"67032ea9de6f0100018c3afe","sort_order":2}],"products":[{"id":"60b6ee611d684200019a9508","name":"A & H Solutions","slug":"default-product","description":null,"created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T04:39:28.000Z","monthly_price_id":null,"yearly_price_id":null,"type":"paid","active":1,"welcome_page_url":"/","visibility":"public"},{"id":"67b142141d62d30001b31ea7","name":"Free","slug":"67b142141d62d30001b31ea7","description":null,"created_at":"2025-02-16T01:40:36.000Z","updated_at":null,"monthly_price_id":null,"yearly_price_id":null,"type":"free","active":1,"welcome_page_url":"/","visibility":"public"}],"products_benefits":[],"roles":[{"id":"60b6ee611d684200019a950a","name":"Administrator","description":"Administrators","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a950b","name":"Editor","description":"Editors","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a950c","name":"Author","description":"Authors","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a950d","name":"Contributor","description":"Contributors","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a950e","name":"Owner","description":"Blog Owner","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a950f","name":"Admin Integration","description":"External Apps","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a9510","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"},{"id":"60b6ee611d684200019a9511","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2021-06-02T02:35:13.000Z","updated_at":"2021-06-02T02:35:13.000Z"}],"roles_users":[{"id":"60b6ee621d684200019a9567","role_id":"60b6ee611d684200019a950d","user_id":"5951f5fca366002ebd5dbef7"},{"id":"60b6ee631d684200019a9671","role_id":"60b6ee611d684200019a950e","user_id":"1"}],"settings":[{"id":"60b6ee641d684200019a9672","group":"core","key":"db_hash","value":"dd5d764d-48ed-4dcd-84ab-26ee1e977284","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee641d684200019a9673","group":"core","key":"routes_hash","value":"a12f173cb5ffb1c17578c99a08347c53","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2023-09-24T01:03:45.000Z"},{"id":"60b6ee641d684200019a9674","group":"core","key":"next_update_check","value":"1757489569","type":"number","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2025-09-09T07:32:49.000Z"},{"id":"60b6ee641d684200019a9675","group":"core","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"0d88bf51-ab48-4a7f-a867-e5096e0026ad\",\"createdAtVersion\":\"4.48.9\",\"custom\":true,\"createdAt\":\"2024-08-21T12:01:19.000Z\",\"type\":\"alert\",\"top\":false,\"message\":\"Critical security update available — please update Ghost as soon as possible. <a href=\\\"https://github.com/TryGhost/Ghost/security/advisories/GHSA-78x2-cwp9-5j42\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Details here.</a>\",\"seen\":false,\"addedAt\":\"2025-04-10T22:01:14.057Z\"},{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"42f1b99a-efae-45a9-8f5c-ff6a041e165f\",\"createdAtVersion\":\"4.48.9\",\"custom\":true,\"createdAt\":\"2025-08-28T09:16:12.000Z\",\"type\":\"info\",\"top\":true,\"message\":\"<strong>Ghost 4.0 is now end-of-life as of January 2023</strong> - You are using an old version of Ghost, which means you don't have access to the latest features. <a href=\\\"https://ghost.org/changelog/6/\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Read more!</a>\",\"seen\":false,\"addedAt\":\"2025-09-09T07:32:49.210Z\"}]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2025-09-09T07:32:49.000Z"},{"id":"60b6ee641d684200019a9676","group":"core","key":"session_secret","value":"f7528d63944270713bf620ee058ad8428db059e35d21ce0adc1302f12ce2f003","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee641d684200019a9677","group":"core","key":"theme_session_secret","value":"b3200971859b0c7faf2c1be3c7acb70cb436cca458f2dd49158762ec5243e031","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9678","group":"core","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBANF3b4ZfOk8n77/Vu8vENZUswhRvGHqHKnmsTnAA2PwEu1dIkiomyhFwGD5k8Chq\ngz6T2ienbh3mWGuukqN8ieU0iIHYCAE9PGB4urgsXKo/KPyz3fs+WGxqZzfcuB0XX5rjwQNO\nJLA/rBGgLeP+KfP1iTBaiF2RMZbWWnb0QOVZAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9679","group":"core","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQDRd2+GXzpPJ++/1bvLxDWVLMIUbxh6hyp5rE5wANj8BLtXSJIqJsoRcBg+\nZPAoaoM+k9onp24d5lhrrpKjfInlNIiB2AgBPTxgeLq4LFyqPyj8s937Plhsamc33LgdF1+a\n48EDTiSwP6wRoC3j/inz9YkwWohdkTGW1lp29EDlWQIDAQABAoGBAJWaPv3yj3uvY01YwqrS\n9Q4ZDHh5rDr+xLhn9xGJmW/NlVvOig4u7A0uSTJu9xN2l0wfXyUJMN2MAcTOxw6ayQSu+46s\neI8XEmu6WtQFaC6L/fP9MuK3dLYjPYZ5A4idcD2Bmcj145IbotAXvzapwosN76yGC9cGANQk\n51Q5YHBxAkEA7C1jyPrfiPbD2V8ub3UjHDg9OFnVgdG96g3GHw2cIWtOgMtXt+LHnAO03qTq\nr6lQKtBo+D90k+9Ikx59Nder5QJBAOMMIG1E40TgZg0mayAIpQDSBZCZ1WFNReFIdrs7b5cs\nNJ405buFquJ4/partPlI4xn74PcYmuv85PmouzLohGUCQDiEaCOZPVLmUlm7OD591ogTQTsx\n7DDjw0G2SBn4RMuOhTu27WfYwhGopWi+KFce+ks0kloNvVUFZuWl3yq6UK0CQCcqS1mt/Wo/\nLztreiUveI8Yq38TGjzLoJdI8bpAeXJhXL5Br7NWTnD8oCYLfo9V5F7yFQ9qviahHDp4wbn5\nZAUCQQCSp3O13dd6eqhVkCQb+6r/sqQxRZedBB9LOZ1ooamUmHvpqZOV27CualVLFYurFowm\nuwMFIJYIw7rQtXs1V53W\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a967a","group":"core","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAI5hca7/rfYRhBQ1tY3k8bzCkDfcrWRBaz8jUEWATXdrKxXWAlzH5qolyprHtXVT\nl7bsm5p5VgxhwBrcUcR6IWKsOOpmvkXskyJgccY464Ro/zOk50aubR1S9dqjiVT0uwhCBgNX\nUA+9k2EhOW+GFeEVsMEIwOooR90e2/ucbTkzAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a967b","group":"core","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXQIBAAKBgQCOYXGu/632EYQUNbWN5PG8wpA33K1kQWs/I1BFgE13aysV1gJcx+aqJcqa\nx7V1U5e27JuaeVYMYcAa3FHEeiFirDjqZr5F7JMiYHHGOOuEaP8zpOdGrm0dUvXao4lU9LsI\nQgYDV1APvZNhITlvhhXhFbDBCMDqKEfdHtv7nG05MwIDAQABAoGAMFaw7ALuIr6rotjhPL1f\nvQApSm3niEQVjygEFrSKmGKQDCfOQYW0sEuIT6hB2QV6WqZV5coQBsAhIn1uXVBBiOYLAREN\nixjHW5f7KhULYubXcCvWYAiwbgAsBJzJwB9kO6BHEqX94bDMsT6rIhHKNQRUNEH9JSpoqfPM\nMQoqk2ECQQDV2uMqcsxSoz55zvg62a2gV0F/+QUnVXjYAQPijrtCP3oBOl7X6xml402311N4\nH6Nz6QuHRnWfpiB77kIejyOrAkEAqnChr7Bsb2g2MltyMr+e10OmuMWHDi+cvdmPwUA4zod8\nnuFgpfSCUK6mC+4lwTF2Xxo+WfOwKTTGRdi0p7q4mQJBAKPMfRSlSzS8jnzT9zN0SVW3n5a6\nkKT5BY9E78nXEz0By5wnLj/pdgKmoH+AcRFgAl0kkczyPeOjZdwmuQQZFPMCQQCg/+ILxti3\naDaB3RYD0DSoXXRY5+YKxXkHcUM1JohCt4NFxdn5Cf4rXr7lWCF73FBXx7+MUwBd/Ecq5WjJ\nZD1JAkBl8STGKQ5nedqC0xQmk2B7QOOeTQ8JrwbAAPFKFAiINCxlUpZcC7mDPMOZUqGCkTST\nvqudE+zvSTBgkp34Xjht\n-----END RSA PRIVATE KEY-----\n","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a967c","group":"core","key":"members_email_auth_secret","value":"364a406d968ed8879fa984c2355e4179a258a02a42f4ff06c4dbd86a21f59b99c86b09d2b63ec8d91b265ee51ebf84046e562d2db5934e28695ad667c146de52","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a967f","group":"site","key":"title","value":"Software Consulting and Contracting - A & H Solutions","type":"string","flags":"PUBLIC","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-11-13T19:11:51.000Z"},{"id":"60b6ee651d684200019a9680","group":"site","key":"description","value":"We help startups, small businesses and Enterprises to build and manage tech products.","type":"string","flags":"PUBLIC","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:22:16.000Z"},{"id":"60b6ee651d684200019a9681","group":"site","key":"logo","value":"__GHOST_URL__/content/images/2022/10/logo.png","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-12T00:40:34.000Z"},{"id":"60b6ee651d684200019a9682","group":"site","key":"cover_image","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-11-23T02:23:01.000Z"},{"id":"60b6ee651d684200019a9683","group":"site","key":"icon","value":"__GHOST_URL__/content/images/2022/10/output-onlinepngtools.png","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-12T00:40:34.000Z"},{"id":"60b6ee651d684200019a9684","group":"site","key":"accent_color","value":"#ffffff","type":"string","flags":"PUBLIC","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-11T00:58:38.000Z"},{"id":"60b6ee651d684200019a9685","group":"site","key":"lang","value":"en","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9686","group":"site","key":"timezone","value":"America/Denver","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-03T05:19:05.000Z"},{"id":"60b6ee651d684200019a9687","group":"site","key":"codeinjection_head","value":"\n\n<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n<link href=\"https://fonts.googleapis.com/css2?family=Noto+Sans+TC&family=Noto+Sans:wght@100&display=swap\" rel=\"stylesheet\">\n<style>\n.author-list,\n.author-name,\n.post-card-byline-content span:first-of-type {\n    display: none;\n}\n.post-card-byline-content {\n    //margin-left: 0;\n}\n     a, abbr, acronym, address, applet, article, aside, audio, big, blockquote, body, canvas, caption, cite, code, dd, del, details, dfn, div, dl, dt, em, embed, fieldset, figcap,body{\n     font-family: 'Noto Sans TC', sans-serif;\n}\n</style>\n<script>function initApollo(){var n=Math.random().toString(36).substring(7),o=document.createElement(\"script\");\no.src=\"https://assets.apollo.io/micro/website-tracker/tracker.iife.js?nocache=\"+n,o.async=!0,o.defer=!0,\no.onload=function(){window.trackingFunctions.onLoad({appId:\"66f0a825eff380019ac62c6e\"})},\ndocument.head.appendChild(o)}initApollo();</script>\n","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2025-04-10T22:03:06.000Z"},{"id":"60b6ee651d684200019a9688","group":"site","key":"codeinjection_foot","value":"<!-- Global site tag (gtag.js) - Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-0PMZ38H9GR\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0PMZ38H9GR');\n</script>\n<style>\n.author-list,\n.author-name,\n.post-card-byline-content span:first-of-type {\n}\n.post-card-byline-content {\n    //margin-left: 0;\n}\n     a, abbr, acronym, address, applet, article, aside, audio, big, blockquote, body, canvas, caption, cite, code, dd, del, details, dfn, div, dl, dt, em, embed, fieldset, figcap,body{\n     font-family: 'Noto Sans TC', sans-serif;\n}\n    \n</style>","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-11-10T16:06:28.000Z"},{"id":"60b6ee651d684200019a9689","group":"site","key":"facebook","value":"aandhsolutions","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-03T01:59:16.000Z"},{"id":"60b6ee651d684200019a968a","group":"site","key":"twitter","value":"@aandhsolutions","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-14T03:17:43.000Z"},{"id":"60b6ee651d684200019a968b","group":"site","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"About\",\"url\":\"/about/\"},{\"label\":\"Contact\",\"url\":\"/#contact\"},{\"label\":\"Blog\",\"url\":\"/blog/\"},{\"label\":\"Case Studies\",\"url\":\"/tag/case-studies/\"},{\"label\":\"Web Builder\",\"url\":\"/web-design-and-development/\"},{\"label\":\"Resume Builder\",\"url\":\"https://resume.aandhsolutions.com/\"}]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2024-11-18T05:45:52.000Z"},{"id":"60b6ee651d684200019a968c","group":"site","key":"secondary_navigation","value":"[{\"label\":\"Youtube\",\"url\":\"https://www.youtube.com/@aandhsolutions\"},{\"label\":\"LinkedIn\",\"url\":\"https://ca.linkedin.com/company/aandhsolutions\"},{\"label\":\"Twitter\",\"url\":\"https://twitter.com/aandhsolutions\"},{\"label\":\"Facebook\",\"url\":\"https://www.facebook.com/aandhsolutions\"},{\"label\":\"Instagram\",\"url\":\"https://www.instagram.com/aandhsolutions/\"},{\"label\":\"Data & Privacy\",\"url\":\"/privacy/\"}]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-11-13T19:09:40.000Z"},{"id":"60b6ee651d684200019a968d","group":"site","key":"meta_title","value":"Software Consulting and Contracting - A & H Solutions","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-11-13T19:22:58.000Z"},{"id":"60b6ee651d684200019a968e","group":"site","key":"meta_description","value":"We help startups, small businesses, and enterprises to build and manage tech products. We follow agile methodologies and business-driven development to ensure your product is well-positioned for the market needs.","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:22:16.000Z"},{"id":"60b6ee651d684200019a968f","group":"site","key":"og_image","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9690","group":"site","key":"og_title","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:23:26.000Z"},{"id":"60b6ee651d684200019a9691","group":"site","key":"og_description","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:23:26.000Z"},{"id":"60b6ee651d684200019a9692","group":"site","key":"twitter_image","value":"__GHOST_URL__/content/images/2022/10/logo_solutions.png","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:23:26.000Z"},{"id":"60b6ee651d684200019a9693","group":"site","key":"twitter_title","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:23:26.000Z"},{"id":"60b6ee651d684200019a9694","group":"site","key":"twitter_description","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-22T20:23:26.000Z"},{"id":"60b6ee651d684200019a9695","group":"theme","key":"active_theme","value":"custom-casper","type":"string","flags":"RO","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2023-09-24T05:33:08.000Z"},{"id":"60b6ee651d684200019a9696","group":"private","key":"is_private","value":"false","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9697","group":"private","key":"password","value":"","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9698","group":"private","key":"public_hash","value":"e58a7aae87dfc55c6f8a238c4c3b23","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a9699","group":"members","key":"default_content_visibility","value":"public","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969a","group":"members","key":"members_signup_access","value":"all","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969b","group":"members","key":"members_from_address","value":"noreply","type":"string","flags":"RO","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969c","group":"members","key":"members_support_address","value":"noreply","type":"string","flags":"PUBLIC,RO","created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969d","group":"members","key":"members_reply_address","value":"newsletter","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969e","group":"members","key":"members_free_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a969f","group":"members","key":"members_paid_signup_redirect","value":"/","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96a0","group":"members","key":"stripe_product_name","value":"Ghost Subscription","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96a3","group":"members","key":"stripe_plans","value":"[]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96a6","group":"members","key":"stripe_connect_livemode","value":null,"type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96a7","group":"members","key":"stripe_connect_display_name","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96a9","group":"members","key":"members_free_price_name","value":"Free","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96aa","group":"members","key":"members_free_price_description","value":"Free preview","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96ab","group":"members","key":"members_monthly_price_id","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96ac","group":"members","key":"members_yearly_price_id","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96ad","group":"portal","key":"portal_name","value":"true","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-07T01:38:27.000Z"},{"id":"60b6ee651d684200019a96ae","group":"portal","key":"portal_button","value":"false","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-12T04:49:05.000Z"},{"id":"60b6ee651d684200019a96af","group":"portal","key":"portal_plans","value":"[\"free\"]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b0","group":"portal","key":"portal_button_style","value":"icon-and-text","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b1","group":"portal","key":"portal_button_icon","value":"icon-1","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-07T01:37:52.000Z"},{"id":"60b6ee651d684200019a96b2","group":"portal","key":"portal_button_signup_text","value":"Subscribe","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-10-12T04:48:33.000Z"},{"id":"60b6ee651d684200019a96b3","group":"email","key":"mailgun_domain","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b4","group":"email","key":"mailgun_api_key","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b5","group":"email","key":"mailgun_base_url","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b6","group":"email","key":"email_track_opens","value":"true","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b7","group":"amp","key":"amp","value":"true","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b8","group":"amp","key":"amp_gtag_id","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96b9","group":"firstpromoter","key":"firstpromoter","value":"false","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96ba","group":"firstpromoter","key":"firstpromoter_id","value":null,"type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96bb","group":"slack","key":"slack_url","value":"","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2025-04-10T22:05:19.000Z"},{"id":"60b6ee651d684200019a96bc","group":"slack","key":"slack_username","value":"Ghost","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2025-04-10T22:05:19.000Z"},{"id":"60b6ee651d684200019a96bd","group":"unsplash","key":"unsplash","value":"true","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96be","group":"views","key":"shared_views","value":"[]","type":"array","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96bf","group":"newsletter","key":"newsletter_show_badge","value":"false","type":"boolean","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-07T01:40:11.000Z"},{"id":"60b6ee651d684200019a96c1","group":"newsletter","key":"newsletter_body_font_category","value":"serif","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-07T01:40:11.000Z"},{"id":"60b6ee651d684200019a96c2","group":"newsletter","key":"newsletter_footer_content","value":"Let's go paperless together.","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2022-02-07T01:40:11.000Z"},{"id":"60b6ee651d684200019a96c5","group":"editor","key":"editor_default_email_recipients","value":"visibility","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"60b6ee651d684200019a96c6","group":"editor","key":"editor_default_email_recipients_filter","value":"all","type":"string","flags":null,"created_at":"2021-06-02T02:35:17.000Z","updated_at":"2021-06-02T02:35:17.000Z"},{"id":"67b1420b1d62d30001b31e94","group":"labs","key":"labs","value":"{}","type":"object","flags":null,"created_at":"2025-02-16T01:40:27.000Z","updated_at":"2025-02-16T01:40:27.000Z"},{"id":"67b1420b1d62d30001b31e95","group":"portal","key":"portal_products","value":"[\"60b6ee611d684200019a9508\"]","type":"array","flags":null,"created_at":"2025-02-16T01:40:27.000Z","updated_at":"2025-02-16T01:40:27.000Z"},{"id":"67b142141d62d30001b31ea6","group":"editor","key":"editor_is_launch_complete","value":"false","type":"boolean","flags":null,"created_at":"2025-02-16T01:40:36.000Z","updated_at":"2025-02-16T01:40:36.000Z"},{"id":"67b142141d62d30001b31ea8","group":"members","key":"default_content_visibility_tiers","value":"[]","type":"array","flags":null,"created_at":"2025-02-16T01:40:36.000Z","updated_at":"2025-02-16T01:40:36.000Z"},{"id":"67b142161d62d30001b31eb9","group":"core","key":"version_notifications","value":"[]","type":"array","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec3","group":"newsletter","key":"newsletter_header_image","value":null,"type":"string","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec4","group":"newsletter","key":"newsletter_show_header_icon","value":"true","type":"boolean","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec5","group":"newsletter","key":"newsletter_show_header_title","value":"true","type":"boolean","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec6","group":"newsletter","key":"newsletter_title_alignment","value":"center","type":"string","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec7","group":"newsletter","key":"newsletter_title_font_category","value":"sans_serif","type":"string","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"},{"id":"67b142161d62d30001b31ec8","group":"newsletter","key":"newsletter_show_feature_image","value":"true","type":"boolean","flags":null,"created_at":"2025-02-16T01:40:38.000Z","updated_at":"2025-02-16T01:40:38.000Z"}],"snippets":[{"id":"61cd13a9f65b490001261da4","name":"test","mobiledoc":"{\"version\":\"0.3.2\",\"atoms\":[],\"cards\":[[\"html\",{\"html\":\"<script src=\\\"https://gist.github.com/Harsimran1/dd88c940661e79ed2582c5ea26ff1571.js\\\"></script>\"}]],\"markups\":[],\"sections\":[[10,0]]}","created_at":"2021-12-30T02:04:25.000Z","updated_at":"2021-12-30T02:04:25.000Z"}],"stripe_prices":[],"stripe_products":[],"tags":[{"id":"60c67d04fda06e0001f83a48","name":"TDD","slug":"tdd","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-13T21:47:48.000Z","updated_at":"2021-06-13T21:47:48.000Z"},{"id":"60c67d04fda06e0001f83a49","name":"Test Driven Development","slug":"test-driven-development","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-13T21:47:48.000Z","updated_at":"2021-06-13T21:47:48.000Z"},{"id":"60c67d04fda06e0001f83a4a","name":"Kent Beck","slug":"kent-beck","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-13T21:47:48.000Z","updated_at":"2021-06-13T21:47:48.000Z"},{"id":"60c67ef2fda06e0001f83a53","name":"Book Summary","slug":"book-summary","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-13T21:56:02.000Z","updated_at":"2021-06-13T21:56:02.000Z"},{"id":"60c67ef2fda06e0001f83a54","name":"Takeaways","slug":"takeaways","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-13T21:56:02.000Z","updated_at":"2021-06-13T21:56:02.000Z"},{"id":"60c6b394fda06e0001f83a6e","name":"JWT","slug":"jwt","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-14T01:40:36.000Z","updated_at":"2021-06-14T01:40:36.000Z"},{"id":"60c6b394fda06e0001f83a6f","name":"Email Verification Token","slug":"email-verification-token","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-14T01:40:36.000Z","updated_at":"2021-06-14T01:40:36.000Z"},{"id":"60c6b394fda06e0001f83a70","name":"Security","slug":"security","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-06-14T01:40:36.000Z","updated_at":"2021-06-14T01:40:36.000Z"},{"id":"61902fd3f65b490001261b59","name":"Ansible","slug":"ansible","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-11-13T21:36:19.000Z","updated_at":"2021-11-13T21:36:19.000Z"},{"id":"61902fd3f65b490001261b5a","name":"logrotate","slug":"logrotate","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-11-13T21:36:19.000Z","updated_at":"2021-11-13T21:36:19.000Z"},{"id":"61902fd3f65b490001261b5b","name":"linux","slug":"linux","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-11-13T21:36:19.000Z","updated_at":"2021-11-13T21:36:19.000Z"},{"id":"61902fd3f65b490001261b5c","name":"logrotation","slug":"logrotation","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-11-13T21:36:19.000Z","updated_at":"2021-11-13T21:36:19.000Z"},{"id":"61ccda3ef65b490001261d3c","name":"Docker","slug":"docker","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-29T21:59:26.000Z","updated_at":"2021-12-29T21:59:26.000Z"},{"id":"61ccda3ef65b490001261d3d","name":"Debug","slug":"debug","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2021-12-29T21:59:26.000Z","updated_at":"2021-12-29T21:59:26.000Z"},{"id":"61eca317d831dd00011bc34d","name":"dead computer","slug":"dead-computer","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-01-23T00:36:39.000Z","updated_at":"2022-01-23T00:36:39.000Z"},{"id":"61eca317d831dd00011bc34e","name":"Arch Linux","slug":"arch-linux","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-01-23T00:36:39.000Z","updated_at":"2022-01-23T00:36:39.000Z"},{"id":"61eca317d831dd00011bc34f","name":"GRUB","slug":"grub","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-01-23T00:36:39.000Z","updated_at":"2022-01-23T00:36:39.000Z"},{"id":"61eca317d831dd00011bc350","name":"bootloader","slug":"bootloader","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-01-23T00:36:39.000Z","updated_at":"2022-01-23T00:36:39.000Z"},{"id":"61fcc3d7021a27000172f446","name":"Apache JMeter","slug":"apache-jmeter","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-04T06:12:39.000Z","updated_at":"2022-02-04T06:12:39.000Z"},{"id":"61fcc3d7021a27000172f447","name":"Load Testing","slug":"load-testing","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-04T06:12:39.000Z","updated_at":"2022-02-04T06:12:39.000Z"},{"id":"61fdf404021a27000172f450","name":"Nginx","slug":"nginx","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T03:50:28.000Z","updated_at":"2022-02-05T03:50:28.000Z"},{"id":"61fdf404021a27000172f451","name":"Kubernetes","slug":"kubernetes","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T03:50:28.000Z","updated_at":"2022-02-05T03:50:28.000Z"},{"id":"61fdf4c4021a27000172f45c","name":"Golang","slug":"golang","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T03:53:40.000Z","updated_at":"2022-02-05T03:53:40.000Z"},{"id":"61fdf4c4021a27000172f45d","name":"Interfaces","slug":"interfaces","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T03:53:40.000Z","updated_at":"2022-02-05T03:53:40.000Z"},{"id":"61fdf50b021a27000172f463","name":"Optimizations","slug":"optimizations","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T03:54:51.000Z","updated_at":"2022-02-05T03:54:51.000Z"},{"id":"61fe0638021a27000172f468","name":"Database","slug":"database","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T05:08:08.000Z","updated_at":"2022-02-05T05:08:08.000Z"},{"id":"61fe069a021a27000172f46d","name":"Siege","slug":"siege","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T05:09:46.000Z","updated_at":"2022-02-05T05:09:46.000Z"},{"id":"61fe0c1a021a27000172f473","name":"Raspberry Pi","slug":"raspberry-pi","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T05:33:14.000Z","updated_at":"2022-02-05T05:33:14.000Z"},{"id":"61fe0c33021a27000172f477","name":"Web Hosting","slug":"web-hosting","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2022-02-05T05:33:39.000Z","updated_at":"2022-02-05T05:33:39.000Z"},{"id":"64a9ec9c2f69e4000100a5c4","name":"Case Studies","slug":"case-studies","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2023-07-08T23:09:16.000Z","updated_at":"2023-07-08T23:09:16.000Z"},{"id":"67032aedde6f0100018c3aec","name":"Electron","slug":"electron","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2024-10-07T00:27:25.000Z","updated_at":"2024-10-07T00:27:25.000Z"},{"id":"67032ea9de6f0100018c3afe","name":"SQL","slug":"sql","description":null,"feature_image":null,"parent_id":null,"visibility":"public","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"meta_title":null,"meta_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null,"created_at":"2024-10-07T00:43:21.000Z","updated_at":"2024-10-07T00:43:21.000Z"}],"users":[{"id":"1","name":"Harsimran Kaur ","slug":"harsimran","password":"$2a$10$ANCzD7UicAv4Y6mHTHx9YOwvNFkYjzE/WypuScOUdGuYV/yEaGN7e","email":"contact@aandhsolutions.com","profile_image":null,"cover_image":null,"bio":null,"website":null,"location":null,"facebook":null,"twitter":null,"accessibility":"{\"nightShift\":false,\"navigation\":{\"expanded\":{\"posts\":true}}}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":"2025-09-09T07:33:05.000Z","created_at":"2021-06-02T02:35:14.000Z","updated_at":"2025-09-09T07:33:05.000Z"},{"id":"5951f5fca366002ebd5dbef7","name":"Ghost","slug":"ghost","password":"$2a$10$vBHuhTNySm7.Om6jW6oJve33r3fQyPCg2kiBqKu/rvN6tuj4RYJlC","email":"ghost-author@example.com","profile_image":"https://static.ghost.org/v4.0.0/images/ghost-user.png","cover_image":null,"bio":"You can delete this user to remove all the welcome posts","website":"https://ghost.org","location":"The Internet","facebook":"ghost","twitter":"ghost","accessibility":null,"status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":null,"last_seen":null,"created_at":"2021-06-02T02:35:14.000Z","updated_at":"2021-06-02T02:35:14.000Z"}]}}]}